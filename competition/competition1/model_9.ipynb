{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ec9d133",
   "metadata": {},
   "source": [
    "Cell 1 — Imports & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Imports & Load Data ===\n",
    "import warnings, re, numpy as np, pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "\n",
    "# Paths\n",
    "TRAIN_PATH = './dataset/train.csv'\n",
    "TEST_PATH  = './dataset/test.csv'\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_test  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "print(df_train.shape, df_test.shape)\n",
    "df_train.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15e675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 2: Robust HTML -> Structured features (Title/Author/Channel/Topic/DateTime/Counts) ===\n",
    "from datetime import datetime\n",
    "\n",
    "_day_map  = {'mon':1,'tue':2,'wed':3,'thu':4,'fri':5,'sat':6,'sun':7}\n",
    "_month_map= {'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,\n",
    "             'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12}\n",
    "\n",
    "def _text(x):\n",
    "    try:\n",
    "        return re.sub(r\"\\s+\", \" \", x.get_text(\" \", strip=True)).strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def _find_first(soup, selectors):\n",
    "    for sel in selectors:\n",
    "        try:\n",
    "            node = soup.select_one(sel)\n",
    "            if node: return node\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def preprocessor(html: str):\n",
    "    # Parse HTML safely\n",
    "    soup = BeautifulSoup(html or \"\", 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    title = \"\"\n",
    "    for sel in [\"body h1\", \"header h1\", \"h1\", \"title\"]:\n",
    "        node = _find_first(soup, [sel])\n",
    "        if node:\n",
    "            title = _text(node).lower()\n",
    "            break\n",
    "\n",
    "    # Author (common patterns)\n",
    "    author = \"\"\n",
    "    node = _find_first(soup, [\n",
    "        \"head .article-info .author_name\", \"head .author_name\", \"span.author a\",\n",
    "        \"span.author\", \"div.byline\", \"p.byline\", \"meta[name='author']\"\n",
    "    ])\n",
    "    if node:\n",
    "        author = _text(node).lower()\n",
    "    else:\n",
    "        # meta author\n",
    "        try:\n",
    "            meta = soup.find('meta', attrs={'name':'author'})\n",
    "            if meta and meta.get('content'): author = meta['content'].strip().lower()\n",
    "        except Exception:\n",
    "            pass\n",
    "    if author.startswith(\"by \"): author = author[3:].strip()\n",
    "\n",
    "    # Channel / Topic (heuristics)\n",
    "    channel = \"\"\n",
    "    node = _find_first(soup, [\".article\", \"[data-channel]\", \"meta[property='article:section']\"])\n",
    "    if node:\n",
    "        if node.has_attr('data-channel'):\n",
    "            channel = (node['data-channel'] or \"\").strip().lower()\n",
    "        else:\n",
    "            channel = _text(node).lower()\n",
    "    topic = \"\"\n",
    "    node = _find_first(soup, [\".article-topics\", \"footer .article-topics\", \"a[rel='tag']\", \"a.tag\"])\n",
    "    if node:\n",
    "        topic = _text(node).lower()\n",
    "\n",
    "    # Datetime pieces (try to find any date-like string)\n",
    "    day,date,month,year,hour,minute,second = \"\",\"\",\"\",\"\",\"\",\"\",\"\"\n",
    "    dt_candidates = []\n",
    "    # meta datetime\n",
    "    for attrs in [{'property':'article:published_time'},{'name':'pubdate'},{'itemprop':'datePublished'},{'name':'date'}]:\n",
    "        try:\n",
    "            m = soup.find('meta', attrs=attrs)\n",
    "            if m and m.get('content'): dt_candidates.append(m['content'])\n",
    "        except Exception:\n",
    "            pass\n",
    "    # visible datetime text\n",
    "    for sel in [\"time\", \"span.time\", \"div.time\", \".date\", \".published\", \".pubdate\"]:\n",
    "        n = _find_first(soup, [sel])\n",
    "        if n:\n",
    "            dt_candidates.append(_text(n))\n",
    "    dt_txt = next((t for t in dt_candidates if t), \"\")\n",
    "    # extract parts\n",
    "    m = re.search(r\"(mon|tue|wed|thu|fri|sat|sun)\", dt_txt, flags=re.I)\n",
    "    if m: day = m.group(1).lower()\n",
    "    m = re.search(r\"(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\", dt_txt, flags=re.I)\n",
    "    if m: month = m.group(1).lower()\n",
    "    m = re.search(r\"\\b(20\\d{2}|19\\d{2})\\b\", dt_txt)\n",
    "    if m: year = m.group(1)\n",
    "    m = re.search(r\"\\b(\\d{1,2}):(\\d{2})(?::(\\d{2}))?\\b\", dt_txt)\n",
    "    if m:\n",
    "        hour, minute, second = m.group(1), m.group(2), (m.group(3) or \"0\")\n",
    "\n",
    "    # Content length, counts\n",
    "    # Strip scripts/styles\n",
    "    for t in soup([\"script\",\"style\",\"noscript\"]):\n",
    "        t.decompose()\n",
    "    content = _text(soup.body or soup)\n",
    "    content_len = len(content)\n",
    "\n",
    "    num_see_also = len(soup.find_all(string=re.compile(r\"see also|related\", re.I)))\n",
    "    num_image    = len(soup.find_all(\"img\"))\n",
    "    num_a        = len(soup.find_all(\"a\"))\n",
    "\n",
    "    return (title or \"unknown\", author or \"unknown\", channel or \"unknown\", topic or \"unknown\",\n",
    "            day or \"unk\", date or \"\", month or \"unk\", year or \"\", hour or \"\", minute or \"\", second or \"\",\n",
    "            content_len, num_see_also, num_image, num_a)\n",
    "\n",
    "# Build combined feature table\n",
    "feature_list = [preprocessor(t) for t in df_train['Page content']]\n",
    "feature_list += [preprocessor(t) for t in df_test['Page content']]\n",
    "\n",
    "df_combine = pd.DataFrame(feature_list, columns=[\n",
    "    'Title', 'Author', 'Channel', 'Topic', 'Day', 'Date', 'Month', 'Year',\n",
    "    'Hour', 'Minute', 'Second', 'Content_Len', 'Num_See_Also', 'Num_Image', 'Num_A'\n",
    "])\n",
    "df_combine.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 3: Map day/month, drop low-value cols, create df_copy ===\n",
    "day_map = {'mon':1, 'tue':2, 'wed':3, 'thu':4, 'fri':5, 'sat':6, 'sun':7}\n",
    "month_map = {'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12}\n",
    "\n",
    "df_copy = df_combine.copy()\n",
    "df_copy['Day']   = df_copy['Day'].map(day_map).fillna(0).astype(int)\n",
    "df_copy['Month'] = df_copy['Month'].map(month_map).fillna(0).astype(int)\n",
    "\n",
    "# High-cardinality or weak signals (keep Author/Topic text; drop others similar to model_8)\n",
    "drop_cols = ['Title', 'Channel', 'Minute', 'Second', 'Num_See_Also', 'Num_Image', 'Num_A', 'Date']\n",
    "df_copy = df_copy.drop(columns=[c for c in drop_cols if c in df_copy.columns])\n",
    "\n",
    "# Fill empties\n",
    "for c in ['Author','Topic','Year','Hour']:\n",
    "    if c in df_copy.columns:\n",
    "        df_copy[c] = df_copy[c].fillna('unknown')\n",
    "for c in df_copy.columns:\n",
    "    if c not in ['Author','Topic']:\n",
    "        df_copy[c] = pd.to_numeric(df_copy[c], errors='coerce').fillna(0)\n",
    "\n",
    "df_copy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 4: Tokenizers (WordNet Lemmatizer) ===\n",
    "import re, numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def tokenizer(text):\n",
    "    if isinstance(text, np.ndarray):\n",
    "        text = text[0]\n",
    "    return re.split(r'\\s+', str(text).strip())\n",
    "\n",
    "def tokenizer_wnl(text):\n",
    "    if isinstance(text, np.ndarray):\n",
    "        text = text[0]\n",
    "    text = re.sub(r\"([\\w]+)'[\\w]+\", lambda m: m.group(1), str(text))\n",
    "    text = re.sub(r\"\\.\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w]+\", \" \", text)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(s) for s in re.split(r'\\s+', text.strip())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: ColumnTransformers (CountVectorizer on Author/Topic) ===\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Build the matrix column order [Author, Topic, (numeric...)]\n",
    "text_cols = ['Author', 'Topic']\n",
    "num_cols  = [c for c in df_copy.columns if c not in text_cols]\n",
    "\n",
    "def build_mats():\n",
    "    X_all = pd.concat([df_copy[text_cols], df_copy[num_cols]], axis=1).values\n",
    "    n_train = df_train.shape[0]\n",
    "    X_train_raw = X_all[:n_train]\n",
    "    X_test      = X_all[n_train:]\n",
    "    y_train_raw = (df_train['Popularity'].values == 1).astype(int)\n",
    "    return X_train_raw, y_train_raw, X_test\n",
    "\n",
    "X_train_raw, y_train_raw, X_test = build_mats()\n",
    "\n",
    "# 检查数据结构\n",
    "print(\"X_train_raw shape:\", X_train_raw.shape)\n",
    "print(\"Text columns:\", text_cols)\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "print(\"Sample data:\")\n",
    "print(\"Author (col 0):\", X_train_raw[0, 0])\n",
    "print(\"Topic (col 1):\", X_train_raw[0, 1])\n",
    "print(\"First numeric col:\", X_train_raw[0, 2])\n",
    "\n",
    "# ColumnTransformer for models that prefer only Topic text + numeric passthrough\n",
    "# 修复：明确指定哪些列是数值列\n",
    "num_col_indices = list(range(2, len(text_cols) + len(num_cols)))  # 从第2列开始都是数值列\n",
    "\n",
    "trans_other = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('Topic', CountVectorizer(tokenizer=tokenizer_wnl, lowercase=False), [1]),  # column 1 is Topic\n",
    "        ('numeric', 'passthrough', num_col_indices)  # 明确指定数值列\n",
    "    ],\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# ColumnTransformer for RandomForest using both Author & Topic\n",
    "trans_forest = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('Author', CountVectorizer(tokenizer=tokenizer, lowercase=False), [0]),   # Author\n",
    "        ('Topic',  CountVectorizer(tokenizer=tokenizer_wnl, lowercase=False), [1]), # Topic\n",
    "        ('numeric', 'passthrough', num_col_indices)  # 明确指定数值列\n",
    "    ],\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 6: Train/Valid split + training() helper (AUC) ===\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_raw, y_train_raw, test_size=0.2, random_state=0, stratify=y_train_raw\n",
    ")\n",
    "\n",
    "def training(clf, cv=5):\n",
    "    cv_results = cross_validate(clf, X_train_raw, y_train_raw, cv=cv,\n",
    "                                scoring='roc_auc', return_train_score=True, n_jobs=-1)\n",
    "    print('CV train AUC:  {:.5f} (+/- {:.5f})'.format(np.mean(cv_results['train_score']), np.std(cv_results['train_score'])))\n",
    "    print('CV valid AUC:  {:.5f} (+/- {:.5f})'.format(np.mean(cv_results['test_score']),  np.std(cv_results['test_score'])))\n",
    "    clf.fit(X_train, y_train)\n",
    "    valid_auc = roc_auc_score(y_valid, clf.predict_proba(X_valid)[:,1])\n",
    "    print('Holdout valid AUC:', round(valid_auc, 5))\n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fdc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 7: LightGBM Pipeline ===\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = Pipeline([('ct', trans_other),\n",
    "                 ('clf', LGBMClassifier(random_state=0, learning_rate=0.009, n_estimators=200))])\n",
    "lgbm = training(lgbm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41dc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 8: RandomForest Pipeline ===\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = Pipeline([('ct', trans_forest),\n",
    "                   ('clf', RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=0))])\n",
    "forest = training(forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 9: XGBoost Pipeline ===\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgboost = Pipeline([('ct', trans_other),\n",
    "                    ('clf', XGBClassifier(n_estimators=300, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, \n",
    "                                          eval_metric='auc', n_jobs=-1, verbosity=0, random_state=0))])\n",
    "xgboost = training(xgboost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 10: CatBoost Pipeline ===\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost = Pipeline([('ct', trans_other),\n",
    "                     ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=300, learning_rate=0.5, random_seed=0))])\n",
    "catboost = training(catboost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d1192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 11: Soft Voting Ensemble ===\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting = VotingClassifier([('lgbm', lgbm), ('forest', forest), ('catboost', catboost), ('xgb', xgboost)],\n",
    "                          voting='soft', weights=[1.0, 0.2, 0.2, 0.6], n_jobs=-1)\n",
    "voting = training(voting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f905cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = voting\n",
    "\n",
    "y_score = best_model.predict_proba(X_test)[:, 1]\n",
    "df_pred = pd.DataFrame({'Id': df_test['Id'], 'Popularity': y_score})\n",
    "df_pred.to_csv('submission_93.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
