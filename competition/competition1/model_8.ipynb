{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf61e8cc",
   "metadata": {},
   "source": [
    "載入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "406a7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('./dataset/train.csv')\n",
    "df_test = pd.read_csv('./dataset/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa594143",
   "metadata": {},
   "source": [
    "整理特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e20af56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:22: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:37: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:45: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:22: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:37: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:45: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:53: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\3966687669.py:22: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  author = re.sub('\\s+', ' ', author.strip().lower())\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\3966687669.py:29: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  author_list = re.split('\\s*&\\s*', author)\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\3966687669.py:31: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  authors = re.split('\\s*,\\s*', author)\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\3966687669.py:36: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  author_list += re.split('\\s*&\\s*', authors[-1])\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\3966687669.py:37: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  author = ' '.join([re.sub('\\s+', '_', a) for a in author_list])\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\3966687669.py:45: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  topic = ' '.join([re.sub('\\s+', '_', t) for t in topic_list])\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\3966687669.py:53: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  match_obj = re.search('([\\w]+),\\s+([\\d]+)\\s+([\\w]+)\\s+([\\d]+)\\s+([\\d]+):([\\d]+):([\\d]+)', date_time)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def preprocessor(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # find title\n",
    "    title = soup.body.h1.string.strip().lower()\n",
    "\n",
    "    # find author\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    author_name = article_info.find('span', {'class': 'author_name'})\n",
    "    if author_name != None:\n",
    "        author = author_name.get_text()\n",
    "    elif article_info.span != None:\n",
    "        author = article_info.span.string\n",
    "    else:\n",
    "        author = article_info.a.string\n",
    "\n",
    "    # clean author\n",
    "    author = re.sub('\\s+', ' ', author.strip().lower())\n",
    "    if author.startswith('by '):\n",
    "        author = author[3:]\n",
    "    author = re.sub('&.*;', '&', author.replace(' and ', ' & '))\n",
    "\n",
    "    author_list = []\n",
    "    if author.find(',') == -1:\n",
    "        author_list = re.split('\\s*&\\s*', author)\n",
    "    else:\n",
    "        authors = re.split('\\s*,\\s*', author)\n",
    "        if authors[-1].find('&') == -1 or len(authors[-1].split('&')[-1].strip().split()) > 3:\n",
    "            author_list.append(authors[0])\n",
    "        else:\n",
    "            author_list += authors[:-1]\n",
    "            author_list += re.split('\\s*&\\s*', authors[-1])\n",
    "    author = ' '.join([re.sub('\\s+', '_', a) for a in author_list])\n",
    "\n",
    "    # find channel\n",
    "    channel = soup.body.article['data-channel'].strip().lower()\n",
    "\n",
    "    # find topic\n",
    "    a_list = soup.body.find('footer', {'class': 'article-topics'}).find_all('a')\n",
    "    topic_list = [a.string.strip().lower() for a in a_list]\n",
    "    topic = ' '.join([re.sub('\\s+', '_', t) for t in topic_list])\n",
    "\n",
    "    # find datetime\n",
    "    article_info = soup.head.find('div', {'class': 'article-info'})\n",
    "    try:\n",
    "        date_time = article_info.time['datetime']\n",
    "    except:\n",
    "        date_time = 'Wed, 10 Oct 2014 15:00:43'\n",
    "    match_obj = re.search('([\\w]+),\\s+([\\d]+)\\s+([\\w]+)\\s+([\\d]+)\\s+([\\d]+):([\\d]+):([\\d]+)', date_time)\n",
    "    day, date, month, year, hour, minute, second = match_obj.groups()\n",
    "    day, month = day.lower(), month.lower()\n",
    "\n",
    "    # find content\n",
    "    content = soup.body.find('section', {'class': 'article-content'}).get_text()\n",
    "    content_len = len(content)\n",
    "\n",
    "    # find see also\n",
    "    num_see_also = len(re.findall('see also', content.lower()))\n",
    "\n",
    "    # find image\n",
    "    num_image = len(soup.body.find_all('img'))\n",
    "\n",
    "    # find a\n",
    "    num_a = len(soup.body.find_all('a'))\n",
    "\n",
    "    return title, author, channel, topic, day, date, month, year, \\\n",
    "        hour, minute, second, content_len, num_see_also, num_image, num_a\n",
    "\n",
    "\n",
    "feature_list = []\n",
    "for text in df_train['Page content']:\n",
    "    feature_list.append(preprocessor(text))\n",
    "for text in df_test['Page content']:\n",
    "    feature_list.append(preprocessor(text))\n",
    "\n",
    "df_combine = pd.DataFrame(\n",
    "    feature_list,\n",
    "    columns=['Title', 'Author', 'Channel', 'Topic', 'Day', 'Date', 'Month', 'Year',\n",
    "             'Hour', 'Minute', 'Second', 'Content_Len', 'Num_See_Also', 'Num_Image', 'Num_A']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c216a8",
   "metadata": {},
   "source": [
    "打印前幾列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4771335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Day</th>\n",
       "      <th>Date</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Second</th>\n",
       "      <th>Content_Len</th>\n",
       "      <th>Num_See_Also</th>\n",
       "      <th>Num_Image</th>\n",
       "      <th>Num_A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nasa's grand challenge: stop asteroids from de...</td>\n",
       "      <td>clara_moskowitz</td>\n",
       "      <td>world</td>\n",
       "      <td>asteroid asteroids challenge earth space u.s. ...</td>\n",
       "      <td>wed</td>\n",
       "      <td>19</td>\n",
       "      <td>jun</td>\n",
       "      <td>2013</td>\n",
       "      <td>15</td>\n",
       "      <td>04</td>\n",
       "      <td>30</td>\n",
       "      <td>3591</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google's new open source patent pledge: we won...</td>\n",
       "      <td>christina_warren</td>\n",
       "      <td>tech</td>\n",
       "      <td>apps_and_software google open_source opn_pledg...</td>\n",
       "      <td>thu</td>\n",
       "      <td>28</td>\n",
       "      <td>mar</td>\n",
       "      <td>2013</td>\n",
       "      <td>17</td>\n",
       "      <td>40</td>\n",
       "      <td>55</td>\n",
       "      <td>1843</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ballin': 2014 nfl draft picks get to choose th...</td>\n",
       "      <td>sam_laird</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment nfl nfl_draft sports television</td>\n",
       "      <td>wed</td>\n",
       "      <td>07</td>\n",
       "      <td>may</td>\n",
       "      <td>2014</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>6646</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cameraperson fails deliver slapstick laughs</td>\n",
       "      <td>sam_laird</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>sports video videos watercooler</td>\n",
       "      <td>fri</td>\n",
       "      <td>11</td>\n",
       "      <td>oct</td>\n",
       "      <td>2013</td>\n",
       "      <td>02</td>\n",
       "      <td>26</td>\n",
       "      <td>50</td>\n",
       "      <td>1821</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nfl star helps young fan prove friendship with...</td>\n",
       "      <td>connor_finnegan</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment instagram instagram_video nfl sp...</td>\n",
       "      <td>thu</td>\n",
       "      <td>17</td>\n",
       "      <td>apr</td>\n",
       "      <td>2014</td>\n",
       "      <td>03</td>\n",
       "      <td>31</td>\n",
       "      <td>43</td>\n",
       "      <td>8919</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title            Author  \\\n",
       "0  nasa's grand challenge: stop asteroids from de...   clara_moskowitz   \n",
       "1  google's new open source patent pledge: we won...  christina_warren   \n",
       "2  ballin': 2014 nfl draft picks get to choose th...         sam_laird   \n",
       "3        cameraperson fails deliver slapstick laughs         sam_laird   \n",
       "4  nfl star helps young fan prove friendship with...   connor_finnegan   \n",
       "\n",
       "         Channel                                              Topic  Day Date  \\\n",
       "0          world  asteroid asteroids challenge earth space u.s. ...  wed   19   \n",
       "1           tech  apps_and_software google open_source opn_pledg...  thu   28   \n",
       "2  entertainment      entertainment nfl nfl_draft sports television  wed   07   \n",
       "3    watercooler                    sports video videos watercooler  fri   11   \n",
       "4  entertainment  entertainment instagram instagram_video nfl sp...  thu   17   \n",
       "\n",
       "  Month  Year Hour Minute Second  Content_Len  Num_See_Also  Num_Image  Num_A  \n",
       "0   jun  2013   15     04     30         3591             4          1     21  \n",
       "1   mar  2013   17     40     55         1843             1          1     16  \n",
       "2   may  2014   19     15     20         6646             1          1      9  \n",
       "3   oct  2013   02     26     50         1821             1          0     11  \n",
       "4   apr  2014   03     31     43         8919             1         51     14  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1926933",
   "metadata": {},
   "source": [
    "最後選擇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f6fbc307",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_map = {'mon': 1, 'tue': 2, 'wed': 3,\n",
    "           'thu': 4, 'fri': 5, 'sat': 6, 'sun': 7}\n",
    "month_map = {'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
    "             'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12}\n",
    "\n",
    "df_copy = df_combine.copy()\n",
    "df_copy['Day'] = df_copy['Day'].map(day_map)\n",
    "df_copy['Month'] = df_copy['Month'].map(month_map)\n",
    "\n",
    "df_copy = df_copy.drop(columns=['Title', 'Channel', 'Minute', 'Second', 'Num_See_Also', 'Num_Image', 'Num_A'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65f45040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Channel</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Day</th>\n",
       "      <th>Date</th>\n",
       "      <th>Month</th>\n",
       "      <th>Year</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Content_Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>clara_moskowitz</td>\n",
       "      <td>world</td>\n",
       "      <td>asteroid asteroids challenge earth space u.s. ...</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>2013</td>\n",
       "      <td>15</td>\n",
       "      <td>3591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>christina_warren</td>\n",
       "      <td>tech</td>\n",
       "      <td>apps_and_software google open_source opn_pledg...</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>17</td>\n",
       "      <td>1843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sam_laird</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment nfl nfl_draft sports television</td>\n",
       "      <td>3</td>\n",
       "      <td>07</td>\n",
       "      <td>5</td>\n",
       "      <td>2014</td>\n",
       "      <td>19</td>\n",
       "      <td>6646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sam_laird</td>\n",
       "      <td>watercooler</td>\n",
       "      <td>sports video videos watercooler</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2013</td>\n",
       "      <td>02</td>\n",
       "      <td>1821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>connor_finnegan</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>entertainment instagram instagram_video nfl sp...</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "      <td>03</td>\n",
       "      <td>8919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Author        Channel  \\\n",
       "0   clara_moskowitz          world   \n",
       "1  christina_warren           tech   \n",
       "2         sam_laird  entertainment   \n",
       "3         sam_laird    watercooler   \n",
       "4   connor_finnegan  entertainment   \n",
       "\n",
       "                                               Topic  Day Date  Month  Year  \\\n",
       "0  asteroid asteroids challenge earth space u.s. ...    3   19      6  2013   \n",
       "1  apps_and_software google open_source opn_pledg...    4   28      3  2013   \n",
       "2      entertainment nfl nfl_draft sports television    3   07      5  2014   \n",
       "3                    sports video videos watercooler    5   11     10  2013   \n",
       "4  entertainment instagram instagram_video nfl sp...    4   17      4  2014   \n",
       "\n",
       "  Hour  Content_Len  \n",
       "0   15         3591  \n",
       "1   17         1843  \n",
       "2   19         6646  \n",
       "3   02         1821  \n",
       "4   03         8919  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41674451",
   "metadata": {},
   "source": [
    "文本處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f0315b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:20: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:21: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\4085094747.py:12: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  return re.split('\\s+', text.strip())\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\4085094747.py:18: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  text = re.sub(\"([\\w]+)'[\\w]+\",\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\4085094747.py:20: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  text = re.sub('\\.', '', text)\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\4085094747.py:21: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  text = re.sub('[^\\w]+', ' ', text)\n",
      "C:\\Users\\11958\\AppData\\Local\\Temp\\ipykernel_36084\\4085094747.py:23: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  return [wnl.lemmatize(s) for s in re.split('\\s+', text.strip())]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\11958\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\11958\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    return re.split('\\s+', text.strip())\n",
    "\n",
    "\n",
    "def tokenizer_wnl(text):\n",
    "    if type(text) == np.ndarray:\n",
    "        text = text[0]\n",
    "    text = re.sub(\"([\\w]+)'[\\w]+\",\n",
    "                  (lambda match_obj: match_obj.group(1)), text)\n",
    "    text = re.sub('\\.', '', text)\n",
    "    text = re.sub('[^\\w]+', ' ', text)\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(s) for s in re.split('\\s+', text.strip())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8299e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 先检查数据结构\n",
    "print(\"df_copy columns:\", df_copy.columns.tolist())\n",
    "print(\"df_copy shape:\", df_copy.shape)\n",
    "print(\"Sample data:\")\n",
    "print(\"Author (col 0):\", df_copy.iloc[0, 0])\n",
    "print(\"Topic (col 1):\", df_copy.iloc[0, 1])\n",
    "\n",
    "# 确定数值列的索引（排除Author和Topic）\n",
    "numeric_cols = [i for i, col in enumerate(df_copy.columns) if col not in ['Author', 'Topic']]\n",
    "print(\"Numeric column indices:\", numeric_cols)\n",
    "\n",
    "trans_forest = ColumnTransformer(\n",
    "    [('Author', CountVectorizer(tokenizer=tokenizer, lowercase=False), [0]),\n",
    "     ('Topic', CountVectorizer(tokenizer=tokenizer_wnl, lowercase=False), [1]),\n",
    "     ('numeric', 'passthrough', numeric_cols)],  # 明确指定数值列\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "trans_other = ColumnTransformer(\n",
    "    [('Topic', CountVectorizer(tokenizer=tokenizer_wnl, lowercase=False), [1]),\n",
    "     ('numeric', 'passthrough', numeric_cols)],  # 明确指定数值列，排除Author\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca14d33f",
   "metadata": {},
   "source": [
    "訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46a22cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_raw = df_copy.values[:df_train.shape[0]]\n",
    "y_train_raw = (df_train['Popularity'].values == 1).astype(int)\n",
    "X_test = df_copy.values[df_train.shape[0]:]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_raw, y_train_raw, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a4d5cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def training(clf):\n",
    "    cv_results = cross_validate(clf, X_train_raw, y_train_raw,\n",
    "                                scoring='roc_auc', return_train_score=True, return_estimator=True)\n",
    "    print('train score: {:.5f} (+/-{:.5f})'.format(\n",
    "        np.mean(cv_results['train_score']), np.std(cv_results['train_score'])))\n",
    "    print('valid score: {:.5f} (+/-{:.5f})'.format(\n",
    "        np.mean(cv_results['test_score']), np.std(cv_results['test_score'])))\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    print('train score: {:.5f}'.format(roc_auc_score(\n",
    "        y_train, clf.predict_proba(X_train)[:, 1])))\n",
    "    print('valid score: {:.5f}'.format(roc_auc_score(\n",
    "        y_valid, clf.predict_proba(X_valid)[:, 1])))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39062d33",
   "metadata": {},
   "source": [
    "LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96fb7947",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1131, in _hstack\n    check_array(X, accept_sparse=True, ensure_all_finite=False)\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1053, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 757, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'istanbul_protests middle_east world'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 655, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1026, in fit_transform\n    return self._hstack(list(Xs), n_samples=n_samples)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1135, in _hstack\n    raise ValueError(\nValueError: For a sparse output, all columns should be a numeric or convertible to a numeric.\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1131, in _hstack\n    check_array(X, accept_sparse=True, ensure_all_finite=False)\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1053, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 757, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'asteroid asteroids challenge earth space u.s. world'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 655, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1026, in fit_transform\n    return self._hstack(list(Xs), n_samples=n_samples)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1135, in _hstack\n    raise ValueError(\nValueError: For a sparse output, all columns should be a numeric or convertible to a numeric.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LGBMClassifier\n\u001b[32m      4\u001b[39m lgbm = Pipeline([(\u001b[33m'\u001b[39m\u001b[33mct\u001b[39m\u001b[33m'\u001b[39m, trans_other),\n\u001b[32m      5\u001b[39m                  (\u001b[33m'\u001b[39m\u001b[33mclf\u001b[39m\u001b[33m'\u001b[39m, LGBMClassifier(random_state=\u001b[32m0\u001b[39m, learning_rate=\u001b[32m0.009\u001b[39m, n_estimators=\u001b[32m300\u001b[39m))])\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m lgbm = \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlgbm\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtraining\u001b[39m\u001b[34m(clf)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining\u001b[39m(clf):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mroc_auc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mtrain score: \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[33m (+/-\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m.format(\n\u001b[32m      9\u001b[39m         np.mean(cv_results[\u001b[33m'\u001b[39m\u001b[33mtrain_score\u001b[39m\u001b[33m'\u001b[39m]), np.std(cv_results[\u001b[33m'\u001b[39m\u001b[33mtrain_score\u001b[39m\u001b[33m'\u001b[39m])))\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mvalid score: \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[33m (+/-\u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m.format(\n\u001b[32m     11\u001b[39m         np.mean(cv_results[\u001b[33m'\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m'\u001b[39m]), np.std(cv_results[\u001b[33m'\u001b[39m\u001b[33mtest_score\u001b[39m\u001b[33m'\u001b[39m])))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:419\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    398\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    399\u001b[39m results = parallel(\n\u001b[32m    400\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    401\u001b[39m         clone(estimator),\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    417\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:505\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    499\u001b[39m     all_fits_failed_message = (\n\u001b[32m    500\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    508\u001b[39m     some_fits_failed_message = (\n\u001b[32m    509\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    510\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    514\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1131, in _hstack\n    check_array(X, accept_sparse=True, ensure_all_finite=False)\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1053, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 757, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'istanbul_protests middle_east world'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 655, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1026, in fit_transform\n    return self._hstack(list(Xs), n_samples=n_samples)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1135, in _hstack\n    raise ValueError(\nValueError: For a sparse output, all columns should be a numeric or convertible to a numeric.\n\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1131, in _hstack\n    check_array(X, accept_sparse=True, ensure_all_finite=False)\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1053, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 757, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: 'asteroid asteroids challenge earth space u.s. world'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 655, in fit\n    Xt = self._fit(X, y, routed_params, raw_params=params)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\joblib\\memory.py\", line 326, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1026, in fit_transform\n    return self._hstack(list(Xs), n_samples=n_samples)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py\", line 1135, in _hstack\n    raise ValueError(\nValueError: For a sparse output, all columns should be a numeric or convertible to a numeric.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm = Pipeline([('ct', trans_other),\n",
    "                 ('clf', LGBMClassifier(random_state=0, learning_rate=0.009, n_estimators=300))])\n",
    "lgbm = training(lgbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9b463f",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63469f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.00000 (+/-0.00000)\n",
      "valid score: 0.58552 (+/-0.00989)\n",
      "train score: 1.00000\n",
      "valid score: 0.58378\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = Pipeline([('ct', trans_forest),\n",
    "                   ('clf', RandomForestClassifier(n_jobs=-1, random_state=0, n_estimators=300))])\n",
    "forest = training(forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e72a8ce",
   "metadata": {},
   "source": [
    "XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3aedd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.81282 (+/-0.00500)\n",
      "valid score: 0.58232 (+/-0.01309)\n",
      "train score: 0.81618\n",
      "valid score: 0.57805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgboost = Pipeline([('ct', trans_other),\n",
    "                    ('clf', XGBClassifier(verbosity=0, n_estimators=300))])\n",
    "xgboost = training(xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38e02e",
   "metadata": {},
   "source": [
    "CatBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2003e65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.68761 (+/-0.00276)\n",
      "valid score: 0.59823 (+/-0.00827)\n",
      "train score: 0.68671\n",
      "valid score: 0.58746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost = Pipeline([('ct', trans_other),\n",
    "                     ('clf', CatBoostClassifier(verbose=False, eval_metric='AUC', n_estimators=290, learning_rate=0.06))])\n",
    "catboost = training(catboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878e18b3",
   "metadata": {},
   "source": [
    "投票"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02241a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10906, number of negative: 11208\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011876 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1575\n",
      "[LightGBM] [Info] Number of data points in the train set: 22114, number of used features: 596\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493172 -> initscore=-0.027315\n",
      "[LightGBM] [Info] Start training from score -0.027315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10905, number of negative: 11209\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027045 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1550\n",
      "[LightGBM] [Info] Number of data points in the train set: 22114, number of used features: 584\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493127 -> initscore=-0.027496\n",
      "[LightGBM] [Info] Start training from score -0.027496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10905, number of negative: 11209\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020836 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1552\n",
      "[LightGBM] [Info] Number of data points in the train set: 22114, number of used features: 587\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493127 -> initscore=-0.027496\n",
      "[LightGBM] [Info] Start training from score -0.027496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10906, number of negative: 11209\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.016203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1557\n",
      "[LightGBM] [Info] Number of data points in the train set: 22115, number of used features: 588\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493149 -> initscore=-0.027404\n",
      "[LightGBM] [Info] Start training from score -0.027404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10906, number of negative: 11209\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012411 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1558\n",
      "[LightGBM] [Info] Number of data points in the train set: 22115, number of used features: 587\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.493149 -> initscore=-0.027404\n",
      "[LightGBM] [Info] Start training from score -0.027404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.78434 (+/-0.00449)\n",
      "valid score: 0.60368 (+/-0.00829)\n",
      "[LightGBM] [Info] Number of positive: 10885, number of negative: 11229\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020727 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1567\n",
      "[LightGBM] [Info] Number of data points in the train set: 22114, number of used features: 592\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.492222 -> initscore=-0.031114\n",
      "[LightGBM] [Info] Start training from score -0.031114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.77807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid score: 0.59836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "voting = VotingClassifier([('lgbm', lgbm), ('forest', forest), ('catboost', catboost)],\n",
    "                          voting='soft', weights=[1, 0.05, 0.05])\n",
    "voting = training(voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d49966d",
   "metadata": {},
   "source": [
    "Testing Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b172e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\11958\\.conda\\envs\\news_pred\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "best_model = voting\n",
    "\n",
    "y_score = best_model.predict_proba(X_test)[:, 1]\n",
    "df_pred = pd.DataFrame({'Id': df_test['Id'], 'Popularity': y_score})\n",
    "df_pred.to_csv('submission_103.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_pred",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
