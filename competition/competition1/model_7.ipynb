{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf61e8cc",
   "metadata": {},
   "source": [
    "前置設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406a7404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\11958\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\11958\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 基礎 =====\n",
    "import os, re, numpy as np, pandas as pd\n",
    "from scipy import sparse as sp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "\n",
    "# 其他\n",
    "from joblib import Parallel, delayed\n",
    "from math import ceil\n",
    "\n",
    "# ===== 路徑與隨機種子 =====\n",
    "DATA_DIR = './dataset'\n",
    "TRAIN_PATH = f'{DATA_DIR}/train.csv'\n",
    "TEST_PATH  = f'{DATA_DIR}/test.csv'\n",
    "OUT_DIR = './output'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===== 速度開關（訓練時可先關慢模塊）=====\n",
    "FAST_NO_NER = True        # 關 NER\n",
    "FAST_NO_LDA = True        # 關 LDA 主題\n",
    "FAST_NO_TEXTSTAT = True   # 關可讀性評分\n",
    "\n",
    "# ===== NLTK 資源（缺什麼補什麼）=====\n",
    "for rid, name in [\n",
    "    ('tokenizers/punkt', 'punkt'),\n",
    "    ('corpora/stopwords', 'stopwords'),\n",
    "    ('corpora/wordnet', 'wordnet'),\n",
    "    ('sentiment/vader_lexicon', 'vader_lexicon'),\n",
    "    ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),\n",
    "    ('chunkers/maxent_ne_chunker', 'maxent_ne_chunker'),\n",
    "    ('corpora/words', 'words'),\n",
    "]:\n",
    "    try:\n",
    "        nltk.data.find(rid)\n",
    "    except LookupError:\n",
    "        nltk.download(name)\n",
    "\n",
    "# 全局 VADER（避免每條樣本重建）\n",
    "try:\n",
    "    VADER = SentimentIntensityAnalyzer()\n",
    "except Exception:\n",
    "    VADER = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa594143",
   "metadata": {},
   "source": [
    "導入數據與切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e20af56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26000, 3) (1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content\n",
       "0   0           0  <html><head><div class=\"article-info\"> <span c...\n",
       "1   1           1  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將 Popularity {-1,1} → {0,1}；並切出 train/val\n",
    "df_all = pd.read_csv(TRAIN_PATH)\n",
    "df_all['Popularity'] = (df_all['Popularity'].astype(int) == 1).astype(int)\n",
    "\n",
    "TRAIN_SIZE = 26000\n",
    "VAL_SIZE   = 1000\n",
    "\n",
    "train_df = df_all.iloc[:TRAIN_SIZE].reset_index(drop=True)\n",
    "val_df   = df_all.iloc[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE].reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape, val_df.shape)\n",
    "train_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eeaefd",
   "metadata": {},
   "source": [
    "工具函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f64080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA skipped by FAST_NO_LDA=True\n"
     ]
    }
   ],
   "source": [
    "# 小工具\n",
    "_BODY_OPEN = re.compile(\n",
    "    r'(?is)<\\s*(section|div|article)\\b[^>]*\\b'\n",
    "    r'(?:article-content|article-body|content-body|post-content)\\b[^>]*>'\n",
    ")\n",
    "_MONTH = dict(jan='01', feb='02', mar='03', apr='04', may='05', jun='06',\n",
    "              jul='07', aug='08', sep='09', oct='10', nov='11', dec='12')\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r'[\\W]+', ' ', (s or '').lower()).strip()\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    return re.sub(r'[^a-z0-9_]+', '', _norm(s).replace(' ', '_'))\n",
    "\n",
    "def _bucket(n, edges):\n",
    "    if n is None: return 'unk'\n",
    "    for i in range(len(edges)-1):\n",
    "        if edges[i] <= n < edges[i+1]:\n",
    "            return f\"b{edges[i]}_{edges[i+1]}\"\n",
    "    return f\"b{edges[-1]}p\"\n",
    "\n",
    "def _aspect_bucket(w, h):\n",
    "    if not w or not h: return 'unk'\n",
    "    r = w / h\n",
    "    if r < 0.9: return 'tall'\n",
    "    if r < 1.2: return 'squareish'\n",
    "    if r < 1.8: return 'landscape'\n",
    "    return 'ultrawide'\n",
    "\n",
    "def _img_size_bucket(w, h):\n",
    "    if not w or not h: return 'unk'\n",
    "    area = (w or 0) * (h or 0)\n",
    "    if area < 80_000: return 'xs'\n",
    "    if area < 230_000: return 'sm'\n",
    "    if area < 920_000: return 'md'\n",
    "    if area < 2_100_000: return 'lg'\n",
    "    return 'xl'\n",
    "\n",
    "def _parse_wh_from_src(src: str):\n",
    "    if not src: return (None, None)\n",
    "    m = re.search(r'/(\\d{2,5})x(\\d{2,5})/', src)\n",
    "    return (int(m.group(1)), int(m.group(2))) if m else (None, None)\n",
    "\n",
    "TRENDING_TOPICS = {\n",
    "    'elon_musk', 'ai', 'climate_change', 'covid', 'blockchain', 'taiwan',\n",
    "    'tesla', 'space', 'crypto', 'elections'\n",
    "}\n",
    "\n",
    "# LDA（可選）\n",
    "def pretrain_lda(df, column='Page content', n_components=10, max_features=1000, max_text_len=500):\n",
    "    def extract_text(html):\n",
    "        if not isinstance(html, str) or not html.strip(): return \"\"\n",
    "        m = _BODY_OPEN.search(html)\n",
    "        header_html = html[:m.start()] if m else html\n",
    "        soup = BeautifulSoup(header_html, 'html.parser')\n",
    "        return ' '.join(soup.get_text().lower().split()[:max_text_len])\n",
    "\n",
    "    corpus = [extract_text(x) for x in df[column].astype(str)]\n",
    "    if not any(corpus): return None, None\n",
    "    vec = CountVectorizer(max_features=max_features, stop_words='english')\n",
    "    X = vec.fit_transform(corpus)\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, random_state=42)\n",
    "    lda.fit(X)\n",
    "    return vec, lda\n",
    "\n",
    "lda_vectorizer, lda_model = (None, None)\n",
    "if not FAST_NO_LDA:\n",
    "    _df_lda = df_all.dropna(subset=['Page content']).astype({'Page content':'str'})\n",
    "    lda_vectorizer, lda_model = pretrain_lda(_df_lda, 'Page content', n_components=10, max_features=1000)\n",
    "    print(\"LDA pretrained\")\n",
    "else:\n",
    "    print(\"LDA skipped by FAST_NO_LDA=True\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56102f5",
   "metadata": {},
   "source": [
    "預處理函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6077bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "#  News Preprocessing + Feature Blocks (6‑tuple) — 对齐 Mashable 61 特征\n",
    "# =========================\n",
    "\n",
    "# ---- Imports ----\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "from bs4 import BeautifulSoup\n",
    "from joblib import Parallel, delayed\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# sklearn\n",
    "from sklearn.feature_extraction.text import HashingVectorizer, FeatureHasher\n",
    "\n",
    "# nltk（带安全降级）\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk import pos_tag\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt', quiet=True)\n",
    "    try:\n",
    "        nltk.data.find('taggers/averaged_perceptron_tagger')\n",
    "    except LookupError:\n",
    "        nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    try:\n",
    "        nltk.data.find('chunkers/maxent_ne_chunker')\n",
    "        nltk.data.find('corpora/words')\n",
    "        from nltk import ne_chunk\n",
    "    except LookupError:\n",
    "        nltk.download('maxent_ne_chunker', quiet=True)\n",
    "        nltk.download('words', quiet=True)\n",
    "        from nltk import ne_chunk\n",
    "    try:\n",
    "        from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "        nltk.data.find('sentiment/vader_lexicon')\n",
    "    except LookupError:\n",
    "        nltk.download('vader_lexicon', quiet=True)\n",
    "    _NLTK_OK = True\n",
    "except Exception:\n",
    "    _NLTK_OK = False\n",
    "    stopwords = None\n",
    "    PorterStemmer = None\n",
    "    pos_tag = None\n",
    "    word_tokenize = None\n",
    "    ne_chunk = None\n",
    "    SentimentIntensityAnalyzer = None\n",
    "\n",
    "# ---- 全局开关 ----\n",
    "FAST_NO_NER = True      # True 可关闭 NER\n",
    "FAST_NO_TEXTSTAT = True # True 可关闭 textstat 可读性\n",
    "\n",
    "# ---- 可读性：textstat（可选）----\n",
    "if not FAST_NO_TEXTSTAT:\n",
    "    try:\n",
    "        from textstat import flesch_reading_ease\n",
    "        _HAS_TEXTSTAT = True\n",
    "    except Exception:\n",
    "        _HAS_TEXTSTAT = False\n",
    "else:\n",
    "    _HAS_TEXTSTAT = False\n",
    "\n",
    "# ---- 停用词与词干 ----\n",
    "if _NLTK_OK and stopwords is not None:\n",
    "    try:\n",
    "        STOP = set(stopwords.words('english'))\n",
    "    except LookupError:\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "        try:\n",
    "            STOP = set(stopwords.words('english'))\n",
    "        except Exception:\n",
    "            STOP = set(\"\"\"the a an and or not no to of in on for with by as at from be is are was were been being it this that these those he she they we you i if else then than when where who whom why what which into out up down over under again further here there all any both each few more most other some such nor only own same so too very can will just don don't should now\"\"\".split())\n",
    "else:\n",
    "    STOP = set(\"\"\"the a an and or not no to of in on for with by as at from be is are was were been being it this that these those he she they we you i if else then than when where who whom why what which into out up down over under again further here there all any both each few more most other some such nor only own same so too very can will just don don't should now\"\"\".split())\n",
    "\n",
    "porter = PorterStemmer() if PorterStemmer else None\n",
    "\n",
    "# ---- VADER（可选）----\n",
    "if _NLTK_OK and SentimentIntensityAnalyzer is not None:\n",
    "    try:\n",
    "        VADER = SentimentIntensityAnalyzer()\n",
    "    except Exception:\n",
    "        VADER = None\n",
    "else:\n",
    "    VADER = None\n",
    "\n",
    "# ---- 月份映射 ----\n",
    "_MONTH = dict(\n",
    "    jan='01', feb='02', mar='03', apr='04', may='05', jun='06',\n",
    "    jul='07', aug='08', sep='09', oct='10', nov='11', dec='12'\n",
    ")\n",
    "\n",
    "# ---- BODY 起点检测 ----\n",
    "_BODY_OPEN = re.compile(\n",
    "    r'(?is)<\\s*(section|div|article)\\b[^>]*\\b(article-content|article-body|content-body|post-content)\\b[^>]*>'\n",
    ")\n",
    "\n",
    "# ---- 工具函数 ----\n",
    "def _norm(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    return ' '.join(s.lower().split())\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = re.sub(r'[^a-z0-9]+', '-', s).strip('-')\n",
    "    return s or \"unknown\"\n",
    "\n",
    "def _bucket(x, cuts):\n",
    "    if x is None: return \"b0\"\n",
    "    try:\n",
    "        for i, c in enumerate(cuts):\n",
    "            if x <= c: return f\"b{i}\"\n",
    "        return f\"b{len(cuts)}\"\n",
    "    except Exception:\n",
    "        return \"b0\"\n",
    "\n",
    "def _parse_wh_from_src(src: str):\n",
    "    if not src: return (None, None)\n",
    "    m = re.search(r'(\\d{2,5})[xX](\\d{2,5})', src)\n",
    "    if m: return int(m.group(1)), int(m.group(2))\n",
    "    m = re.search(r'[?&]w=(\\d{2,5}).*?[?&]h=(\\d{2,5})', src)\n",
    "    if m: return int(m.group(1)), int(m.group(2))\n",
    "    return (None, None)\n",
    "\n",
    "def _img_size_bucket(w, h):\n",
    "    if not w or not h: return \"unk\"\n",
    "    area = w * h\n",
    "    if area < 160*160: return \"tiny\"\n",
    "    if area < 320*320: return \"small\"\n",
    "    if area < 640*640: return \"mid\"\n",
    "    if area < 1280*720: return \"large\"\n",
    "    return \"hero\"\n",
    "\n",
    "def _aspect_bucket(w, h):\n",
    "    if not w or not h: return \"unk\"\n",
    "    r = w / float(h)\n",
    "    if 0.9 <= r <= 1.1: return \"square\"\n",
    "    if r > 1.6: return \"wide\"\n",
    "    if r < 0.6: return \"tall\"\n",
    "    return \"std\"\n",
    "\n",
    "def _safe_div(a, b, eps=1e-9):\n",
    "    return float(a) / float(b + eps)\n",
    "\n",
    "def _stem_list(text, STOP, porter):\n",
    "    if not text: return []\n",
    "    toks = [w for w in re.findall(r'[A-Za-z]+', text.lower()) if w not in STOP]\n",
    "    return [porter.stem(w) for w in toks] if porter else toks\n",
    "\n",
    "def _sent_split(txt: str):\n",
    "    if not txt: return []\n",
    "    parts = re.split(r'[.!?]+', txt)\n",
    "    return [p.strip() for p in parts if p.strip()]\n",
    "\n",
    "def _extract_canonical_domain(soup: BeautifulSoup):\n",
    "    url = None\n",
    "    link = soup.find('link', rel=lambda v: v and 'canonical' in v.lower())\n",
    "    if link and link.get('href'): url = link['href']\n",
    "    if not url:\n",
    "        og = soup.find('meta', property='og:url')\n",
    "        if og and og.get('content'): url = og['content']\n",
    "    if not url:\n",
    "        # 最后尝试 article:url / twitter:url\n",
    "        for prop in ['article:url', 'twitter:url']:\n",
    "            m = soup.find('meta', property=prop)\n",
    "            if m and m.get('content'): url = m['content']; break\n",
    "    if not url: return None, None\n",
    "    try:\n",
    "        p = urlparse(url)\n",
    "        host = (p.netloc or '').lower()\n",
    "        path = (p.path or '/')\n",
    "        depth = max(0, len([seg for seg in path.split('/') if seg]))\n",
    "        return host, depth\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "# ---- 点击诱饵/语气/数字检测 ----\n",
    "CLICKBAIT_PATTERNS = [\n",
    "    (r\"\\bhow to\\b\", \"howto\"),\n",
    "    (r\"\\bwhat happens when\\b|\\byou won't believe\\b|\\bcan't believe\\b\", \"shock_believe\"),\n",
    "    (r\"\\b\\d+\\s+(ways|reasons)\\b\", \"list_reason\"),\n",
    "    (r\"\\bexplained\\b|\\bq&a\\b|\\btimeline\\b\", \"explainer\"),\n",
    "    (r\"\\blive updates?\\b|\\blive blog\\b\", \"live\"),\n",
    "    (r\"\\bvs\\.?\\b|\\bversus\\b\", \"vs\"),\n",
    "    (r\"\\b(opinion|op\\-ed|editorial)\\b\", \"opinion\"),\n",
    "]\n",
    "\n",
    "HEDGE_WORDS = {\"may\",\"might\",\"could\",\"possibly\",\"reportedly\",\"allegedly\",\"appears\",\"suggests\",\"seems\",\"likely\"}\n",
    "ASSERTIVE_WORDS = {\"will\",\"must\",\"definitely\",\"confirmed\",\"official\",\"guaranteed\",\"undeniably\"}\n",
    "CURRENCY_SYMS = \"$€£¥\"\n",
    "CURRENCY_WORDS = {\"usd\",\"euro\",\"eur\",\"dollar\",\"dollars\",\"pound\",\"yen\",\"rmb\",\"cny\"}\n",
    "PERCENT_RE = re.compile(r\"\\b\\d+(?:\\.\\d+)?\\s*%\")\n",
    "MONEY_RE   = re.compile(rf\"[{re.escape(CURRENCY_SYMS)}]\\s*\\d[\\d,]*(?:\\.\\d+)?|\\b\\d[\\d,]*(?:\\.\\d+)?\\s*(?:{'|'.join(CURRENCY_WORDS)})\\b\", re.I)\n",
    "NUMBER_RE  = re.compile(r\"\\b\\d+(?:\\.\\d+)?\\b\")\n",
    "\n",
    "# ---- 词袋清洗 ----\n",
    "def _bow_clean(txt: str) -> str:\n",
    "    toks = [w for w in re.findall(r'[A-Za-z]+', (txt or '').lower()) if w not in STOP]\n",
    "    return ' '.join(porter.stem(w) for w in toks) if porter else ' '.join(toks)\n",
    "\n",
    "# ---- Tokenizer（保留元数据规则）----\n",
    "def tokenizer_stem_keepmeta(text: str, entities: set) -> list:\n",
    "    toks = re.split(r'\\s+', (text or '').strip()); out=[]\n",
    "    for w in toks:\n",
    "        if not w: continue\n",
    "        if '_' in w or any(ch.isdigit() for ch in w) or (w.startswith('entity_') and w[7:] in entities):\n",
    "            out.append(w)\n",
    "        elif w.lower() not in STOP and re.fullmatch(r'[a-zA-Z]+', w):\n",
    "            out.append(porter.stem(w.lower()) if porter else w.lower())\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "#     改进版 preprocessor（对齐 Mashable 61 特征）\n",
    "# =========================\n",
    "def preprocessor(html: str, lda_vectorizer=None, lda_model=None, max_text_len=500):\n",
    "    if not isinstance(html, str) or not html.strip():\n",
    "        return \"empty_content\", set(), \"\", \"\", \"\", \"\"\n",
    "\n",
    "    # --- header/body 切分 ---\n",
    "    m = _BODY_OPEN.search(html)\n",
    "    if m:\n",
    "        header_html = html[:m.start()]\n",
    "        body_html = html[m.start():]\n",
    "    else:\n",
    "        header_html = html\n",
    "        body_html = \"\"\n",
    "\n",
    "    soup = BeautifulSoup(header_html, 'html.parser')\n",
    "\n",
    "    # --- URL/Domain/Depth ---\n",
    "    domain, url_depth = _extract_canonical_domain(soup)\n",
    "\n",
    "    # --- 标题 ---\n",
    "    title_raw = None\n",
    "    h1 = soup.find('h1', class_=lambda c: (isinstance(c, list) and any('title' in x for x in c)) or (isinstance(c, str) and 'title' in c)) \\\n",
    "         or soup.find('h1')\n",
    "    if h1: title_raw = h1.get_text(' ', strip=True)\n",
    "    elif soup.title: title_raw = soup.title.get_text(' ', strip=True)\n",
    "    title_tokens_norm = _norm(title_raw)\n",
    "    title_words = [w for w in re.findall(r\"[A-Za-z]+\", title_tokens_norm)]\n",
    "    n_tokens_title = len(title_words)\n",
    "    avg_title_tok_len = _safe_div(sum(len(w) for w in title_words), max(1, n_tokens_title))\n",
    "\n",
    "    # --- 作者 / 频道 / 发布者 ---\n",
    "    author = None\n",
    "    by = soup.find(class_=lambda c: c and ('byline' in c or 'author_name' in c))\n",
    "    if by: author = by.get_text(' ', strip=True)\n",
    "    if not author:\n",
    "        a = soup.find('a', href=re.compile(r'/author/[^/]+/?$', re.I))\n",
    "        if a: author = a.get_text(' ', strip=True)\n",
    "    author_slug = _slug(re.sub(r'^\\s*by\\s+', '', author or '', flags=re.I))\n",
    "\n",
    "    channel = None\n",
    "    art = soup.find('article')\n",
    "    if art and art.has_attr('data-channel'): channel = art['data-channel']\n",
    "    if not channel and art:\n",
    "        cls = ' '.join(art.get('class', []))\n",
    "        mch = re.search(r'\\b(news|tech|world|sports?|business|entertainment|culture|life|science|lifestyle|socmed)\\b', cls, re.I)\n",
    "        if mch: channel = mch.group(1)\n",
    "    channel_slug = _slug(channel or 'unknown')\n",
    "\n",
    "    publisher = None\n",
    "    pub = soup.find('a', href=re.compile(r'/publishers/[^/]+/?', re.I))\n",
    "    if pub:\n",
    "        publisher = pub.get_text(' ', strip=True) or re.sub(r'.*/publishers/([^/]+)/?.*', r'\\1', pub['href'], flags=re.I)\n",
    "    publisher_slug = _slug(publisher or 'unknown')\n",
    "\n",
    "    # --- 时间 ---\n",
    "    year = month = weekday = tod = season = None\n",
    "    is_weekend = None\n",
    "    tm = soup.find('time')\n",
    "    dt = tm['datetime'] if (tm and tm.has_attr('datetime')) else (tm.get_text(' ', strip=True) if tm else None)\n",
    "    if dt:\n",
    "        y = re.search(r'(20\\d{2}|19\\d{2})', dt);  year = y.group(1) if y else None\n",
    "        mo = re.search(r'-(\\d{2})-', dt) or re.search(r'\\b(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\b', dt, re.I)\n",
    "        if mo:\n",
    "            mm = mo.group(1).lower() if mo.lastindex else mo.group(0).lower()\n",
    "            month = _MONTH.get(mm, mm)\n",
    "        wd = re.search(r'\\b(mon|tue|wed|thu|fri|sat|sun)\\b', dt, re.I)\n",
    "        if wd:\n",
    "            weekday = wd.group(1).lower(); is_weekend = weekday in ('sat','sun')\n",
    "        hh = re.search(r'\\b(\\d{2}):(\\d{2})', dt)\n",
    "        if hh:\n",
    "            h = int(hh.group(1))\n",
    "            tod = 'morning' if 5<=h<12 else 'afternoon' if 12<=h<17 else 'evening' if 17<=h<22 else 'night'\n",
    "        if month:\n",
    "            m_i = int(month)\n",
    "            season = 'spring' if 3<=m_i<=5 else 'summer' if 6<=m_i<=8 else 'autumn' if 9<=m_i<=11 else 'winter'\n",
    "\n",
    "    # weekday flags（模仿 weekday_is_*）\n",
    "    weekday_flags = []\n",
    "    if weekday:\n",
    "        wd_map = dict(mon='monday', tue='tuesday', wed='wednesday', thu='thursday', fri='friday', sat='saturday', sun='sunday')\n",
    "        for k,v in wd_map.items():\n",
    "            weekday_flags.append(f\"weekday_is_{v}\" if weekday==k else f\"weekday_is_{v}=0\")\n",
    "\n",
    "    # --- 媒体元素 + 链接 ---\n",
    "    imgs = soup.find_all('img'); img_count = len(imgs); has_image = img_count > 0\n",
    "    leadimg = soup.find(attrs={'data-fragment':'lead-image'}) is not None\n",
    "    max_w=max_h=None\n",
    "    for im in imgs:\n",
    "        w,h = _parse_wh_from_src(im.get('src',''))\n",
    "        if w and h and ((not max_w) or (w*h) > ((max_w or 0)*(max_h or 0))):\n",
    "            max_w,max_h = w,h\n",
    "    img_size_bucket = _img_size_bucket(max_w, max_h)\n",
    "    img_aspect_bucket = _aspect_bucket(max_w, max_h)\n",
    "    videos = soup.find_all('video'); iframes = soup.find_all('iframe')\n",
    "    # iframe 中判定第三方视频\n",
    "    video_iframes = [fr for fr in iframes if re.search(r'(youtube|vimeo|dailymotion)', (fr.get('src') or ''), re.I)]\n",
    "    num_videos = len(videos) + len(video_iframes)\n",
    "    audio = soup.find_all('audio'); has_audio = len(audio)>0\n",
    "\n",
    "    a_tags_header = soup.find_all('a')\n",
    "    link_count = len(a_tags_header)\n",
    "    link_bucket = _bucket(link_count, [0,1,3,6,10])\n",
    "\n",
    "    domain_self_cnt = 0\n",
    "    if domain:\n",
    "        for a in a_tags_header:\n",
    "            href = (a.get('href') or '').strip()\n",
    "            if href.startswith('http'):\n",
    "                try:\n",
    "                    host = urlparse(href).netloc.lower()\n",
    "                    if host and domain and (host.endswith(domain) or domain.endswith(host)):\n",
    "                        domain_self_cnt += 1\n",
    "                except Exception:\n",
    "                    pass\n",
    "    num_self_hrefs = domain_self_cnt\n",
    "    self_ref_ratio = _safe_div(num_self_hrefs, max(1, link_count))\n",
    "\n",
    "    authoritative_domains = ['.edu','.gov','.org']\n",
    "    authoritative_links = sum(1 for a in a_tags_header if any(d in (a.get('href') or '').lower() for d in authoritative_domains))\n",
    "    authoritative_link_bucket = _bucket(authoritative_links, [0,1,3,5])\n",
    "\n",
    "    # --- 标题形态 + 点击诱饵 ---\n",
    "    raw = title_raw or ''\n",
    "    raw_l = raw.lower()\n",
    "    title_has_num = bool(re.search(r'\\d', raw))\n",
    "    title_has_year = bool(re.search(r'\\b(19|20)\\d{2}\\b', raw))\n",
    "    title_has_q = '?' in raw\n",
    "    title_has_exclaim = '!' in raw\n",
    "    title_has_colon = ':' in raw\n",
    "    is_listicle = bool(re.match(r'^\\s*\\d+', raw))\n",
    "    upper_ratio = (sum(ch.isupper() for ch in raw) / max(1, sum(ch.isalpha() for ch in raw)))\n",
    "    upper_bucket = 'low' if upper_ratio < 0.15 else 'mid' if upper_ratio < 0.4 else 'high'\n",
    "    title_word_len = n_tokens_title\n",
    "    tw_bucket = _bucket(title_word_len, [0,4,8,12,20])\n",
    "    title_char_len = len(re.sub(r'\\s+','',raw)); tc_bucket = _bucket(title_char_len, [0,30,60,90,140])\n",
    "\n",
    "    title_clickbait_tags = []\n",
    "    for pat, tag in CLICKBAIT_PATTERNS:\n",
    "        if re.search(pat, raw_l):\n",
    "            title_clickbait_tags.append(f\"title_clickbait_{tag}\")\n",
    "    if not title_clickbait_tags:\n",
    "        title_clickbait_tags.append(\"title_clickbait_none\")\n",
    "\n",
    "    # --- header 文本内容 + 情感 ---\n",
    "    text_content = ' '.join(soup.get_text().lower().split()[:max_text_len])\n",
    "\n",
    "    # VADER 标题/头部情感\n",
    "    if VADER is not None:\n",
    "        vader_header = VADER.polarity_scores(text_content)\n",
    "        vader_title  = VADER.polarity_scores(title_raw or '')\n",
    "        header_compound = vader_header['compound']\n",
    "        header_pos, header_neg, header_neu = vader_header['pos'], vader_header['neg'], vader_header['neu']\n",
    "        title_compound  = vader_title['compound']\n",
    "        title_pos, title_neg, title_neu = vader_title['pos'], vader_title['neg'], vader_title['neu']\n",
    "    else:\n",
    "        header_compound = title_compound = 0.0\n",
    "        header_pos = header_neg = header_neu = 0.0\n",
    "        title_pos  = title_neg  = title_neu  = 0.0\n",
    "\n",
    "    def _pol_bucket(x):\n",
    "        return _bucket(x, [0.0, 0.1, 0.2, 0.4, 0.6, 0.8])\n",
    "\n",
    "    # 词性标注（安全降级）\n",
    "    if _NLTK_OK and word_tokenize and pos_tag:\n",
    "        tokens = word_tokenize(text_content)\n",
    "        try:\n",
    "            tagged = pos_tag(tokens)\n",
    "        except Exception:\n",
    "            tagged = [(w,'NN') for w in tokens]\n",
    "    else:\n",
    "        tokens = text_content.split()\n",
    "        tagged = [(w,'NN') for w in tokens]\n",
    "    nouns = [w for w,pos_ in tagged if pos_.startswith('NN') and w.lower() not in STOP]\n",
    "    noun_count_bucket = _bucket(len(nouns), [0,5,10,20,50])\n",
    "\n",
    "    # --- NER（细类计数，可关） ---\n",
    "    entities = set()\n",
    "    person_cnt = org_cnt = gpe_cnt = 0\n",
    "    if not FAST_NO_NER and _NLTK_OK and ne_chunk:\n",
    "        try:\n",
    "            chunked = ne_chunk(tagged)\n",
    "            for ch in chunked:\n",
    "                if hasattr(ch,'label') and ch.label() in ['PERSON','ORGANIZATION','GPE']:\n",
    "                    ent = '_'.join(c[0].lower() for c in ch)\n",
    "                    entities.add(f'entity_{ent}')\n",
    "                    if ch.label() == 'PERSON': person_cnt += 1\n",
    "                    elif ch.label() == 'ORGANIZATION': org_cnt += 1\n",
    "                    elif ch.label() == 'GPE': gpe_cnt += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "    entity_count_bucket = _bucket(len(entities), [0,1,3,5])\n",
    "\n",
    "    # --- LDA：多 topic 分布（模仿 LDA_00..LDA_04） ---\n",
    "    lda_tokens = []\n",
    "    if (lda_vectorizer is not None) and (lda_model is not None):\n",
    "        try:\n",
    "            # 优先用正文，否则用 header\n",
    "            text_for_lda = text_content\n",
    "            if body_html:\n",
    "                soup_body_tmp = BeautifulSoup(body_html, 'html.parser')\n",
    "                text_for_lda = ' '.join(soup_body_tmp.get_text().lower().split())\n",
    "            X_ = lda_vectorizer.transform([text_for_lda] if text_for_lda else [''])\n",
    "            topic_dist = lda_model.transform(X_)\n",
    "            if topic_dist.ndim == 2 and topic_dist.shape[1] > 0:\n",
    "                K = min(topic_dist.shape[1], 5)  # 最多取5个主题，对齐 LDA_00..LDA_04\n",
    "                for i in range(K):\n",
    "                    s = float(topic_dist[0][i])\n",
    "                    lda_tokens.append(f\"LDA_{i:02d}_b{_bucket(s,[0,0.2,0.4,0.6,0.8])}\")\n",
    "                dom = int(np.argmax(topic_dist[0]))\n",
    "                lda_tokens.append(f\"LDA_dom_t{dom}\")\n",
    "            else:\n",
    "                lda_tokens.append('LDA_00_unk')\n",
    "        except Exception:\n",
    "            lda_tokens.append('LDA_00_unk')\n",
    "    else:\n",
    "        lda_tokens.append('LDA_00_unk')\n",
    "\n",
    "    # --- 可读性（可关） ---\n",
    "    if _HAS_TEXTSTAT:\n",
    "        try:\n",
    "            readability_score = flesch_reading_ease(text_content) if text_content else 0\n",
    "        except Exception:\n",
    "            readability_score = 0\n",
    "    else:\n",
    "        readability_score = 50\n",
    "    readability_bucket = ('very_easy' if readability_score>80 else\n",
    "                          'easy' if readability_score>60 else\n",
    "                          'standard' if readability_score>50 else\n",
    "                          'difficult' if readability_score>30 else 'very_difficult')\n",
    "\n",
    "    # --- header 结构统计 ---\n",
    "    div_count = len(soup.find_all('div'))\n",
    "    section_count = len(soup.find_all('section'))\n",
    "    list_count = len(soup.find_all(['ul','ol']))\n",
    "    div_count_bucket = _bucket(div_count, [0,5,10,20,50])\n",
    "    section_count_bucket = _bucket(section_count, [0,1,3,5])\n",
    "    list_count_bucket = _bucket(list_count, [0,1,3,5])\n",
    "    header_word_count_bucket = _bucket(len(text_content.split()), [0,50,100,200,500])\n",
    "\n",
    "    # ====== header 特征拼装 ======\n",
    "    feats = []\n",
    "    feats += [\n",
    "        f'author_{author_slug or \"unknown\"}',\n",
    "        f'channel_{channel_slug}', f'publisher_{publisher_slug}',\n",
    "        f'year_{year or \"unk\"}', f'month_{month or \"unk\"}',\n",
    "        f'weekday_{weekday or \"unk\"}', f'tod_{tod or \"unk\"}', f'season_{season or \"unk\"}',\n",
    "        'weekend' if is_weekend else 'weekday' if is_weekend is not None else 'weekend_unk',\n",
    "    ]\n",
    "    # weekday flags\n",
    "    feats += weekday_flags\n",
    "\n",
    "    # Mashable 61 特征的核心部分\n",
    "    feats += [\n",
    "        # 基础计数特征\n",
    "        f'n_tokens_title_{_bucket(n_tokens_title, [0,4,8,12,20,30])}',\n",
    "        f'num_hrefs_{_bucket(link_count, [0,1,3,6,10,20,40])}',\n",
    "        f'num_self_hrefs_{_bucket(num_self_hrefs, [0,1,2,3,5,10])}',\n",
    "        f'num_imgs_{_bucket(img_count, [0,1,2,4,8,16])}',\n",
    "        f'num_videos_{_bucket(num_videos, [0,1,2,3,5])}',\n",
    "        f'average_token_length_{_bucket(avg_title_tok_len, [0,3,5,7,9])}',\n",
    "        f'num_keywords_{_bucket(len(kws) if 'kws' in locals() else 0, [0,1,3,5,10,20])}',\n",
    "        \n",
    "        # 媒体特征\n",
    "        'has_image' if has_image else 'no_image',\n",
    "        'has_leadimg' if leadimg else 'no_leadimg',\n",
    "        f'imgsize_{img_size_bucket}', f'imgaspect_{img_aspect_bucket}',\n",
    "        'has_video' if num_videos>0 else 'no_video',\n",
    "        'has_audio' if has_audio else 'no_audio',\n",
    "    ]\n",
    "\n",
    "    # 频道标志（data_channel_is_*）\n",
    "    channel_map = {\n",
    "        'lifestyle': 'lifestyle', 'life': 'lifestyle',\n",
    "        'entertainment': 'entertainment', 'culture': 'entertainment',\n",
    "        'business': 'bus', 'news': 'world', 'world':'world',\n",
    "        'socmed':'socmed', 'social':'socmed',\n",
    "        'tech':'tech', 'science':'tech', 'sports':'sports', 'sport':'sports'\n",
    "    }\n",
    "    cat = channel_map.get(channel_slug, None)\n",
    "    for key in ['lifestyle','entertainment','bus','socmed','tech','world','sports']:\n",
    "        feats.append(f'data_channel_is_{key}' if cat==key else f'data_channel_is_{key}=0')\n",
    "\n",
    "    # 标题特征\n",
    "    feats += [\n",
    "        'is_listicle' if is_listicle else 'not_listicle',\n",
    "        'title_has_num' if title_has_num else 'title_no_num',\n",
    "        'title_has_year' if title_has_year else 'title_no_year',\n",
    "        'title_has_q' if title_has_q else 'title_no_q',\n",
    "        'title_has_exclaim' if title_has_exclaim else 'title_no_exclaim',\n",
    "        'title_has_colon' if title_has_colon else 'title_no_colon',\n",
    "        f'title_len_word_{tw_bucket}', f'title_len_char_{tc_bucket}', f'title_upper_{upper_bucket}',\n",
    "    ]\n",
    "\n",
    "    # 标题点击诱饵/风格\n",
    "    feats += (title_clickbait_tags or [\"title_clickbait_none\"])  \n",
    "\n",
    "    # NER / 结构 / 可读性 / 名词数\n",
    "    feats += [\n",
    "        f'ner_person_{_bucket(person_cnt,[0,1,3,6])}',\n",
    "        f'ner_org_{_bucket(org_cnt,[0,1,3,6])}',\n",
    "        f'ner_gpe_{_bucket(gpe_cnt,[0,1,3,6])}',\n",
    "        f'div_count_{div_count_bucket}', f'section_count_{section_count_bucket}',\n",
    "        f'list_count_{list_count_bucket}', f'readability_{readability_bucket}',\n",
    "        f'header_word_count_{header_word_count_bucket}',\n",
    "        f'noun_count_{noun_count_bucket}',\n",
    "    ]\n",
    "\n",
    "    # 关键词（meta keywords / article:tag）\n",
    "    kw_tokens = []\n",
    "    meta_kw = soup.find('meta', attrs={'name':'keywords'})\n",
    "    kws = []\n",
    "    if meta_kw and meta_kw.get('content'):\n",
    "        kws += [k.strip().lower() for k in meta_kw['content'].split(',') if k.strip()]\n",
    "    for meta_tag in soup.find_all('meta', attrs={'property':'article:tag'}):\n",
    "        if meta_tag.get('content'): kws.append(meta_tag['content'].strip().lower())\n",
    "    kws = [k for k in kws if k]\n",
    "    num_keywords = len(kws)\n",
    "    \n",
    "    # 关键词统计特征（模拟 kw_min_min, kw_max_min 等）\n",
    "    if kws:\n",
    "        kw_lengths = [len(kw) for kw in kws]\n",
    "        feats += [\n",
    "            f'kw_min_min_{_bucket(min(kw_lengths) if kw_lengths else 0, [0,3,5,7,10])}',\n",
    "            f'kw_max_min_{_bucket(max(kw_lengths) if kw_lengths else 0, [0,5,10,15,20])}',\n",
    "            f'kw_avg_min_{_bucket(np.mean(kw_lengths) if kw_lengths else 0, [0,3,6,9,12])}',\n",
    "        ]\n",
    "    else:\n",
    "        feats += ['kw_min_min_0', 'kw_max_min_0', 'kw_avg_min_0']\n",
    "\n",
    "    kw_set = set(kws)\n",
    "    title_set = set(w.lower() for w in title_words)\n",
    "    kw_overlap_title = _safe_div(len(kw_set & title_set), max(1, len(kw_set))) if kw_set else 0.0\n",
    "    feats += [f'keyword_overlap_title_{_bucket(int(100*kw_overlap_title), [0,10,25,50,75,90])}']\n",
    "\n",
    "    # URL 特征\n",
    "    if domain:\n",
    "        feats += [f'url_domain_{_slug(domain)}']\n",
    "    if url_depth is not None:\n",
    "        feats += [f'url_depth_{_bucket(url_depth, [0,1,2,3,5,8])}']\n",
    "\n",
    "    # 情感特征（61-like）\n",
    "    feats += [\n",
    "        f'global_subjectivity_{_bucket(header_neu, [0,0.2,0.4,0.6,0.8])}',  # 用中性词比例代理主观性\n",
    "        f'global_sentiment_polarity_{_pol_bucket(header_compound)}',\n",
    "        f'global_rate_positive_words_{_pol_bucket(header_pos)}',\n",
    "        f'global_rate_negative_words_{_pol_bucket(header_neg)}',\n",
    "        f'rate_positive_words_{_pol_bucket(_safe_div(header_pos, header_pos+header_neg))}',\n",
    "        f'rate_negative_words_{_pol_bucket(_safe_div(header_neg, header_pos+header_neg))}',\n",
    "        f'title_subjectivity_{_pol_bucket(title_neu)}',\n",
    "        f'title_sentiment_polarity_{_pol_bucket(title_compound)}',\n",
    "        f'abs_title_subjectivity_{_pol_bucket(abs(title_neu))}',\n",
    "        f'abs_title_sentiment_polarity_{_pol_bucket(abs(title_compound))}',\n",
    "    ]\n",
    "\n",
    "    # 将 LDA token 也加入 header feats\n",
    "    feats += lda_tokens\n",
    "\n",
    "    header_bow_text = _bow_clean(text_content)\n",
    "\n",
    "    # ====== 正文特征 ======\n",
    "    body_feats = []\n",
    "    body_bow_text = \"\"\n",
    "    title_stems = set(_stem_list(title_raw or \"\", STOP, porter))\n",
    "\n",
    "    if body_html:\n",
    "        soup_body = BeautifulSoup(body_html, 'html.parser')\n",
    "        body_text = ' '.join(soup_body.get_text().lower().split())\n",
    "        body_bow_text = _bow_clean(body_text)\n",
    "\n",
    "        # 基本规模\n",
    "        body_words = [w for w in re.findall(r\"[A-Za-z]+\", body_text)]\n",
    "        body_len = len(body_words)\n",
    "        \n",
    "        # Mashable 61 特征：正文相关\n",
    "        uniq = len(set(body_words))\n",
    "        n_unique_tokens = _safe_div(uniq, max(1, body_len))\n",
    "        nostop_words = [w for w in body_words if w.lower() not in STOP]\n",
    "        n_non_stop_words = _safe_div(len(nostop_words), max(1, body_len))\n",
    "        n_non_stop_unique_tokens = _safe_div(len(set(w.lower() for w in nostop_words)), max(1, body_len))\n",
    "        avg_token_length = _safe_div(sum(len(w) for w in body_words), max(1, body_len))\n",
    "\n",
    "        body_feats += [\n",
    "            f'n_tokens_content_{_bucket(body_len, [0,100,300,600,1000,2000,4000])}',\n",
    "            f'n_unique_tokens_{_bucket(int(100*n_unique_tokens), [0,20,30,40,50,60,70,80])}',\n",
    "            f'n_non_stop_words_{_bucket(int(100*n_non_stop_words), [0,20,40,60,75,90])}',\n",
    "            f'n_non_stop_unique_tokens_{_bucket(int(100*n_non_stop_unique_tokens), [0,10,20,30,40,50,60])}',\n",
    "            f'average_token_length_{_bucket(avg_token_length, [0,3,4,5,6,7,8])}',\n",
    "        ]\n",
    "\n",
    "        # 段落/引用/代码/小标题\n",
    "        p_count = len(soup_body.find_all('p'))\n",
    "        bq_count = len(soup_body.find_all('blockquote'))\n",
    "        code_count = len(soup_body.find_all(['pre','code']))\n",
    "        h2h3_count = len(soup_body.find_all(['h2','h3']))\n",
    "        body_feats += [\n",
    "            f'body_p_{_bucket(p_count,[0,5,15,30,50])}',\n",
    "            f'body_bq_{_bucket(bq_count,[0,1,3,5])}',\n",
    "            'body_has_code' if code_count>0 else 'body_no_code',\n",
    "            f'body_h2h3_{_bucket(h2h3_count,[0,1,3,6,10])}',\n",
    "        ]\n",
    "\n",
    "        # 外链比例/权威外链（正文）\n",
    "        a_tags = soup_body.find_all('a')\n",
    "        ext_links = [a for a in a_tags if re.match(r'^https?://', (a.get('href') or ''), re.I)]\n",
    "        ext_ratio = _safe_div(len(ext_links), max(1,len(a_tags)))\n",
    "        body_feats += [\n",
    "            f'links_ext_ratio_{_bucket(int(100*ext_ratio), [0,10,30,60,90])}',\n",
    "            f'links_ext_cnt_{_bucket(len(ext_links), [0,1,3,6,10,20])}',\n",
    "        ]\n",
    "        authoritative_domains = ['.edu','.gov','.org']\n",
    "        auth_links_body = sum(1 for a in ext_links if any(d in (a.get('href') or '').lower() for d in authoritative_domains))\n",
    "        body_feats.append(f'links_auth_cnt_{_bucket(auth_links_body,[0,1,3,5,10])}')\n",
    "\n",
    "        # 引号/标点/句长\n",
    "        quotes_cnt = body_text.count('\"') + body_text.count('\"') + body_text.count('\"') + body_text.count(\"'\") + body_text.count(\"'\")\n",
    "        exclaim_cnt = body_text.count('!')\n",
    "        qmark_cnt = body_text.count('?')\n",
    "        sents = _sent_split(body_text)\n",
    "        avg_sl = _safe_div(sum(len(s.split()) for s in sents), max(1,len(sents)))\n",
    "        body_feats += [\n",
    "            f'quotes_{_bucket(quotes_cnt,[0,1,3,6,10])}',\n",
    "            f'body_exclaim_{_bucket(exclaim_cnt,[0,1,2,4,8])}',\n",
    "            f'body_qmark_{_bucket(qmark_cnt,[0,1,2,4,8])}',\n",
    "            f'avg_sentence_len_{_bucket(int(avg_sl), [0,10,15,20,30,40])}'\n",
    "        ]\n",
    "\n",
    "        # 词汇多样性/阅读时长\n",
    "        lex_div = _safe_div(uniq, max(1,body_len))\n",
    "        read_min = int(round(body_len / 200.0))  # 200 wpm 粗估\n",
    "        body_feats += [\n",
    "            f'lexdiv_{_bucket(int(100*lex_div), [0,20,30,40,50,60])}',\n",
    "            f'readtime_min_{_bucket(read_min,[0,1,2,4,6,10])}'\n",
    "        ]\n",
    "\n",
    "        # 金额/百分比/数字密度\n",
    "        money_cnt = len(MONEY_RE.findall(body_text))\n",
    "        pct_cnt = len(PERCENT_RE.findall(body_text))\n",
    "        num_cnt = len(NUMBER_RE.findall(body_text))\n",
    "        body_feats += [\n",
    "            f'money_{_bucket(money_cnt,[0,1,2,5,10])}',\n",
    "            f'percent_{_bucket(pct_cnt,[0,1,2,5,10])}',\n",
    "            f'numdens_{_bucket(int(1000*_safe_div(num_cnt, max(1,body_len))), [0,5,10,20,40,80])}'\n",
    "        ]\n",
    "\n",
    "        # 情感极性统计（正文）\n",
    "        if VADER is not None:\n",
    "            vd_body = VADER.polarity_scores(body_text)\n",
    "            # 句子级情感统计\n",
    "            if sents:\n",
    "                sent_scores = [VADER.polarity_scores(s)['compound'] for s in sents]\n",
    "                pos_scores = [s for s in sent_scores if s > 0]\n",
    "                neg_scores = [s for s in sent_scores if s < 0]\n",
    "                \n",
    "                if pos_scores:\n",
    "                    body_feats += [\n",
    "                        f'avg_positive_polarity_{_pol_bucket(np.mean(pos_scores))}',\n",
    "                        f'min_positive_polarity_{_pol_bucket(min(pos_scores))}',\n",
    "                        f'max_positive_polarity_{_pol_bucket(max(pos_scores))}',\n",
    "                    ]\n",
    "                else:\n",
    "                    body_feats += ['avg_positive_polarity_0', 'min_positive_polarity_0', 'max_positive_polarity_0']\n",
    "                    \n",
    "                if neg_scores:\n",
    "                    body_feats += [\n",
    "                        f'avg_negative_polarity_{_pol_bucket(abs(np.mean(neg_scores)))}',\n",
    "                        f'min_negative_polarity_{_pol_bucket(abs(min(neg_scores)))}',\n",
    "                        f'max_negative_polarity_{_pol_bucket(abs(max(neg_scores)))}',\n",
    "                    ]\n",
    "                else:\n",
    "                    body_feats += ['avg_negative_polarity_0', 'min_negative_polarity_0', 'max_negative_polarity_0']\n",
    "            else:\n",
    "                body_feats += ['avg_positive_polarity_0', 'min_positive_polarity_0', 'max_positive_polarity_0',\n",
    "                              'avg_negative_polarity_0', 'min_negative_polarity_0', 'max_negative_polarity_0']\n",
    "\n",
    "        # 标题-正文一致性\n",
    "        body_stems = set(_stem_list(body_text, STOP, porter))\n",
    "        jacc = _safe_div(len(title_stems & body_stems), max(1, len(title_stems | body_stems)))\n",
    "        title_ents_like = set(re.findall(r'[A-Za-z][A-Za-z0-9_]+', (title_tokens_norm or '')))\n",
    "        ent_hit = _safe_div(sum(1 for t in title_ents_like if (t or \"\").lower() in body_text), max(1, len(title_ents_like)))\n",
    "        body_feats += [\n",
    "            f'align_jacc_{_bucket(int(100*jacc), [0,10,20,35,50,70])}',\n",
    "            f'align_ent_{_bucket(int(100*ent_hit), [0,10,25,50,75,90])}'\n",
    "        ]\n",
    "\n",
    "        # 图片密度\n",
    "        body_imgs = soup_body.find_all('img')\n",
    "        img_density = _safe_div(len(body_imgs), max(1, body_len/500.0))\n",
    "        body_feats.append(f'imgdens_{_bucket(int(10*img_density), [0,1,2,3,5,8])}')\n",
    "\n",
    "        # 关键词与正文重合\n",
    "        if num_keywords>0:\n",
    "            kw_hit_body = _safe_div(len(kw_set & set(w.lower() for w in body_words)), max(1, len(kw_set)))\n",
    "            body_feats.append(f'keyword_overlap_body_{_bucket(int(100*kw_hit_body), [0,10,25,50,75,90])}')\n",
    "\n",
    "    else:\n",
    "        body_feats.append('no_body')\n",
    "        # 如果没有正文，设置默认值\n",
    "        body_feats += [\n",
    "            'n_tokens_content_0', 'n_unique_tokens_0', 'n_non_stop_words_0',\n",
    "            'n_non_stop_unique_tokens_0', 'average_token_length_0'\n",
    "        ]\n",
    "\n",
    "    # === 将标准化后的标题返回 ===\n",
    "    return _norm(title_tokens_norm), entities, ' '.join(feats), _bow_clean(text_content), ' '.join(body_feats), body_bow_text\n",
    "\n",
    "# =========================\n",
    "#  向量器 / Hasher 定义 - 更新前缀映射以包含新特征\n",
    "# =========================\n",
    "\n",
    "# 词袋向量（标题 / header / 实体 / 正文）\n",
    "title_vec   = HashingVectorizer(n_features=2**20, alternate_sign=False, ngram_range=(1,2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "header_vec  = HashingVectorizer(n_features=2**18, alternate_sign=True,  ngram_range=(1,2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "entities_vec= HashingVectorizer(n_features=2**12, alternate_sign=False, token_pattern=r'(?u)\\b\\w+\\b')\n",
    "body_vec    = HashingVectorizer(n_features=2**20, alternate_sign=False, ngram_range=(1,2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "\n",
    "make_hasher = lambda n: FeatureHasher(n_features=2**n, input_type='string', alternate_sign=False)\n",
    "\n",
    "# 原有组 + 新增组\n",
    "author_h, channel_h, publisher_h = make_hasher(10), make_hasher(10), make_hasher(10)\n",
    "time_h, media_h, titleshape_h   = make_hasher(10), make_hasher(12), make_hasher(10)\n",
    "social_h, content_h, structure_h= make_hasher(14), make_hasher(14), make_hasher(14)\n",
    "length_h, extra_h               = make_hasher(12), make_hasher(14)\n",
    "bodyfeats_h                     = make_hasher(12)\n",
    "\n",
    "# Mashable 61 特征专用分组\n",
    "mashable_basic_h = make_hasher(12)   # n_tokens_title, num_hrefs 等基础特征\n",
    "mashable_channel_h = make_hasher(8)  # data_channel_is_*\n",
    "mashable_kw_h = make_hasher(11)      # kw_* 关键词特征\n",
    "mashable_sentiment_h = make_hasher(12) # 情感相关特征\n",
    "mashable_lda_h = make_hasher(10)     # LDA 主题特征\n",
    "mashable_content_h = make_hasher(12) # n_tokens_content 等正文特征\n",
    "\n",
    "# =========================\n",
    "#  前缀映射（分组哈希）- 更新以包含 Mashable 特征\n",
    "# =========================\n",
    "PREFIX_MAP = {\n",
    "    'author_':'author','channel_':'channel','publisher_':'publisher',\n",
    "    'year_':'time','month_':'time','weekday_':'time','tod_':'time','season_':'time',\n",
    "    'weekend':'time','weekday':'time',\n",
    "    'weekday_is_':'time',\n",
    "\n",
    "    'has_image':'media','no_image':'media','imgcnt_':'media','has_leadimg':'media','no_leadimg':'media',\n",
    "    'imgsize_':'media','imgaspect_':'media','has_video':'media','no_video':'media','has_audio':'media','no_audio':'media',\n",
    "\n",
    "    # Mashable 61 基础特征\n",
    "    'n_tokens_title_':'mashable_basic',\n",
    "    'num_hrefs_':'mashable_basic','num_self_hrefs_':'mashable_basic','num_imgs_':'mashable_basic','num_videos_':'mashable_basic',\n",
    "    'average_token_length_':'mashable_basic','num_keywords_':'mashable_basic',\n",
    "    \n",
    "    # Mashable 频道标志\n",
    "    'data_channel_is_':'mashable_channel',\n",
    "    \n",
    "    # Mashable 关键词特征\n",
    "    'kw_min_min_':'mashable_kw','kw_max_min_':'mashable_kw','kw_avg_min_':'mashable_kw',\n",
    "    \n",
    "    # Mashable 情感特征\n",
    "    'global_subjectivity_':'mashable_sentiment','global_sentiment_polarity_':'mashable_sentiment',\n",
    "    'global_rate_positive_words_':'mashable_sentiment','global_rate_negative_words_':'mashable_sentiment',\n",
    "    'rate_positive_words_':'mashable_sentiment','rate_negative_words_':'mashable_sentiment',\n",
    "    'title_subjectivity_':'mashable_sentiment','title_sentiment_polarity_':'mashable_sentiment',\n",
    "    'abs_title_subjectivity_':'mashable_sentiment','abs_title_sentiment_polarity_':'mashable_sentiment',\n",
    "    'avg_positive_polarity_':'mashable_sentiment','min_positive_polarity_':'mashable_sentiment','max_positive_polarity_':'mashable_sentiment',\n",
    "    'avg_negative_polarity_':'mashable_sentiment','min_negative_polarity_':'mashable_sentiment','max_negative_polarity_':'mashable_sentiment',\n",
    "    \n",
    "    # Mashable LDA\n",
    "    'LDA_':'mashable_lda',\n",
    "    \n",
    "    # Mashable 正文特征\n",
    "    'n_tokens_content_':'mashable_content','n_unique_tokens_':'mashable_content',\n",
    "    'n_non_stop_words_':'mashable_content','n_non_stop_unique_tokens_':'mashable_content',\n",
    "    \n",
    "    # 原有其他特征\n",
    "    'linkcnt_':'media','authoritative_links_':'media',\n",
    "    'num_hrefs_b':'links','num_self_hrefs_b':'links','self_ref_ratio_b':'links',\n",
    "    'is_listicle':'titleshape','not_listicle':'titleshape','title_has_':'titleshape',\n",
    "    'title_len_word_':'titleshape','title_len_char_':'titleshape','title_upper_':'titleshape',\n",
    "    'title_clickbait_':'style',\n",
    "    'social_buttons_':'social','comment_sections_':'social','share_count_':'social',\n",
    "    'urgency_indicators_':'content','question_words_':'content','noun_count_':'content','cta_count_':'content',\n",
    "    'div_count_':'structure','section_count_':'structure','list_count_':'structure','readability_':'structure',\n",
    "    'header_word_count_':'length',\n",
    "    'entity_count_':'extra','topic_':'extra','sentiment_':'extra','positive_words_':'extra','negative_words_':'extra',\n",
    "    'ner_person_':'extra','ner_org_':'extra','ner_gpe_':'extra',\n",
    "    'body_len_':'length',\n",
    "    'body_p_':'sections','body_bq_':'sections','body_has_code':'sections','body_no_code':'sections','body_h2h3_':'sections',\n",
    "    'links_ext_ratio_':'links','links_ext_cnt_':'links','links_auth_cnt_':'links',\n",
    "    'quotes_':'quotes','body_exclaim_':'quotes','body_qmark_':'quotes','avg_sentence_len_':'quotes',\n",
    "    'lexdiv_':'lexical','readtime_min_':'readtime',\n",
    "    'money_':'numbers','percent_':'numbers','numdens_':'numbers',\n",
    "    'subj_adj_':'subjectivity','subj_adv_':'subjectivity','pronouns_':'subjectivity',\n",
    "    'hedge_':'subjectivity','assertive_':'subjectivity',\n",
    "    'align_jacc_':'alignment','align_ent_':'alignment',\n",
    "    'imgdens_':'media',\n",
    "    'keyword_overlap_title_':'keywords','keyword_overlap_body_':'keywords',\n",
    "    'url_domain_':'url','url_depth_':'url',\n",
    "}\n",
    "\n",
    "def _split_feat_tokens(feats_string: str):\n",
    "    buckets = {k: [] for k in [\n",
    "        'author','channel','publisher','time','media','titleshape','style',\n",
    "        'social','content','structure','length','extra',\n",
    "        'links','numbers','subjectivity','quotes','sections','readtime','lexical','alignment',\n",
    "        'keywords','url',\n",
    "        # Mashable 61 特征分组\n",
    "        'mashable_basic','mashable_channel','mashable_kw','mashable_sentiment','mashable_lda','mashable_content'\n",
    "    ]}\n",
    "    if not feats_string:\n",
    "        for k in buckets: buckets[k] = [f'{k}=_none']\n",
    "        return buckets\n",
    "\n",
    "    for tok in feats_string.split():\n",
    "        placed = False\n",
    "        for pref, grp in PREFIX_MAP.items():\n",
    "            if tok.startswith(pref):\n",
    "                buckets[grp].append(tok); placed=True; break\n",
    "        if not placed:\n",
    "            buckets['extra'].append(tok)\n",
    "\n",
    "    for k,v in buckets.items():\n",
    "        if not v:\n",
    "            buckets[k] = [f'{k}=_none']\n",
    "    return buckets\n",
    "\n",
    "# =========================\n",
    "#  featurize_split（兼容新增 Mashable 分组）\n",
    "# =========================\n",
    "def featurize_split(html_series: pd.Series, lda_vectorizer=None, lda_model=None, n_jobs=1) -> sp.csr_matrix:\n",
    "    rows = html_series.astype(str).tolist()\n",
    "\n",
    "    if n_jobs is None or n_jobs == 1:\n",
    "        processed_data = [preprocessor(h, lda_vectorizer, lda_model) for h in rows]\n",
    "    else:\n",
    "        processed_data = Parallel(n_jobs=n_jobs, backend=\"loky\", prefer=\"processes\")(\n",
    "            delayed(preprocessor)(h, lda_vectorizer, lda_model) for h in rows\n",
    "        )\n",
    "\n",
    "    # 收集器\n",
    "    titles, header_bows, entity_texts = [], [], []\n",
    "    author_tokens, channel_tokens, publisher_tokens = [], [], []\n",
    "    time_tokens, media_tokens, titleshape_tokens = [], [], []\n",
    "    social_tokens, content_tokens, structure_tokens = [], [], []\n",
    "    length_tokens, extra_tokens = [], []\n",
    "    body_feats_tokens, body_bows = [], []\n",
    "    \n",
    "    # Mashable 61 特征收集器\n",
    "    mashable_basic_tokens, mashable_channel_tokens, mashable_kw_tokens = [], [], []\n",
    "    mashable_sentiment_tokens, mashable_lda_tokens, mashable_content_tokens = [], [], []\n",
    "\n",
    "    for title, ents, header_feats, header_bow, body_feats, body_bow in processed_data:\n",
    "        titles.append(' '.join(tokenizer_stem_keepmeta(title, set(ents))) if title else '')\n",
    "        header_bows.append(header_bow or '')\n",
    "        entity_texts.append(' '.join(sorted(ents)) if ents else '')\n",
    "\n",
    "        # header -> 分组\n",
    "        b = _split_feat_tokens(header_feats)\n",
    "        author_tokens.append(b['author']);    channel_tokens.append(b['channel']);   publisher_tokens.append(b['publisher'])\n",
    "        time_tokens.append(b['time']);        media_tokens.append(b['media']);       titleshape_tokens.append(b['titleshape'])\n",
    "        social_tokens.append(b['social']);    content_tokens.append(b['content']);   structure_tokens.append(b['structure'])\n",
    "        length_tokens.append(b['length']);    extra_tokens.append(b['extra'])\n",
    "        \n",
    "        # Mashable 61 特征\n",
    "        mashable_basic_tokens.append(b.get('mashable_basic', []))\n",
    "        mashable_channel_tokens.append(b.get('mashable_channel', []))\n",
    "        mashable_kw_tokens.append(b.get('mashable_kw', []))\n",
    "        mashable_sentiment_tokens.append(b.get('mashable_sentiment', []))\n",
    "        mashable_lda_tokens.append(b.get('mashable_lda', []))\n",
    "\n",
    "        # body -> 分组\n",
    "        bf = _split_feat_tokens(body_feats or '')\n",
    "        body_feats_tokens.append(body_feats.split() if body_feats else ['no_body'])\n",
    "        body_bows.append(body_bow or '')\n",
    "\n",
    "        # Mashable 正文特征\n",
    "        mashable_content_tokens.append(bf.get('mashable_content', []))\n",
    "        mashable_sentiment_tokens[-1] += bf.get('mashable_sentiment', [])\n",
    "\n",
    "    # 编码\n",
    "    X_title      = title_vec.transform(titles)\n",
    "    X_header     = header_vec.transform(header_bows)\n",
    "    X_entities   = entities_vec.transform(entity_texts)\n",
    "\n",
    "    X_author     = author_h.transform(author_tokens)\n",
    "    X_channel    = channel_h.transform(channel_tokens)\n",
    "    X_publisher  = publisher_h.transform(publisher_tokens)\n",
    "    X_time       = time_h.transform(time_tokens)\n",
    "    X_media      = media_h.transform(media_tokens)\n",
    "    X_titleshape = titleshape_h.transform(titleshape_tokens)\n",
    "    X_social     = social_h.transform(social_tokens)\n",
    "    X_content    = content_h.transform(content_tokens)\n",
    "    X_structure  = structure_h.transform(structure_tokens)\n",
    "    X_length     = length_h.transform(length_tokens)\n",
    "    X_extra      = extra_h.transform(extra_tokens)\n",
    "\n",
    "    # Mashable 61 特征编码\n",
    "    X_mashable_basic = mashable_basic_h.transform(mashable_basic_tokens)\n",
    "    X_mashable_channel = mashable_channel_h.transform(mashable_channel_tokens)\n",
    "    X_mashable_kw = mashable_kw_h.transform(mashable_kw_tokens)\n",
    "    X_mashable_sentiment = mashable_sentiment_h.transform(mashable_sentiment_tokens)\n",
    "    X_mashable_lda = mashable_lda_h.transform(mashable_lda_tokens)\n",
    "    X_mashable_content = mashable_content_h.transform(mashable_content_tokens)\n",
    "\n",
    "    # 正文块\n",
    "    X_body_feats   = bodyfeats_h.transform(body_feats_tokens)\n",
    "    X_body_bow     = body_vec.transform(body_bows)\n",
    "\n",
    "    # 拼接 - 将 Mashable 特征集成到现有管道中\n",
    "    X = sp.hstack([\n",
    "        # 词袋块\n",
    "        X_title, X_header, X_entities,\n",
    "        # header 类别块\n",
    "        X_author, X_channel, X_publisher, X_time,\n",
    "        X_media, X_titleshape, X_social, X_content, X_structure, X_length, X_extra,\n",
    "        # Mashable 61 特征块\n",
    "        X_mashable_basic, X_mashable_channel, X_mashable_kw, \n",
    "        X_mashable_sentiment, X_mashable_lda, X_mashable_content,\n",
    "        # body 类别块 + 词袋\n",
    "        X_body_feats, X_body_bow\n",
    "    ], format='csr')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145702c0",
   "metadata": {},
   "source": [
    "把驗證集轉成特徵，後面評估直接用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e50e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 一次性把驗證集轉成特徵，後面評估直接用\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_val \u001b[38;5;241m=\u001b[39m featurize_split(val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPage content\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      3\u001b[0m y_val \u001b[38;5;241m=\u001b[39m val_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPopularity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      4\u001b[0m sp\u001b[38;5;241m.\u001b[39msave_npz(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/X_val_split.npz\u001b[39m\u001b[38;5;124m'\u001b[39m, X_val)\n",
      "Cell \u001b[1;32mIn[83], line 257\u001b[0m, in \u001b[0;36mfeaturize_split\u001b[1;34m(html_series, lda_vectorizer, lda_model, n_jobs)\u001b[0m\n\u001b[0;32m    254\u001b[0m titles, header_bows \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    255\u001b[0m author_tokens, topic_tokens, time_tokens, content_tokens, other_tokens \u001b[38;5;241m=\u001b[39m [], [], [], [], []\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m title, ents, header_feats, header_bow, body_feats, body_bow \u001b[38;5;129;01min\u001b[39;00m processed_data:\n\u001b[0;32m    258\u001b[0m     titles\u001b[38;5;241m.\u001b[39mappend(title \u001b[38;5;28;01mif\u001b[39;00m title \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    259\u001b[0m     header_bows\u001b[38;5;241m.\u001b[39mappend(header_bow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 6)"
     ]
    }
   ],
   "source": [
    "# 一次性把驗證集轉成特徵，後面評估直接用\n",
    "X_val = featurize_split(val_df['Page content'].astype(str), lda_vectorizer, lda_model, n_jobs=1)\n",
    "y_val = val_df['Popularity'].values\n",
    "sp.save_npz(f'{OUT_DIR}/X_val_split.npz', X_val)\n",
    "np.save(f'{OUT_DIR}/y_val.npy', y_val)\n",
    "X_val.shape, np.bincount(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c484b75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/50 ===\n",
      "Epoch 1 [1/26] Train AUC=0.7679 | Val AUC=0.5480\n",
      "Epoch 1 [2/26] Train AUC=0.7124 | Val AUC=0.5711\n",
      "Epoch 1 [3/26] Train AUC=0.7172 | Val AUC=0.5728\n",
      "Epoch 1 [4/26] Train AUC=0.7248 | Val AUC=0.5983\n",
      "Epoch 1 [5/26] Train AUC=0.6885 | Val AUC=0.6036\n",
      "Epoch 1 [6/26] Train AUC=0.7010 | Val AUC=0.5962\n",
      "Epoch 1 [7/26] Train AUC=0.7043 | Val AUC=0.6081\n",
      "Epoch 1 [8/26] Train AUC=0.6831 | Val AUC=0.5980\n",
      "Epoch 1 [9/26] Train AUC=0.6754 | Val AUC=0.5908\n",
      "Epoch 1 [10/26] Train AUC=0.7085 | Val AUC=0.5919\n",
      "Epoch 1 [11/26] Train AUC=0.6503 | Val AUC=0.6099\n",
      "Epoch 1 [12/26] Train AUC=0.6637 | Val AUC=0.5960\n",
      "Epoch 1 [13/26] Train AUC=0.6443 | Val AUC=0.6093\n",
      "Epoch 1 [14/26] Train AUC=0.6541 | Val AUC=0.6218\n",
      "Epoch 1 [15/26] Train AUC=0.6824 | Val AUC=0.6229\n",
      "Epoch 1 [16/26] Train AUC=0.6701 | Val AUC=0.6137\n",
      "Epoch 1 [17/26] Train AUC=0.6868 | Val AUC=0.6145\n",
      "Epoch 1 [18/26] Train AUC=0.6831 | Val AUC=0.5969\n",
      "Epoch 1 [19/26] Train AUC=0.6683 | Val AUC=0.5995\n",
      "Epoch 1 [20/26] Train AUC=0.6423 | Val AUC=0.6089\n",
      "Epoch 1 [21/26] Train AUC=0.6630 | Val AUC=0.6012\n",
      "Epoch 1 [22/26] Train AUC=0.6778 | Val AUC=0.6094\n",
      "Epoch 1 [23/26] Train AUC=0.6689 | Val AUC=0.6099\n",
      "Epoch 1 [24/26] Train AUC=0.6365 | Val AUC=0.6151\n",
      "Epoch 1 [25/26] Train AUC=0.6896 | Val AUC=0.6123\n",
      "Epoch 1 [26/26] Train AUC=0.6740 | Val AUC=0.6152\n",
      "\n",
      "Epoch 1 结果: Train AUC=0.6822 | Val AUC=0.6152\n",
      "📝 已保存當前最後模型（last）: ./output/clf-sgd-last.pkl\n",
      "🎯 新的最佳模型! Val AUC: 0.6152 (Epoch 1)\n",
      "💾 最佳模型已保存: ./output/clf-sgd-best.pkl\n",
      "当前最佳: Epoch 1, Val AUC: 0.6152\n",
      "\n",
      "=== Epoch 2/50 ===\n",
      "Epoch 2 [1/26] Train AUC=0.7421 | Val AUC=0.6104\n",
      "Epoch 2 [2/26] Train AUC=0.7308 | Val AUC=0.6208\n",
      "Epoch 2 [3/26] Train AUC=0.7273 | Val AUC=0.6201\n",
      "Epoch 2 [4/26] Train AUC=0.7080 | Val AUC=0.6172\n",
      "Epoch 2 [5/26] Train AUC=0.7064 | Val AUC=0.6108\n",
      "Epoch 2 [6/26] Train AUC=0.6497 | Val AUC=0.6055\n",
      "Epoch 2 [7/26] Train AUC=0.7223 | Val AUC=0.5959\n",
      "Epoch 2 [8/26] Train AUC=0.6778 | Val AUC=0.6028\n",
      "Epoch 2 [9/26] Train AUC=0.6749 | Val AUC=0.5957\n",
      "Epoch 2 [10/26] Train AUC=0.6984 | Val AUC=0.6157\n",
      "Epoch 2 [11/26] Train AUC=0.7028 | Val AUC=0.6075\n",
      "Epoch 2 [12/26] Train AUC=0.6776 | Val AUC=0.6094\n",
      "Epoch 2 [13/26] Train AUC=0.6845 | Val AUC=0.6109\n",
      "Epoch 2 [14/26] Train AUC=0.6871 | Val AUC=0.6072\n",
      "Epoch 2 [15/26] Train AUC=0.7378 | Val AUC=0.6097\n",
      "Epoch 2 [16/26] Train AUC=0.6683 | Val AUC=0.6181\n",
      "Epoch 2 [17/26] Train AUC=0.6978 | Val AUC=0.6119\n",
      "Epoch 2 [18/26] Train AUC=0.6792 | Val AUC=0.6231\n",
      "Epoch 2 [19/26] Train AUC=0.6966 | Val AUC=0.6258\n",
      "Epoch 2 [20/26] Train AUC=0.6975 | Val AUC=0.6176\n",
      "Epoch 2 [21/26] Train AUC=0.6864 | Val AUC=0.6195\n",
      "Epoch 2 [22/26] Train AUC=0.7133 | Val AUC=0.6117\n",
      "Epoch 2 [23/26] Train AUC=0.6944 | Val AUC=0.6060\n",
      "Epoch 2 [24/26] Train AUC=0.6825 | Val AUC=0.6105\n",
      "Epoch 2 [25/26] Train AUC=0.7130 | Val AUC=0.6268\n",
      "Epoch 2 [26/26] Train AUC=0.7131 | Val AUC=0.6305\n",
      "\n",
      "Epoch 2 结果: Train AUC=0.6988 | Val AUC=0.6305\n",
      "📝 已保存當前最後模型（last）: ./output/clf-sgd-last.pkl\n",
      "🎯 新的最佳模型! Val AUC: 0.6305 (Epoch 2)\n",
      "💾 最佳模型已保存: ./output/clf-sgd-best.pkl\n",
      "当前最佳: Epoch 2, Val AUC: 0.6305\n",
      "\n",
      "=== Epoch 3/50 ===\n",
      "Epoch 3 [1/26] Train AUC=0.7303 | Val AUC=0.6364\n",
      "Epoch 3 [2/26] Train AUC=0.7269 | Val AUC=0.6214\n",
      "Epoch 3 [3/26] Train AUC=0.7098 | Val AUC=0.6124\n",
      "Epoch 3 [4/26] Train AUC=0.7032 | Val AUC=0.6145\n",
      "Epoch 3 [5/26] Train AUC=0.7108 | Val AUC=0.6207\n",
      "Epoch 3 [6/26] Train AUC=0.7320 | Val AUC=0.6164\n",
      "Epoch 3 [7/26] Train AUC=0.7168 | Val AUC=0.6188\n",
      "Epoch 3 [8/26] Train AUC=0.7159 | Val AUC=0.6180\n",
      "Epoch 3 [9/26] Train AUC=0.7323 | Val AUC=0.6246\n",
      "Epoch 3 [10/26] Train AUC=0.7298 | Val AUC=0.6236\n",
      "Epoch 3 [11/26] Train AUC=0.7223 | Val AUC=0.6142\n",
      "Epoch 3 [12/26] Train AUC=0.7093 | Val AUC=0.6174\n",
      "Epoch 3 [13/26] Train AUC=0.7249 | Val AUC=0.6143\n",
      "Epoch 3 [14/26] Train AUC=0.7056 | Val AUC=0.6182\n",
      "Epoch 3 [15/26] Train AUC=0.7261 | Val AUC=0.6191\n",
      "Epoch 3 [16/26] Train AUC=0.6880 | Val AUC=0.6122\n",
      "Epoch 3 [17/26] Train AUC=0.7037 | Val AUC=0.6078\n",
      "Epoch 3 [18/26] Train AUC=0.7452 | Val AUC=0.6214\n",
      "Epoch 3 [19/26] Train AUC=0.7245 | Val AUC=0.6271\n",
      "Epoch 3 [20/26] Train AUC=0.6934 | Val AUC=0.6353\n",
      "Epoch 3 [21/26] Train AUC=0.7259 | Val AUC=0.6379\n",
      "Epoch 3 [22/26] Train AUC=0.7141 | Val AUC=0.6367\n",
      "Epoch 3 [23/26] Train AUC=0.7195 | Val AUC=0.6396\n",
      "Epoch 3 [24/26] Train AUC=0.6818 | Val AUC=0.6315\n",
      "Epoch 3 [25/26] Train AUC=0.7233 | Val AUC=0.6236\n",
      "Epoch 3 [26/26] Train AUC=0.7226 | Val AUC=0.6277\n",
      "\n",
      "Epoch 3 结果: Train AUC=0.7168 | Val AUC=0.6277\n",
      "📝 已保存當前最後模型（last）: ./output/clf-sgd-last.pkl\n",
      "当前最佳: Epoch 2, Val AUC: 0.6305\n",
      "\n",
      "=== Epoch 4/50 ===\n",
      "Epoch 4 [1/26] Train AUC=0.7288 | Val AUC=0.6289\n",
      "Epoch 4 [2/26] Train AUC=0.7367 | Val AUC=0.6237\n",
      "Epoch 4 [3/26] Train AUC=0.7546 | Val AUC=0.6178\n",
      "Epoch 4 [4/26] Train AUC=0.7562 | Val AUC=0.6167\n",
      "Epoch 4 [5/26] Train AUC=0.7198 | Val AUC=0.6196\n",
      "Epoch 4 [6/26] Train AUC=0.7350 | Val AUC=0.6196\n",
      "Epoch 4 [7/26] Train AUC=0.7509 | Val AUC=0.6261\n",
      "Epoch 4 [8/26] Train AUC=0.7373 | Val AUC=0.6274\n",
      "Epoch 4 [9/26] Train AUC=0.7572 | Val AUC=0.6172\n",
      "Epoch 4 [10/26] Train AUC=0.7342 | Val AUC=0.6313\n",
      "Epoch 4 [11/26] Train AUC=0.7296 | Val AUC=0.6233\n",
      "Epoch 4 [12/26] Train AUC=0.7216 | Val AUC=0.6272\n",
      "Epoch 4 [13/26] Train AUC=0.7517 | Val AUC=0.6263\n",
      "Epoch 4 [14/26] Train AUC=0.7309 | Val AUC=0.6321\n",
      "Epoch 4 [15/26] Train AUC=0.7450 | Val AUC=0.6270\n",
      "Epoch 4 [16/26] Train AUC=0.7157 | Val AUC=0.6348\n",
      "Epoch 4 [17/26] Train AUC=0.7368 | Val AUC=0.6311\n",
      "Epoch 4 [18/26] Train AUC=0.7351 | Val AUC=0.6289\n",
      "Epoch 4 [19/26] Train AUC=0.7434 | Val AUC=0.6288\n",
      "Epoch 4 [20/26] Train AUC=0.7130 | Val AUC=0.6259\n",
      "Epoch 4 [21/26] Train AUC=0.7385 | Val AUC=0.6165\n",
      "Epoch 4 [22/26] Train AUC=0.7309 | Val AUC=0.6207\n",
      "Epoch 4 [23/26] Train AUC=0.7148 | Val AUC=0.6320\n",
      "Epoch 4 [24/26] Train AUC=0.7380 | Val AUC=0.6378\n",
      "Epoch 4 [25/26] Train AUC=0.7344 | Val AUC=0.6366\n",
      "Epoch 4 [26/26] Train AUC=0.7175 | Val AUC=0.6276\n",
      "\n",
      "Epoch 4 结果: Train AUC=0.7349 | Val AUC=0.6276\n",
      "📝 已保存當前最後模型（last）: ./output/clf-sgd-last.pkl\n",
      "当前最佳: Epoch 2, Val AUC: 0.6305\n",
      "\n",
      "=== Epoch 5/50 ===\n",
      "Epoch 5 [1/26] Train AUC=0.7622 | Val AUC=0.6289\n",
      "Epoch 5 [2/26] Train AUC=0.7701 | Val AUC=0.6345\n",
      "Epoch 5 [3/26] Train AUC=0.7418 | Val AUC=0.6353\n",
      "Epoch 5 [4/26] Train AUC=0.7626 | Val AUC=0.6288\n",
      "Epoch 5 [5/26] Train AUC=0.7732 | Val AUC=0.6372\n",
      "Epoch 5 [6/26] Train AUC=0.7459 | Val AUC=0.6312\n",
      "Epoch 5 [7/26] Train AUC=0.7635 | Val AUC=0.6195\n",
      "Epoch 5 [8/26] Train AUC=0.7257 | Val AUC=0.6152\n",
      "Epoch 5 [9/26] Train AUC=0.7765 | Val AUC=0.6268\n",
      "Epoch 5 [10/26] Train AUC=0.7265 | Val AUC=0.6179\n",
      "Epoch 5 [11/26] Train AUC=0.7380 | Val AUC=0.6246\n",
      "Epoch 5 [12/26] Train AUC=0.7563 | Val AUC=0.6183\n",
      "Epoch 5 [13/26] Train AUC=0.7658 | Val AUC=0.6209\n",
      "Epoch 5 [14/26] Train AUC=0.7587 | Val AUC=0.6202\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m epoch_train_aucs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(stream, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 92\u001b[0m     Xtr \u001b[38;5;241m=\u001b[39m featurize_split(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPage content\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m), lda_vectorizer, lda_model, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     93\u001b[0m     ytr \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPopularity\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[83], line 247\u001b[0m, in \u001b[0;36mfeaturize_split\u001b[1;34m(html_series, lda_vectorizer, lda_model, n_jobs)\u001b[0m\n\u001b[0;32m    244\u001b[0m rows \u001b[38;5;241m=\u001b[39m html_series\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 247\u001b[0m     processed_data \u001b[38;5;241m=\u001b[39m [preprocessor(h, lda_vectorizer, lda_model) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m rows]\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     processed_data \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m\"\u001b[39m, prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\n\u001b[0;32m    250\u001b[0m         delayed(preprocessor)(h, lda_vectorizer, lda_model) \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m rows\n\u001b[0;32m    251\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[83], line 33\u001b[0m, in \u001b[0;36mpreprocessor\u001b[1;34m(html, lda_vectorizer, lda_model, max_text_len)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(html, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m html\u001b[38;5;241m.\u001b[39mstrip():\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty_content\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mset\u001b[39m(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 33\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# --- 标题 ---\u001b[39;00m\n\u001b[0;32m     36\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\site-packages\\bs4\\__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feed()\n\u001b[0;32m    336\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\site-packages\\bs4\\__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m--> 478\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mfeed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmarkup)\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    378\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     parser\u001b[38;5;241m.\u001b[39mfeed(markup)\n\u001b[0;32m    381\u001b[0m     parser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\html\\parser.py:111\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoahead(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\html\\parser.py:173\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    171\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_starttag(i)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[1;32m--> 173\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_endtag(i)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<!--\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[0;32m    175\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_comment(i)\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\html\\parser.py:414\u001b[0m, in \u001b[0;36mHTMLParser.parse_endtag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_data(rawdata[i:gtpos])\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gtpos\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_endtag(elem)\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cdata_mode()\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gtpos\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:176\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_endtag\u001b[1;34m(self, name, check_already_closed)\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malready_closed_empty_element\u001b[38;5;241m.\u001b[39mremove(name)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\u001b[38;5;241m.\u001b[39mhandle_endtag(name)\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\site-packages\\bs4\\__init__.py:770\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_endtag\u001b[1;34m(self, name, nsprefix)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Called by the tree builder when an ending tag is encountered.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03m:param name: Name of the tag.\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;124;03m:param nsprefix: Namespace prefix for the tag.\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;66;03m#print(\"End tag: \" + name)\u001b[39;00m\n\u001b[1;32m--> 770\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popToTag(name, nsprefix)\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\site-packages\\bs4\\__init__.py:616\u001b[0m, in \u001b[0;36mBeautifulSoup.endData\u001b[1;34m(self, containerClass)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagStack) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    612\u001b[0m        (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;129;01mor\u001b[39;00m \\\n\u001b[0;32m    613\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_only\u001b[38;5;241m.\u001b[39msearch(current_data)):\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 616\u001b[0m containerClass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring_container(containerClass)\n\u001b[0;32m    617\u001b[0m o \u001b[38;5;241m=\u001b[39m containerClass(current_data)\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobject_was_parsed(o)\n",
      "File \u001b[1;32mc:\\Anaconda\\Lib\\site-packages\\bs4\\__init__.py:527\u001b[0m, in \u001b[0;36mBeautifulSoup.string_container\u001b[1;34m(self, base_class)\u001b[0m\n\u001b[0;32m    524\u001b[0m container \u001b[38;5;241m=\u001b[39m base_class \u001b[38;5;129;01mor\u001b[39;00m NavigableString\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# There may be a general override of NavigableString.\u001b[39;00m\n\u001b[1;32m--> 527\u001b[0m container \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39melement_classes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    528\u001b[0m     container, container\n\u001b[0;32m    529\u001b[0m )\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# On top of that, we may be inside a tag that needs a special\u001b[39;00m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# container class.\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstring_container_stack \u001b[38;5;129;01mand\u001b[39;00m container \u001b[38;5;129;01mis\u001b[39;00m NavigableString:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 先导入所有需要的库\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "import copy\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ---------- 路径与原子写入工具 ----------\n",
    "OUT_DIR = './output'\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "BEST_PATH  = f'{OUT_DIR}/clf-sgd-best.pkl'   # 历史最优（被提升时覆盖）\n",
    "LAST_PATH  = f'{OUT_DIR}/clf-sgd-last.pkl'   # 每个 epoch 结束后覆盖\n",
    "FINAL_PATH = f'{OUT_DIR}/clf-sgd-final.pkl'  # 全部训练结束后保存一次\n",
    "LDA_PATH   = f'{OUT_DIR}/lda.pkl'            # LDA 组件（若有）\n",
    "\n",
    "def dump_atomic(obj, path):\n",
    "    tmp = path + '.tmp'\n",
    "    with open(tmp, 'wb') as f:\n",
    "        pkl.dump(obj, f, protocol=pkl.HIGHEST_PROTOCOL)\n",
    "    os.replace(tmp, path)  # 原子替换，避免损坏\n",
    "\n",
    "# ---------- 训练生成器 ----------\n",
    "BATCH_SIZE = 1000  # 可调大些提高吞吐\n",
    "def train_streamer(df, batch_size):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        yield df.iloc[i:i+batch_size]\n",
    "\n",
    "# ---------- 模型与训练超参 ----------\n",
    "classes = np.array([0, 1])\n",
    "clf = SGDClassifier(\n",
    "    # 损失函数\n",
    "    loss=\"squared_hinge\",  # 可选: 'hinge', 'log_loss', 'modified_huber', 'squared_hinge', \n",
    "                          # 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', \n",
    "                          # 'squared_epsilon_insensitive'\n",
    "    \n",
    "    # 正则化\n",
    "    penalty=\"elasticnet\",  # 可选: 'l2', 'l1', 'elasticnet', None\n",
    "    alpha=1e-6,           # 正则化强度，通常范围: 1e-7 到 1e-2\n",
    "    l1_ratio=0.15,        # Elastic Net 混合参数，范围: 0 到 1\n",
    "    \n",
    "    # 学习率调度\n",
    "    learning_rate=\"constant\",  # 可选: 'constant', 'optimal', 'invscaling', 'adaptive'\n",
    "    eta0=1e-3,                 # 初始学习率\n",
    "    power_t=0.25,              # inverse scaling 的指数 [默认 0.25]\n",
    "    \n",
    "    # 训练控制\n",
    "    max_iter=1000,             # 最大迭代次数，建议: 100-5000\n",
    "    tol=1e-4,                  # 停止容忍度，建议: 1e-4 到 1e-2\n",
    "    shuffle=True,              # 是否每个epoch后打乱数据\n",
    "    early_stopping=False,      # 是否使用早停\n",
    "    validation_fraction=0.1,   # 早停验证集比例\n",
    "    n_iter_no_change=5,        # 早停等待轮数\n",
    "    \n",
    "    # 模型配置\n",
    "    fit_intercept=True,        # 是否计算截距\n",
    "    average=False,             # 是否使用平均SGD\n",
    "    n_jobs=None,               # 并行数 (-1 使用所有CPU)\n",
    "    random_state=28,\n",
    "    \n",
    "    # 类别权重\n",
    "    class_weight=None,         # 可选: 'balanced', dict, 或 None\n",
    "    \n",
    "    # 其他\n",
    "    verbose=0,                 # 详细程度\n",
    "    warm_start=False,          # 是否热启动\n",
    ")\n",
    "\n",
    "EPOCHS = 50\n",
    "EVAL_EVERY = 1  # 每步都评估（你原来的逻辑）\n",
    "best_val_auc = -np.inf\n",
    "best_clf = None\n",
    "best_epoch = 0\n",
    "\n",
    "train_auc_hist, val_auc_hist = [], []\n",
    "\n",
    "# ---------- 多 epoch 训练 ----------\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n=== Epoch {epoch}/{EPOCHS} ===\")\n",
    "\n",
    "    # 每个 epoch 打乱\n",
    "    epoch_train_df = train_df.sample(frac=1.0, random_state=28 + epoch).reset_index(drop=True)\n",
    "    stream = train_streamer(epoch_train_df, BATCH_SIZE)\n",
    "    iters = ceil(len(epoch_train_df) / BATCH_SIZE)\n",
    "\n",
    "    epoch_train_aucs = []\n",
    "\n",
    "    for i, batch in enumerate(stream, start=1):\n",
    "        Xtr = featurize_split(batch['Page content'].astype(str), lda_vectorizer, lda_model, n_jobs=1)\n",
    "        ytr = batch['Popularity'].values\n",
    "        if epoch == 1 and i == 1:\n",
    "            clf.partial_fit(Xtr, ytr, classes=classes)\n",
    "        else:\n",
    "            clf.partial_fit(Xtr, ytr)\n",
    "\n",
    "        # 批内 AUC\n",
    "        if np.unique(ytr).size == 2:\n",
    "            tr_auc = roc_auc_score(ytr, clf.decision_function(Xtr))\n",
    "        else:\n",
    "            tr_auc = np.nan\n",
    "        epoch_train_aucs.append(tr_auc)\n",
    "        train_auc_hist.append(tr_auc)\n",
    "\n",
    "        # 週期性评估（用预计算 X_val）\n",
    "        if i % EVAL_EVERY == 0 or i == iters:\n",
    "            va_auc = roc_auc_score(y_val, clf.decision_function(X_val))\n",
    "            val_auc_hist.append(va_auc)\n",
    "            print(f\"Epoch {epoch} [{i}/{iters}] Train AUC={tr_auc:.4f} | Val AUC={va_auc:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch} [{i}/{iters}] Train AUC={tr_auc:.4f}\")\n",
    "\n",
    "    # —— epoch 结束：统计一次 Val AUC\n",
    "    epoch_val_auc = roc_auc_score(y_val, clf.decision_function(X_val))\n",
    "    epoch_train_auc = np.nanmean(epoch_train_aucs)\n",
    "    print(f\"\\nEpoch {epoch} 结果: Train AUC={epoch_train_auc:.4f} | Val AUC={epoch_val_auc:.4f}\")\n",
    "\n",
    "    # 1) 保存“last”（当前 epoch 结束后的模型）\n",
    "    dump_atomic(clf, LAST_PATH)\n",
    "    print(f\"📝 已保存當前最後模型（last）: {LAST_PATH}\")\n",
    "\n",
    "    # 2) 若更优，刷新并保存“best”\n",
    "    if epoch_val_auc > best_val_auc:\n",
    "        best_val_auc = epoch_val_auc\n",
    "        best_epoch = epoch\n",
    "        best_clf = copy.deepcopy(clf)\n",
    "        dump_atomic(best_clf, BEST_PATH)\n",
    "        print(f\"🎯 新的最佳模型! Val AUC: {best_val_auc:.4f} (Epoch {epoch})\")\n",
    "        print(f\"💾 最佳模型已保存: {BEST_PATH}\")\n",
    "\n",
    "    print(f\"当前最佳: Epoch {best_epoch}, Val AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "# ---------- 训练完成：保存 final、LDA，并把 best 载入内存 ----------\n",
    "print(f\"\\n=== 训练完成 ===\")\n",
    "print(f\"最佳模型: Epoch {best_epoch}, Val AUC: {best_val_auc:.4f}\")\n",
    "\n",
    "dump_atomic(clf, FINAL_PATH)  # 注意：final = 整个训练流程结束时的“最后模型”（不等同于最佳）\n",
    "print(f\"✅ 最终模型已保存: {FINAL_PATH}\")\n",
    "\n",
    "# LDA 组件（若有就保存、没有也无所谓）\n",
    "try:\n",
    "    dump_atomic({'lda_vec': lda_vectorizer, 'lda_model': lda_model}, LDA_PATH)\n",
    "    print(f\"✅ LDA组件已保存: {LDA_PATH}\")\n",
    "except Exception as e:\n",
    "    print(\"保存 LDA 组件失败（可忽略）:\", e)\n",
    "\n",
    "# 加载最佳模型用于后续使用\n",
    "if best_clf is not None:\n",
    "    clf = best_clf\n",
    "    print(f\"✅ 已加载最佳模型 (Epoch {best_epoch}) 用于后续预测\")\n",
    "else:\n",
    "    print(\"⚠️ 未产生更优模型，clf 保持为最后一次状态\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4deb076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model (prefer last): output\\clf-sgd-best.pkl\n",
      "Loaded LDA pack: ./output/lda.pkl\n",
      "Saved: ./output/submission_90.csv\n"
     ]
    }
   ],
   "source": [
    "import sys, os, _pickle as pkl\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "# 1) 兼容 shim：某些 pickle 会去找顶层模块 \"_loss\"\n",
    "try:\n",
    "    import sklearn._loss as skl__loss\n",
    "    sys.modules['_loss'] = skl__loss     # 把 sklearn._loss 映射成顶层 _loss，供 pickle 找\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "OUT_DIR = './output'\n",
    "TEST_PATH = './dataset/test.csv'\n",
    "\n",
    "# 2) 优先加载“最後一次(last)”；找不到再回退\n",
    "candidates = ['clf-sgd-best.pkl', 'clf-sgd-final.pkl', 'clf-sgd-last.pkl', 'clf-sgd.pkl']\n",
    "model_path = None\n",
    "for name in candidates:\n",
    "    p = Path(OUT_DIR) / name\n",
    "    if p.exists():\n",
    "        model_path = str(p)\n",
    "        break\n",
    "\n",
    "if model_path is None:\n",
    "    raise FileNotFoundError(\"没有找到模型文件：./output/{clf-sgd-last.pkl, clf-sgd-final.pkl, clf-sgd-best.pkl, clf-sgd.pkl}\")\n",
    "\n",
    "with open(model_path, 'rb') as f:\n",
    "    clf = pkl.load(f)\n",
    "print(\"Loaded model (prefer last):\", model_path)\n",
    "\n",
    "# 3) （可选）LDA pack：没有就跳过\n",
    "lda_vectorizer = None\n",
    "lda_model = None\n",
    "try:\n",
    "    with open(f'{OUT_DIR}/lda.pkl','rb') as f:\n",
    "        lda_pack = pkl.load(f)\n",
    "        lda_vectorizer = lda_pack.get('lda_vec', None)\n",
    "        lda_model = lda_pack.get('lda_model', None)\n",
    "    print(\"Loaded LDA pack:\", f'{OUT_DIR}/lda.pkl')\n",
    "except Exception as e:\n",
    "    print(\"LDA pack not loaded (ok to ignore):\", e)\n",
    "\n",
    "# 4) 推理\n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "B = 2000\n",
    "preds = []\n",
    "for i in range(0, len(df_test), B):\n",
    "    sl = df_test['Page content'].iloc[i:i+B].astype(str)\n",
    "    Xt = featurize_split(sl, lda_vectorizer, lda_model, n_jobs=1)  # 你训练时的同名函数\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        probs = expit(clf.decision_function(Xt))\n",
    "    elif hasattr(clf, \"predict_proba\"):\n",
    "        probs = clf.predict_proba(Xt)[:, 1]\n",
    "    else:\n",
    "        raise RuntimeError(\"Loaded classifier has neither decision_function nor predict_proba.\")\n",
    "    preds.append(probs)\n",
    "\n",
    "test_pred = np.concatenate(preds)\n",
    "submission = pd.DataFrame({'Id': df_test['Id'], 'Popularity': test_pred})\n",
    "sub_path = f'{OUT_DIR}/submission_90.csv'\n",
    "submission.to_csv(sub_path, index=False)\n",
    "print(\"Saved:\", sub_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
