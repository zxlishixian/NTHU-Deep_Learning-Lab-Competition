{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf61e8cc",
   "metadata": {},
   "source": [
    "前置設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406a7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===== 基礎 =====\n",
    "import os, re, numpy as np, pandas as pd\n",
    "from scipy import sparse as sp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "\n",
    "# 其他\n",
    "from joblib import Parallel, delayed\n",
    "from math import ceil\n",
    "\n",
    "# ===== 路徑與隨機種子 =====\n",
    "DATA_DIR = './dataset'\n",
    "TRAIN_PATH = f'{DATA_DIR}/train.csv'\n",
    "TEST_PATH  = f'{DATA_DIR}/test.csv'\n",
    "OUT_DIR = './output'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===== 速度開關（訓練時可先關慢模塊）=====\n",
    "FAST_NO_NER = True        # 關 NER\n",
    "FAST_NO_LDA = True        # 關 LDA 主題\n",
    "FAST_NO_TEXTSTAT = True   # 關可讀性評分\n",
    "\n",
    "# ===== NLTK 資源（缺什麼補什麼）=====\n",
    "for rid, name in [\n",
    "    ('tokenizers/punkt', 'punkt'),\n",
    "    ('corpora/stopwords', 'stopwords'),\n",
    "    ('corpora/wordnet', 'wordnet'),\n",
    "    ('sentiment/vader_lexicon', 'vader_lexicon'),\n",
    "    ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),\n",
    "    ('chunkers/maxent_ne_chunker', 'maxent_ne_chunker'),\n",
    "    ('corpora/words', 'words'),\n",
    "]:\n",
    "    try:\n",
    "        nltk.data.find(rid)\n",
    "    except LookupError:\n",
    "        nltk.download(name)\n",
    "\n",
    "# 全局 VADER（避免每條樣本重建）\n",
    "try:\n",
    "    VADER = SentimentIntensityAnalyzer()\n",
    "except Exception:\n",
    "    VADER = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa594143",
   "metadata": {},
   "source": [
    "導入數據與切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20af56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 Popularity {-1,1} → {0,1}；並切出 train/val\n",
    "df_all = pd.read_csv(TRAIN_PATH)\n",
    "df_all['Popularity'] = (df_all['Popularity'].astype(int) == 1).astype(int)\n",
    "\n",
    "TRAIN_SIZE = 26000\n",
    "VAL_SIZE   = 1000\n",
    "\n",
    "train_df = df_all.iloc[:TRAIN_SIZE].reset_index(drop=True)\n",
    "val_df   = df_all.iloc[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE].reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape, val_df.shape)\n",
    "train_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eeaefd",
   "metadata": {},
   "source": [
    "工具函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f64080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小工具\n",
    "_BODY_OPEN = re.compile(\n",
    "    r'(?is)<\\s*(section|div|article)\\b[^>]*\\b'\n",
    "    r'(?:article-content|article-body|content-body|post-content)\\b[^>]*>'\n",
    ")\n",
    "_MONTH = dict(jan='01', feb='02', mar='03', apr='04', may='05', jun='06',\n",
    "              jul='07', aug='08', sep='09', oct='10', nov='11', dec='12')\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r'[\\W]+', ' ', (s or '').lower()).strip()\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    return re.sub(r'[^a-z0-9_]+', '', _norm(s).replace(' ', '_'))\n",
    "\n",
    "def _bucket(n, edges):\n",
    "    if n is None: return 'unk'\n",
    "    for i in range(len(edges)-1):\n",
    "        if edges[i] <= n < edges[i+1]:\n",
    "            return f\"b{edges[i]}_{edges[i+1]}\"\n",
    "    return f\"b{edges[-1]}p\"\n",
    "\n",
    "def _aspect_bucket(w, h):\n",
    "    if not w or not h: return 'unk'\n",
    "    r = w / h\n",
    "    if r < 0.9: return 'tall'\n",
    "    if r < 1.2: return 'squareish'\n",
    "    if r < 1.8: return 'landscape'\n",
    "    return 'ultrawide'\n",
    "\n",
    "def _img_size_bucket(w, h):\n",
    "    if not w or not h: return 'unk'\n",
    "    area = (w or 0) * (h or 0)\n",
    "    if area < 80_000: return 'xs'\n",
    "    if area < 230_000: return 'sm'\n",
    "    if area < 920_000: return 'md'\n",
    "    if area < 2_100_000: return 'lg'\n",
    "    return 'xl'\n",
    "\n",
    "def _parse_wh_from_src(src: str):\n",
    "    if not src: return (None, None)\n",
    "    m = re.search(r'/(\\d{2,5})x(\\d{2,5})/', src)\n",
    "    return (int(m.group(1)), int(m.group(2))) if m else (None, None)\n",
    "\n",
    "TRENDING_TOPICS = {\n",
    "    'elon_musk', 'ai', 'climate_change', 'covid', 'blockchain', 'taiwan',\n",
    "    'tesla', 'space', 'crypto', 'elections'\n",
    "}\n",
    "\n",
    "# LDA（可選）\n",
    "def pretrain_lda(df, column='Page content', n_components=10, max_features=1000, max_text_len=500):\n",
    "    def extract_text(html):\n",
    "        if not isinstance(html, str) or not html.strip(): return \"\"\n",
    "        m = _BODY_OPEN.search(html)\n",
    "        header_html = html[:m.start()] if m else html\n",
    "        soup = BeautifulSoup(header_html, 'html.parser')\n",
    "        return ' '.join(soup.get_text().lower().split()[:max_text_len])\n",
    "\n",
    "    corpus = [extract_text(x) for x in df[column].astype(str)]\n",
    "    if not any(corpus): return None, None\n",
    "    vec = CountVectorizer(max_features=max_features, stop_words='english')\n",
    "    X = vec.fit_transform(corpus)\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, random_state=42)\n",
    "    lda.fit(X)\n",
    "    return vec, lda\n",
    "\n",
    "lda_vectorizer, lda_model = (None, None)\n",
    "if not FAST_NO_LDA:\n",
    "    _df_lda = df_all.dropna(subset=['Page content']).astype({'Page content':'str'})\n",
    "    lda_vectorizer, lda_model = pretrain_lda(_df_lda, 'Page content', n_components=10, max_features=1000)\n",
    "    print(\"LDA pretrained\")\n",
    "else:\n",
    "    print(\"LDA skipped by FAST_NO_LDA=True\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56102f5",
   "metadata": {},
   "source": [
    "預處理函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6077bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 幫助函數：清洗文本做詞袋 ======\n",
    "porter = PorterStemmer()\n",
    "STOP = set(stopwords.words('english'))\n",
    "\n",
    "def _bow_clean(txt: str) -> str:\n",
    "    toks = [w for w in re.findall(r'[A-Za-z]+', (txt or '').lower()) if w not in STOP]\n",
    "    return ' '.join(porter.stem(w) for w in toks)\n",
    "\n",
    "# ====== ★★★ 修正版 preprocessor：回傳 6 個值（新增 body_feats, body_bow_text） ★★★ ======\n",
    "def preprocessor(html: str, lda_vectorizer=None, lda_model=None, max_text_len=500):\n",
    "    if not isinstance(html, str) or not html.strip():\n",
    "        # ★ 回傳 6 個空值\n",
    "        return \"empty_content\", set(), \"\", \"\", \"\", \"\"\n",
    "\n",
    "    # ★ 將 HTML 分為 header 和 body\n",
    "    m = _BODY_OPEN.search(html)\n",
    "    if m:\n",
    "        header_html = html[:m.start()]\n",
    "        body_html = html[m.start():]\n",
    "    else:\n",
    "        header_html = html\n",
    "        body_html = \"\"\n",
    "\n",
    "    soup = BeautifulSoup(header_html, 'html.parser')\n",
    "\n",
    "    # --- [原有的 header 特徵提取代碼，保持不變] ---\n",
    "    # 標題\n",
    "    title_raw = None\n",
    "    h1 = soup.find('h1', class_=lambda c: (isinstance(c, list) and any('title' in x for x in c)) or (isinstance(c, str) and 'title' in c)) \\\n",
    "         or soup.find('h1')\n",
    "    if h1: title_raw = h1.get_text(' ', strip=True)\n",
    "    elif soup.title: title_raw = soup.title.get_text(' ', strip=True)\n",
    "    title_tokens = _norm(title_raw)\n",
    "\n",
    "    # 作者 / 頻道 / 發佈者\n",
    "    author = None\n",
    "    by = soup.find(class_=lambda c: c and ('byline' in c or 'author_name' in c))\n",
    "    if by: author = by.get_text(' ', strip=True)\n",
    "    if not author:\n",
    "        a = soup.find('a', href=re.compile(r'/author/[^/]+/?$', re.I))\n",
    "        if a: author = a.get_text(' ', strip=True)\n",
    "    author_slug = _slug(re.sub(r'^\\s*by\\s+', '', author or '', flags=re.I))\n",
    "    channel = None\n",
    "    art = soup.find('article')\n",
    "    if art and art.has_attr('data-channel'): channel = art['data-channel']\n",
    "    if not channel and art:\n",
    "        cls = ' '.join(art.get('class', []))\n",
    "        mch = re.search(r'\\b(news|tech|world|sports?|business|entertainment|culture|life|science)\\b', cls, re.I)\n",
    "        if mch: channel = mch.group(1)\n",
    "    channel_slug = _slug(channel or 'unknown')\n",
    "    publisher = None\n",
    "    pub = soup.find('a', href=re.compile(r'/publishers/[^/]+/?', re.I))\n",
    "    if pub: publisher = pub.get_text(' ', strip=True) or re.sub(r'.*/publishers/([^/]+)/?.*', r'\\1', pub['href'], flags=re.I)\n",
    "    publisher_slug = _slug(publisher or 'unknown')\n",
    "\n",
    "    # 時間\n",
    "    year = month = weekday = tod = season = None\n",
    "    is_weekend = None\n",
    "    tm = soup.find('time')\n",
    "    dt = tm['datetime'] if (tm and tm.has_attr('datetime')) else (tm.get_text(' ', strip=True) if tm else None)\n",
    "    if dt:\n",
    "        y = re.search(r'(20\\d{2}|19\\d{2})', dt);  year = y.group(1) if y else None\n",
    "        mo = re.search(r'-(\\d{2})-', dt) or re.search(r'\\b(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\b', dt, re.I)\n",
    "        if mo:\n",
    "            mm = mo.group(1).lower() if mo.lastindex else mo.group(0).lower()\n",
    "            month = _MONTH.get(mm, mm)\n",
    "        wd = re.search(r'\\b(mon|tue|wed|thu|fri|sat|sun)\\b', dt, re.I)\n",
    "        if wd: weekday = wd.group(1).lower(); is_weekend = weekday in ('sat','sun')\n",
    "        hh = re.search(r'\\b(\\d{2}):(\\d{2})', dt)\n",
    "        if hh:\n",
    "            h = int(hh.group(1))\n",
    "            tod = 'morning' if 5<=h<12 else 'afternoon' if 12<=h<17 else 'evening' if 17<=h<22 else 'night'\n",
    "        if month:\n",
    "            m_i = int(month)\n",
    "            season = 'spring' if 3<=m_i<=5 else 'summer' if 6<=m_i<=8 else 'autumn' if 9<=m_i<=11 else 'winter'\n",
    "\n",
    "    # 媒體元素\n",
    "    imgs = soup.find_all('img'); img_count = len(imgs); has_image = img_count>0\n",
    "    leadimg = soup.find(attrs={'data-fragment':'lead-image'}) is not None\n",
    "    max_w=max_h=None\n",
    "    for im in imgs:\n",
    "        w,h = _parse_wh_from_src(im.get('src',''))\n",
    "        if w and h:\n",
    "            if not max_w or (w*h) > ((max_w or 0)*(max_h or 0)):\n",
    "                max_w,max_h = w,h\n",
    "    img_size_bucket = _img_size_bucket(max_w, max_h)\n",
    "    img_aspect_bucket = _aspect_bucket(max_w, max_h)\n",
    "    videos = soup.find_all('video'); iframes = soup.find_all('iframe')\n",
    "    has_video = bool(videos) or any(re.search(r'(youtube|vimeo|dailymotion)', (fr.get('src') or ''), re.I) for fr in iframes)\n",
    "    audio = soup.find_all('audio'); has_audio = len(audio)>0\n",
    "    interactive_elements = soup.find_all(['canvas','svg', lambda tag: tag.name=='div' and 'interactive' in (tag.get('class') or [])])\n",
    "    has_interactive = len(interactive_elements)>0\n",
    "    link_count = len(soup.find_all('a'))\n",
    "    link_bucket = _bucket(link_count, [0,1,3,6,10])\n",
    "    img_bucket  = _bucket(img_count, [0,1,3,5])\n",
    "\n",
    "    authoritative_domains = ['.edu','.gov','.org']\n",
    "    authoritative_links = sum(1 for a in soup.find_all('a') if any(d in (a.get('href') or '').lower() for d in authoritative_domains))\n",
    "    authoritative_link_bucket = _bucket(authoritative_links, [0,1,3,5])\n",
    "\n",
    "    # 標題形態\n",
    "    raw = title_raw or ''\n",
    "    title_has_num = bool(re.search(r'\\d', raw))\n",
    "    title_has_year = bool(re.search(r'\\b(19|20)\\d{2}\\b', raw))\n",
    "    title_has_q = '?' in raw\n",
    "    title_has_exclaim = '!' in raw\n",
    "    title_has_colon = ':' in raw\n",
    "    is_listicle = bool(re.match(r'^\\s*\\d+', raw))\n",
    "    upper_ratio = (sum(ch.isupper() for ch in raw) / max(1, sum(ch.isalpha() for ch in raw)))\n",
    "    upper_bucket = 'low' if upper_ratio < 0.15 else 'mid' if upper_ratio < 0.4 else 'high'\n",
    "    title_word_len = len(_norm(raw).split()); tw_bucket = _bucket(title_word_len, [0,4,8,12,20])\n",
    "    title_char_len = len(re.sub(r'\\s+','',raw)); tc_bucket = _bucket(title_char_len, [0,30,60,90,140])\n",
    "\n",
    "    # 社交\n",
    "    social_keywords = ['share','twitter','facebook','linkedin','whatsapp','telegram']\n",
    "    social_elements = soup.find_all(lambda tag: any(\n",
    "        kw in (tag.get('class') or []) or kw in (tag.get('id') or '') or kw in tag.get_text().lower()\n",
    "        for kw in social_keywords\n",
    "    ))\n",
    "    social_count = len(social_elements)\n",
    "    social_count_bucket = _bucket(social_count, [0,1,3,5])\n",
    "    share_count = 0\n",
    "    for elem in social_elements:\n",
    "        m = re.search(r'(\\d+)\\s*(shares?|likes?|retweets?)', elem.get_text(), re.I)\n",
    "        if m: share_count += int(m.group(1))\n",
    "    shares_bucket = _bucket(share_count, [0,10,100,1000])\n",
    "\n",
    "    comment_selectors = ['.comments','#comments','.comment','.discussion']\n",
    "    comment_count = sum(len(soup.select(sel)) for sel in comment_selectors)\n",
    "    comment_count_bucket = _bucket(comment_count, [0,1,3,5])\n",
    "\n",
    "    # 內容與情感\n",
    "    text_content = ' '.join(soup.get_text().lower().split()[:max_text_len])\n",
    "    sentiment_compound = VADER.polarity_scores(text_content)['compound'] if 'VADER' in globals() and VADER else 0.0\n",
    "    sentiment_bucket = ('strong_positive' if sentiment_compound>0.5 else\n",
    "                        'positive' if sentiment_compound>0.05 else\n",
    "                        'strong_negative' if sentiment_compound<-0.5 else\n",
    "                        'negative' if sentiment_compound<-0.05 else 'neutral')\n",
    "    positive_words = ['amazing','great','excellent','wonderful','best','success','win','good','positive']\n",
    "    negative_words = ['terrible','awful','bad','worst','failure','lose','problem','negative']\n",
    "    pos_count = sum(1 for w in positive_words if w in text_content)\n",
    "    neg_count = sum(1 for w in negative_words if w in text_content)\n",
    "\n",
    "    # 緊急/問句/名詞/CTA\n",
    "    cta_phrases = ['read more','subscribe now','click here','learn more','join us','sign up']\n",
    "    cta_count = sum(1 for p in cta_phrases if p in text_content)\n",
    "    cta_count_bucket = _bucket(cta_count, [0,1,3,5])\n",
    "\n",
    "    urgency_indicators = ['breaking','urgent','alert','crisis','emergency','important']\n",
    "    urgency_count = sum(1 for w in urgency_indicators if w in text_content)\n",
    "    urgency_count_bucket = _bucket(urgency_count, [0,1,3,5])\n",
    "\n",
    "    question_words = ['what','why','how','when','where','who']\n",
    "    question_count = sum(1 for w in question_words if w in text_content)\n",
    "    question_count_bucket = _bucket(question_count, [0,1,3,5])\n",
    "\n",
    "    tokens = word_tokenize(text_content)\n",
    "    try:\n",
    "        tagged = pos_tag(tokens)\n",
    "    except Exception:\n",
    "        tagged = [(w,'NN') for w in tokens]\n",
    "    nouns = [w for w,pos_ in tagged if pos_.startswith('NN') and w.lower() not in STOP]\n",
    "    noun_count_bucket = _bucket(len(nouns), [0,5,10,20,50])\n",
    "\n",
    "    # NER（可關）\n",
    "    entities = set()\n",
    "    if 'FAST_NO_NER' not in globals() or not FAST_NO_NER:\n",
    "        try:\n",
    "            from nltk import ne_chunk\n",
    "            chunked = ne_chunk(tagged)\n",
    "            for ch in chunked:\n",
    "                if hasattr(ch,'label') and ch.label() in ['PERSON','ORGANIZATION','GPE']:\n",
    "                    ent = '_'.join(c[0].lower() for c in ch)\n",
    "                    entities.add(f'entity_{ent}')\n",
    "        except Exception:\n",
    "            pass\n",
    "    entity_count_bucket = _bucket(len(entities), [0,1,3,5])\n",
    "\n",
    "    # LDA（可關）\n",
    "    if ('FAST_NO_LDA' not in globals() or not FAST_NO_LDA) and (lda_vectorizer is not None) and (lda_model is not None):\n",
    "        X_ = lda_vectorizer.transform([text_content] if text_content else [''])\n",
    "        topic_dist = lda_model.transform(X_)\n",
    "        dom = int(np.argmax(topic_dist[0])) if topic_dist.size>0 else 0\n",
    "        score = float(topic_dist[0][dom]) if topic_dist.size>0 else 0.0\n",
    "        topic_bucket = f'topic_{dom}_b{_bucket(score,[0,0.5,0.7,0.9])}'\n",
    "    else:\n",
    "        topic_bucket = 'topic_unk'\n",
    "\n",
    "    # 參與度\n",
    "    engagement_metrics = {'clicks':0, 'shares':share_count, 'comments':comment_count}\n",
    "    click_elements = soup.find_all(lambda tag: 'click' in tag.get_text().lower() and re.search(r'\\d+', tag.get_text()))\n",
    "    for elem in click_elements:\n",
    "        m = re.search(r'(\\d+)\\s*clicks?', elem.get_text(), re.I)\n",
    "        if m: engagement_metrics['clicks'] += int(m.group(1))\n",
    "    clicks_bucket   = _bucket(engagement_metrics['clicks'],   [0,100,1000,10000])\n",
    "    comments_bucket = _bucket(engagement_metrics['comments'], [0,1,10,50])\n",
    "\n",
    "    # 可讀性（可關）\n",
    "    if 'FAST_NO_TEXTSTAT' not in globals() or not FAST_NO_TEXTSTAT:\n",
    "        try:\n",
    "            from textstat import flesch_reading_ease\n",
    "            readability_score = flesch_reading_ease(text_content) if text_content else 0\n",
    "        except Exception:\n",
    "            readability_score = 0\n",
    "    else:\n",
    "        readability_score = 50  # 中性\n",
    "    readability_bucket = ('very_easy' if readability_score>80 else\n",
    "                          'easy' if readability_score>60 else\n",
    "                          'standard' if readability_score>50 else\n",
    "                          'difficult' if readability_score>30 else 'very_difficult')\n",
    "\n",
    "    # 版面結構\n",
    "    div_count = len(soup.find_all('div'))\n",
    "    section_count = len(soup.find_all('section'))\n",
    "    list_count = len(soup.find_all(['ul','ol']))\n",
    "    div_count_bucket = _bucket(div_count, [0,5,10,20,50])\n",
    "    section_count_bucket = _bucket(section_count, [0,1,3,5])\n",
    "    list_count_bucket = _bucket(list_count, [0,1,3,5])\n",
    "    header_word_count_bucket = _bucket(len(text_content.split()), [0,50,100,200,500])\n",
    "\n",
    "    # 拼元特徵 token\n",
    "    feats = []\n",
    "    feats += [\n",
    "        f'author_{author_slug or \"unknown\"}',\n",
    "        f'channel_{channel_slug}',\n",
    "        f'publisher_{publisher_slug}',\n",
    "        f'year_{year or \"unk\"}', f'month_{month or \"unk\"}',\n",
    "        f'weekday_{weekday or \"unk\"}', f'tod_{tod or \"unk\"}', f'season_{season or \"unk\"}',\n",
    "        'weekend' if is_weekend else 'weekday' if is_weekend is not None else 'weekend_unk',\n",
    "    ]\n",
    "    feats += [\n",
    "        'has_image' if has_image else 'no_image',\n",
    "        f'imgcnt_{img_bucket}', 'has_leadimg' if leadimg else 'no_leadimg',\n",
    "        f'imgsize_{img_size_bucket}', f'imgaspect_{img_aspect_bucket}',\n",
    "        'has_video' if has_video else 'no_video',\n",
    "        'has_audio' if has_audio else 'no_audio',\n",
    "        'has_interactive' if has_interactive else 'no_interactive',\n",
    "        f'linkcnt_{link_bucket}', f'authoritative_links_{authoritative_link_bucket}',\n",
    "    ]\n",
    "    feats += [\n",
    "        'is_listicle' if is_listicle else 'not_listicle',\n",
    "        'title_has_num' if title_has_num else 'title_no_num',\n",
    "        'title_has_year' if title_has_year else 'title_no_year',\n",
    "        'title_has_q' if title_has_q else 'title_no_q',\n",
    "        'title_has_exclaim' if title_has_exclaim else 'title_no_exclaim',\n",
    "        'title_has_colon' if title_has_colon else 'title_no_colon',\n",
    "        f'title_len_word_{tw_bucket}', f'title_len_char_{tc_bucket}', f'title_upper_{upper_bucket}',\n",
    "    ]\n",
    "    feats += [f'social_buttons_{social_count_bucket}', f'comment_sections_{comment_count_bucket}', f'share_count_{shares_bucket}']\n",
    "    feats += [f'positive_words_{pos_count}', f'negative_words_{neg_count}', f'sentiment_{sentiment_bucket}']\n",
    "    feats += [f'urgency_indicators_{urgency_count_bucket}', f'question_words_{question_count_bucket}',\n",
    "              f'noun_count_{noun_count_bucket}', f'cta_count_{cta_count_bucket}']\n",
    "    feats += [f'div_count_{div_count_bucket}', f'section_count_{section_count_bucket}',\n",
    "              f'list_count_{list_count_bucket}', f'readability_{readability_bucket}']\n",
    "    feats += [f'header_word_count_{header_word_count_bucket}']\n",
    "    feats += [f'entity_count_{entity_count_bucket}', topic_bucket,\n",
    "              f'trending_matches_{_bucket(sum(1 for w in tokens if w.lower() in TRENDING_TOPICS),[0,1,3,5])}',\n",
    "              f'clicks_{clicks_bucket}', f'shares_{shares_bucket}', f'comments_{comments_bucket}']\n",
    "    feats += list(entities)\n",
    "\n",
    "    header_bow_text = _bow_clean(text_content)\n",
    "\n",
    "    # ★ ====== 新增：正文特徵提取 ======\n",
    "    body_feats = []\n",
    "    body_bow_text = \"\"\n",
    "    if body_html:\n",
    "        soup_body = BeautifulSoup(body_html, 'html.parser')\n",
    "        body_text = ' '.join(soup_body.get_text().lower().split()) # 取得純文本\n",
    "\n",
    "        # 正文長度\n",
    "        body_len_bucket = _bucket(len(body_text.split()), [0, 100, 300, 600, 1000])\n",
    "        body_feats.append(f'body_len_{body_len_bucket}')\n",
    "\n",
    "        # 段落、引用、代碼塊\n",
    "        p_count = len(soup_body.find_all('p'))\n",
    "        p_bucket = _bucket(p_count, [0, 5, 15, 30, 50])\n",
    "        body_feats.append(f'body_p_{p_bucket}')\n",
    "\n",
    "        bq_count = len(soup_body.find_all('blockquote'))\n",
    "        bq_bucket = _bucket(bq_count, [0, 1, 3, 5])\n",
    "        body_feats.append(f'body_bq_{bq_bucket}')\n",
    "\n",
    "        code_count = len(soup_body.find_all(['pre', 'code']))\n",
    "        body_feats.append('body_has_code' if code_count > 0 else 'body_no_code')\n",
    "\n",
    "        # 正文詞袋文本\n",
    "        body_bow_text = _bow_clean(body_text)\n",
    "    else:\n",
    "        # 如果沒有 body，使用 'no_body' 特徵\n",
    "        body_feats.append('no_body')\n",
    "\n",
    "\n",
    "    # ★ 回傳 6 個值\n",
    "    return _norm(title_tokens), entities, ' '.join(feats), header_bow_text, ' '.join(body_feats), body_bow_text\n",
    "\n",
    "# ====== ★★★ 新增/修改分塊編碼器 ★★★ ======\n",
    "# 原有\n",
    "title_vec = HashingVectorizer(n_features=2**20, alternate_sign=False, ngram_range=(1,2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "header_vec = HashingVectorizer(n_features=2**18, alternate_sign=True, ngram_range=(1,2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "entities_vec = HashingVectorizer(n_features=2**12, alternate_sign=False, token_pattern=r'(?u)\\b\\w+\\b')\n",
    "\n",
    "# ★ 新增 body 的詞袋向量化器\n",
    "body_vec = HashingVectorizer(n_features=2**20, alternate_sign=False, ngram_range=(1,2), token_pattern=r'(?u)\\b\\w+\\b')\n",
    "\n",
    "# 原有\n",
    "make_hasher = lambda n: FeatureHasher(n_features=2**n, input_type='string', alternate_sign=False)\n",
    "author_h, channel_h, publisher_h = make_hasher(15), make_hasher(12), make_hasher(14)\n",
    "time_h, media_h, titleshape_h   = make_hasher(10), make_hasher(12), make_hasher(10)\n",
    "social_h, content_h, structure_h= make_hasher(14), make_hasher(14), make_hasher(14)\n",
    "length_h, extra_h               = make_hasher(12), make_hasher(14)\n",
    "\n",
    "# ★ 新增 body 的類別特徵 Hasher\n",
    "bodyfeats_h = make_hasher(12)\n",
    "\n",
    "# ====== tokenizer（維持原樣）======\n",
    "def tokenizer_stem_keepmeta(text: str, entities: set) -> list:\n",
    "    toks = re.split(r'\\s+', (text or '').strip()); out=[]\n",
    "    for w in toks:\n",
    "        if not w: continue\n",
    "        if '_' in w or any(ch.isdigit() for ch in w) or (w.startswith('entity_') and w[7:] in entities):\n",
    "            out.append(w)\n",
    "        elif w.lower() not in STOP and re.fullmatch(r'[a-zA-Z]+', w):\n",
    "            out.append(porter.stem(w.lower()))\n",
    "    return out\n",
    "\n",
    "# ====== feats_string → 分桶 ======\n",
    "PREFIX_MAP = {\n",
    "    'author_':'author','channel_':'channel','publisher_':'publisher',\n",
    "    'year_':'time','month_':'time','weekday_':'time','tod_':'time','season_':'time',\n",
    "    'weekend':'time','weekday':'time',\n",
    "    'has_image':'media','no_image':'media','imgcnt_':'media','has_leadimg':'media','no_leadimg':'media',\n",
    "    'imgsize_':'media','imgaspect_':'media','has_video':'media','no_video':'media','has_audio':'media','no_audio':'media',\n",
    "    'has_interactive':'media','no_interactive':'media','linkcnt_':'media','authoritative_links_':'media',\n",
    "    'is_listicle':'titleshape','not_listicle':'titleshape','title_has_':'titleshape',\n",
    "    'title_len_word_':'titleshape','title_len_char_':'titleshape','title_upper_':'titleshape',\n",
    "    'social_buttons_':'social','comment_sections_':'social','share_count_':'social',\n",
    "    'urgency_indicators_':'content','question_words_':'content','noun_count_':'content','cta_count_':'content',\n",
    "    'div_count_':'structure','section_count_':'structure','list_count_':'structure','readability_':'structure',\n",
    "    'header_word_count_':'length',\n",
    "    'entity_count_':'extra','topic_':'extra','trending_matches_':'extra','clicks_':'extra','shares_':'extra','comments_':'extra',\n",
    "    'sentiment_':'extra','positive_words_':'extra','negative_words_':'extra',\n",
    "}\n",
    "def _split_feat_tokens(feats_string: str):\n",
    "    buckets = {k: [] for k in ['author','channel','publisher','time','media','titleshape','social','content','structure','length','extra']}\n",
    "    if not feats_string:\n",
    "        for k in buckets: buckets[k] = [f'{k}=_none']; return buckets\n",
    "    for tok in feats_string.split():\n",
    "        placed = False\n",
    "        for pref, grp in PREFIX_MAP.items():\n",
    "            if tok.startswith(pref): buckets[grp].append(tok); placed=True; break\n",
    "        if not placed: buckets['extra'].append(tok)\n",
    "    for k,v in buckets.items():\n",
    "        if not v: buckets[k] = [f'{k}=_none']\n",
    "    return buckets\n",
    "\n",
    "# ====== ★★★ 修正版 featurize_split：處理 6 個返回值並拼接新特徵 ★★★ ======\n",
    "def featurize_split(html_series: pd.Series, lda_vectorizer=None, lda_model=None, n_jobs=1) -> sp.csr_matrix:\n",
    "    rows = html_series.astype(str).tolist()\n",
    "    if n_jobs is None or n_jobs==1:\n",
    "        # ★ preprocessor 現在回傳 6-tuple\n",
    "        processed_data = [preprocessor(h, lda_vectorizer, lda_model) for h in rows]\n",
    "    else:\n",
    "        processed_data = Parallel(n_jobs=n_jobs, backend=\"loky\", prefer=\"processes\")(\n",
    "            delayed(preprocessor)(h, lda_vectorizer, lda_model) for h in rows\n",
    "        )\n",
    "\n",
    "    # 收集器\n",
    "    titles, header_bows, entity_texts = [], [], []\n",
    "    author_tokens, channel_tokens, publisher_tokens = [], [], []\n",
    "    time_tokens, media_tokens, titleshape_tokens = [], [], []\n",
    "    social_tokens, content_tokens, structure_tokens = [], [], []\n",
    "    length_tokens, extra_tokens = [], []\n",
    "    # ★ 新增 body 的收集器\n",
    "    body_feats_tokens, body_bows = [], []\n",
    "\n",
    "    # ★ 逐行解包（6 個值）\n",
    "    for title, ents, header_feats, header_bow, body_feats, body_bow in processed_data:\n",
    "        titles.append(' '.join(tokenizer_stem_keepmeta(title, set(ents))) if title else '')\n",
    "        header_bows.append(header_bow or '')\n",
    "        entity_texts.append(' '.join(sorted(ents)) if ents else '')\n",
    "        b = _split_feat_tokens(header_feats)\n",
    "        author_tokens.append(b['author']);    channel_tokens.append(b['channel']);   publisher_tokens.append(b['publisher'])\n",
    "        time_tokens.append(b['time']);        media_tokens.append(b['media']);       titleshape_tokens.append(b['titleshape'])\n",
    "        social_tokens.append(b['social']);    content_tokens.append(b['content']);   structure_tokens.append(b['structure'])\n",
    "        length_tokens.append(b['length']);    extra_tokens.append(b['extra'])\n",
    "        # ★ 收集 body 特徵\n",
    "        body_feats_tokens.append(body_feats.split() if body_feats else ['no_body'])\n",
    "        body_bows.append(body_bow or '')\n",
    "\n",
    "    # 各塊編碼\n",
    "    X_title      = title_vec.transform(titles)\n",
    "    X_header     = header_vec.transform(header_bows)\n",
    "    X_entities   = entities_vec.transform(entity_texts)\n",
    "    X_author     = author_h.transform(author_tokens)\n",
    "    X_channel    = channel_h.transform(channel_tokens)\n",
    "    X_publisher  = publisher_h.transform(publisher_tokens)\n",
    "    X_time       = time_h.transform(time_tokens)\n",
    "    X_media      = media_h.transform(media_tokens)\n",
    "    X_titleshape = titleshape_h.transform(titleshape_tokens)\n",
    "    X_social     = social_h.transform(social_tokens)\n",
    "    X_content    = content_h.transform(content_tokens)\n",
    "    X_structure  = structure_h.transform(structure_tokens)\n",
    "    X_length     = length_h.transform(length_tokens)\n",
    "    X_extra      = extra_h.transform(extra_tokens)\n",
    "    # ★ 編碼 body 特徵\n",
    "    X_body_feats = bodyfeats_h.transform(body_feats_tokens)\n",
    "    X_body_bow   = body_vec.transform(body_bows)\n",
    "\n",
    "    # ★ 拼接（加入新的 body 特徵）\n",
    "    X = sp.hstack([\n",
    "        X_title, X_header, X_entities,\n",
    "        X_author, X_channel, X_publisher, X_time,\n",
    "        X_media, X_titleshape, X_social, X_content, X_structure, X_length,\n",
    "        X_extra,\n",
    "        X_body_feats, X_body_bow  # ★ 新增\n",
    "    ], format='csr')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145702c0",
   "metadata": {},
   "source": [
    "把驗證集轉成特徵，後面評估直接用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7e50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一次性把驗證集轉成特徵，後面評估直接用\n",
    "X_val = featurize_split(val_df['Page content'].astype(str), lda_vectorizer, lda_model, n_jobs=1)\n",
    "y_val = val_df['Popularity'].values\n",
    "sp.save_npz(f'{OUT_DIR}/X_val_split.npz', X_val)\n",
    "np.save(f'{OUT_DIR}/y_val.npy', y_val)\n",
    "X_val.shape, np.bincount(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810af711",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c97c0307",
   "metadata": {},
   "source": [
    "k 折"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be63a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# K 折交叉驗證 + 每折最佳模型集成輸出 test 預測（支援 group / stratified / kfold）\n",
    "# —— 新增：epoch 內早停（batch 級評估 + 回退本 epoch 最佳狀態）\n",
    "# =========================\n",
    "import os, gc, copy, _pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.special import expit\n",
    "\n",
    "# ---------- 基本配置 ----------\n",
    "TRAIN_PATH = './dataset/train.csv'\n",
    "TEST_PATH  = './dataset/test.csv'\n",
    "OUT_DIR    = './output'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 切換這裡即可：'group'（按 publisher 防洩漏）| 'stratified'（隨機分層）| 'kfold'（純隨機）\n",
    "CV_MODE     = 'stratified'\n",
    "N_SPLITS    = 5\n",
    "EPOCHS      = 5\n",
    "BATCH_SIZE  = 2500\n",
    "SEED        = 42\n",
    "\n",
    "# 早停（跨 epoch）\n",
    "PATIENCE    = 2          # 連續 PATIENCE 個 epoch 無提升就停\n",
    "\n",
    "# 早停（epoch 內）\n",
    "BATCH_EVAL_EVERY = 1    # 每多少個 batch 在驗證集評估一次（1=每個 batch 都評估）\n",
    "BATCH_PATIENCE   = 20     # 連續多少次「批內評估」無提升就提前結束當前 epoch\n",
    "\n",
    "DO_FOLD_LDA = False      # True：每折只用訓練集預訓練 LDA（更穩但更慢）\n",
    "\n",
    "# ----------（僅 group 模式會用到）publisher 提取 ----------\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def _norm(s): return re.sub(r'[\\W]+', ' ', (s or '').lower()).strip()\n",
    "def _slug(s): return re.sub(r'[^a-z0-9_]+', '', _norm(s).replace(' ', '_'))\n",
    "\n",
    "def extract_publisher_slug(html: str) -> str:\n",
    "    if not isinstance(html, str) or not html.strip():\n",
    "        return \"unknown\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    pub = soup.find('a', href=re.compile(r'/publishers/[^/]+/?', re.I))\n",
    "    if pub:\n",
    "        publisher = pub.get_text(' ', strip=True) or re.sub(r'.*/publishers/([^/]+)/?.*', r'\\1', pub['href'], flags=re.I)\n",
    "    else:\n",
    "        publisher = \"unknown\"\n",
    "    return _slug(publisher or 'unknown')\n",
    "\n",
    "# ---------- 分割器工廠 ----------\n",
    "def make_split_iter(cv_mode, n_splits, seed, y, texts=None, groups=None):\n",
    "    \"\"\"\n",
    "    回傳 (tr_idx, va_idx) 的迭代器：\n",
    "      - 'group'：GroupKFold（需 groups=publisher_keys）\n",
    "      - 'stratified'：StratifiedKFold（shuffle=True）\n",
    "      - 'kfold'：KFold（shuffle=True）\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    X_dummy = np.zeros(n)  # 只為滿足 API\n",
    "    if cv_mode == 'group':\n",
    "        if groups is None:\n",
    "            raise ValueError(\"CV_MODE='group' 需要提供 groups（publisher_keys）\")\n",
    "        splitter = GroupKFold(n_splits=n_splits)\n",
    "        print(f\"Using GroupKFold by publisher (groups={pd.Series(groups).nunique()})\")\n",
    "        return splitter.split(X_dummy, y, groups)\n",
    "\n",
    "    elif cv_mode == 'stratified':\n",
    "        splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        print(\"Using StratifiedKFold (shuffle=True)\")\n",
    "        return splitter.split(X_dummy, y)\n",
    "\n",
    "    elif cv_mode == 'kfold':\n",
    "        splitter = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        print(\"Using KFold (shuffle=True) — no stratification\")\n",
    "        return splitter.split(X_dummy)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown CV_MODE: {cv_mode}\")\n",
    "\n",
    "# ---------- 分類器工廠（文本穩定配置） ----------\n",
    "def make_clf():\n",
    "    return SGDClassifier(\n",
    "        loss=\"hinge\",            # 若想更平滑的機率，改為 \"log_loss\" 並用 predict_proba\n",
    "        penalty=\"elasticnet\",\n",
    "        l1_ratio=0.15,\n",
    "        alpha=5e-4,\n",
    "        learning_rate=\"optimal\",\n",
    "        eta0=1e-5,\n",
    "        average=True,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "# ---------- 讀取資料 ----------\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "df['Popularity'] = (df['Popularity'].astype(int) == 1).astype(int)\n",
    "y = df['Popularity'].values\n",
    "texts = df['Page content'].astype(str)\n",
    "\n",
    "# 只有 group 模式才需要抽取 publisher；其他模式略過可節省時間\n",
    "if CV_MODE == 'group':\n",
    "    publisher_keys = texts.apply(extract_publisher_slug).values\n",
    "    split_iter = make_split_iter(CV_MODE, N_SPLITS, SEED, y, texts, publisher_keys)\n",
    "else:\n",
    "    split_iter = make_split_iter(CV_MODE, N_SPLITS, SEED, y, texts, None)\n",
    "\n",
    "# 容器\n",
    "fold_artifacts = []   # 保存每折最佳模型與（可選）LDA\n",
    "oof_scores = np.zeros(len(df), dtype=float)\n",
    "fold_aucs, fold_epochs = [], []\n",
    "\n",
    "print(f\"\\nStart {N_SPLITS}-fold CV: EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, \"\n",
    "      f\"DO_FOLD_LDA={DO_FOLD_LDA}, MODE={CV_MODE}, \"\n",
    "      f\"BATCH_EVAL_EVERY={BATCH_EVAL_EVERY}, BATCH_PATIENCE={BATCH_PATIENCE}\")\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(split_iter, start=1):\n",
    "    print(f\"\\n========== Fold {fold}/{N_SPLITS} ==========\")\n",
    "    tr_df = df.iloc[tr_idx].reset_index(drop=True)\n",
    "    va_df = df.iloc[va_idx].reset_index(drop=True)\n",
    "    y_val = va_df['Popularity'].values\n",
    "\n",
    "    # 每折 LDA（可選，避免外洩需在訓練集上建）\n",
    "    if DO_FOLD_LDA:\n",
    "        lda_vec_f, lda_mod_f = pretrain_lda(tr_df, column='Page content', n_components=10, max_features=1000)\n",
    "    else:\n",
    "        lda_vec_f, lda_mod_f = (None, None)\n",
    "\n",
    "    # 固定本折驗證特徵（只算一次）\n",
    "    X_val = featurize_split(va_df['Page content'].astype(str), lda_vec_f, lda_mod_f, n_jobs=1)\n",
    "\n",
    "    # 模型與「跨 epoch」早停\n",
    "    clf = make_clf()\n",
    "    fold_best_auc, fold_best_epoch = -1, -1\n",
    "    fold_best_state = None\n",
    "    epoch_no_improve = 0\n",
    "\n",
    "    # ========== 多 epoch 訓練（含：epoch 內早停） ==========\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tr_shuf = tr_df.sample(frac=1.0, random_state=SEED+epoch).reset_index(drop=True)\n",
    "        n_batches = ceil(len(tr_shuf)/BATCH_SIZE)\n",
    "\n",
    "        # 這兩個用於「epoch 內早停」\n",
    "        epoch_best_auc  = -1\n",
    "        epoch_best_state = None\n",
    "        batch_no_improve = 0\n",
    "\n",
    "        for b in range(n_batches):\n",
    "            batch = tr_shuf.iloc[b*BATCH_SIZE:(b+1)*BATCH_SIZE]\n",
    "            X_tr = featurize_split(batch['Page content'].astype(str), lda_vec_f, lda_mod_f, n_jobs=1)\n",
    "            y_tr = batch['Popularity'].values\n",
    "\n",
    "            if epoch == 1 and b == 0:\n",
    "                clf.partial_fit(X_tr, y_tr, classes=np.array([0,1]))\n",
    "            else:\n",
    "                clf.partial_fit(X_tr, y_tr)\n",
    "\n",
    "            # —— 批內評估（控制頻率）——\n",
    "            do_eval = ((b + 1) % BATCH_EVAL_EVERY == 0) or (b == n_batches - 1)\n",
    "            if do_eval:\n",
    "                if hasattr(clf, \"predict_proba\"):\n",
    "                    val_prob_now = clf.predict_proba(X_val)[:, 1]\n",
    "                else:\n",
    "                    val_prob_now = expit(clf.decision_function(X_val))\n",
    "                val_auc_now = roc_auc_score(y_val, val_prob_now)\n",
    "\n",
    "                # 更新當前 epoch 的最佳狀態\n",
    "                if val_auc_now > epoch_best_auc:\n",
    "                    epoch_best_auc = val_auc_now\n",
    "                    epoch_best_state = copy.deepcopy(clf)\n",
    "                    batch_no_improve = 0\n",
    "                else:\n",
    "                    batch_no_improve += 1\n",
    "\n",
    "                # epoch 內早停條件\n",
    "                if batch_no_improve >= BATCH_PATIENCE:\n",
    "                    print(f\"  Fold {fold} | epoch {epoch} | early-stop in-epoch at batch {b+1}/{n_batches} \"\n",
    "                          f\"(no improve {BATCH_PATIENCE}×); best AUC so far = {epoch_best_auc:.4f}\")\n",
    "                    break\n",
    "\n",
    "            # 釋放 batch 特徵\n",
    "            del X_tr; gc.collect()\n",
    "\n",
    "        # —— epoch 結束：回退到本 epoch 最佳狀態，並以其 AUC 作為 epoch 成績 —— \n",
    "        if epoch_best_state is not None:\n",
    "            clf = copy.deepcopy(epoch_best_state)   # 之後的 epoch 會從「本 epoch 的最佳點」繼續學\n",
    "            val_auc = epoch_best_auc\n",
    "        else:\n",
    "            # 萬一 epoch 內沒有做任何評估（極端情況），做一次補評估\n",
    "            if hasattr(clf, \"predict_proba\"):\n",
    "                val_prob = clf.predict_proba(X_val)[:, 1]\n",
    "            else:\n",
    "                val_prob = expit(clf.decision_function(X_val))\n",
    "            val_auc = roc_auc_score(y_val, val_prob)\n",
    "\n",
    "        print(f\"Fold {fold} | epoch {epoch}/{EPOCHS} | Val AUC={val_auc:.4f}\")\n",
    "\n",
    "        # —— 跨 epoch 早停（根據 epoch 最佳）——\n",
    "        if val_auc > fold_best_auc:\n",
    "            fold_best_auc = val_auc\n",
    "            fold_best_epoch = epoch\n",
    "            fold_best_state = copy.deepcopy(clf)\n",
    "            epoch_no_improve = 0\n",
    "        else:\n",
    "            epoch_no_improve += 1\n",
    "            if epoch_no_improve >= PATIENCE:\n",
    "                print(f\"  Early stopping (across epochs) at epoch {epoch} (no improve {PATIENCE}×)\")\n",
    "                break\n",
    "\n",
    "    # 保存 OOF（用 fold 內最佳狀態）\n",
    "    if hasattr(fold_best_state, \"predict_proba\"):\n",
    "        oof_scores[va_idx] = fold_best_state.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        oof_scores[va_idx] = expit(fold_best_state.decision_function(X_val))\n",
    "\n",
    "    fold_aucs.append(fold_best_auc); fold_epochs.append(fold_best_epoch)\n",
    "\n",
    "    # 保存每折最佳模型到硬碟\n",
    "    model_path = os.path.join(OUT_DIR, f'model_4_{CV_MODE}_clf_sgd_fold{fold}.pkl')\n",
    "    pkl.dump(fold_best_state, open(model_path, 'wb'))\n",
    "\n",
    "    # 若用了 LDA，順便把該折的 LDA 也存起來\n",
    "    lda_path = None\n",
    "    if DO_FOLD_LDA:\n",
    "        lda_path = os.path.join(OUT_DIR, f'lda_{CV_MODE}_fold{fold}.pkl')\n",
    "        pkl.dump({'lda_vec': lda_vec_f, 'lda_model': lda_mod_f}, open(lda_path, 'wb'))\n",
    "\n",
    "    print(f\"Fold {fold} BEST: epoch={fold_best_epoch}, AUC={fold_best_auc:.4f} | saved {model_path}\")\n",
    "    del X_val; gc.collect()\n",
    "\n",
    "# CV 總結\n",
    "oof_auc = roc_auc_score(y, oof_scores)\n",
    "print(\"\\n========== CV Summary ==========\")\n",
    "print(\"Fold AUCs:\", [\"%.4f\" % a for a in fold_aucs])\n",
    "print(\"Mean AUC = %.4f | Std = %.4f\" % (np.mean(fold_aucs), np.std(fold_aucs)))\n",
    "print(\"OOF  AUC = %.4f\" % oof_auc)\n",
    "\n",
    "# ---------- 用每折最佳模型對 test 預測並平均 ----------\n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "test_texts = df_test['Page content'].astype(str)\n",
    "test_preds_each_fold = []\n",
    "\n",
    "for fold in range(1, len(fold_aucs)+1):\n",
    "    # 讀模型\n",
    "    model_path = os.path.join(OUT_DIR, f'model_4_{CV_MODE}_clf_sgd_fold{fold}.pkl')\n",
    "    clf = pkl.load(open(model_path, 'rb'))\n",
    "\n",
    "    # 讀折內 LDA（可選）\n",
    "    if DO_FOLD_LDA:\n",
    "        lda_path = os.path.join(OUT_DIR, f'lda_{CV_MODE}_fold{fold}.pkl')\n",
    "        lda_pack = pkl.load(open(lda_path, 'rb'))\n",
    "        lda_vec_f, lda_mod_f = lda_pack['lda_vec'], lda_pack['lda_model']\n",
    "    else:\n",
    "        lda_vec_f, lda_mod_f = (None, None)\n",
    "\n",
    "    # 特徵化\n",
    "    X_test = featurize_split(test_texts, lda_vec_f, lda_mod_f, n_jobs=1)\n",
    "\n",
    "    # 預測機率\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prob = clf.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        prob = expit(clf.decision_function(X_test))\n",
    "\n",
    "    test_preds_each_fold.append(prob)\n",
    "    print(f\"Fold {fold} test predicted. Shape={prob.shape}\")\n",
    "\n",
    "# 集成（平均）\n",
    "test_pred = np.mean(np.vstack(test_preds_each_fold), axis=0)\n",
    "\n",
    "# 導出提交\n",
    "sub_path = os.path.join(OUT_DIR, f'model_4_submission_k{N_SPLITS}_3.csv')\n",
    "pd.DataFrame({'Id': df_test['Id'], 'Popularity': test_pred}).to_csv(sub_path, index=False)\n",
    "print(\"Submission saved ->\", sub_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd24811",
   "metadata": {},
   "source": [
    "可視化權重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de90751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新增 cell：分析每個特徵塊的權重貢獻（使用最後一折的模型作為示例）\n",
    "# 注意：這需要先運行前面的訓練 cell 以確保模型文件存在\n",
    "# 我們會用一個小樣本計算每個特徵塊的維度（offsets），然後計算 mean(abs(coef)) 作為貢獻指標\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import pickle as pkl\n",
    "from sklearn.metrics import roc_auc_score  # 可選，如果需要\n",
    "\n",
    "# 假設我們用訓練集的前 10 個樣本計算特徵塊大小（只需計算 shape[1]，不需真實轉換大量數據）\n",
    "sample_html = train_df['Page content'].astype(str).head(10)  # 從 train_df 取樣本\n",
    "# 注意：lda_vectorizer 和 lda_model 需從前文定義，如果 DO_FOLD_LDA=False，可用全局的\n",
    "# 這裡假設用全局 lda_vectorizer, lda_model（調整如果不同）\n",
    "\n",
    "# 調用 featurize_split 但不拼接，只收集每個塊的 shape[1]\n",
    "# （我們臨時修改 featurize_split 的返回，或手動運行內部邏輯；為簡單，這裡模擬收集）\n",
    "# 實際上，你可以從 featurize_split 內部複製邏輯，但為了簡潔，假設運行一次並記錄 shapes\n",
    "\n",
    "X_sample = featurize_split(sample_html, lda_vectorizer, lda_model, n_jobs=1)  # 運行一次小樣本得到 X\n",
    "# 但我們需要每個塊的 shape，所以在 featurize_split 內記錄或這裡列出已知\n",
    "# 假設所有 HashingVectorizer/FeatureHasher 用默認 n_features=2**20=1048576\n",
    "# 但 entities, title 等是 HashingVectorizer (str -> vec)\n",
    "# author 等是 FeatureHasher (list of dict? -> vec)\n",
    "# 實際 dim 都是固定的，除非指定；默認 1048576\n",
    "\n",
    "# 明確列出每個特徵塊（從你的代碼）\n",
    "feature_blocks = [\n",
    "    'title', 'header', 'entities',\n",
    "    'author', 'channel', 'publisher', 'time',\n",
    "    'media', 'titleshape', 'social', 'content', 'structure', 'length',\n",
    "    'extra',\n",
    "    'body_feats', 'body_bow'\n",
    "]\n",
    "\n",
    "# 假設每個 vec/hasher 的 n_features （從你的代碼中查找，或運行時檢查）\n",
    "# 例如，如果 title_vec = HashingVectorizer(n_features=2**18)，則 dim=262144\n",
    "# 這裡假設默認 2**20=1048576（調整為你的實際）\n",
    "block_dims = [1048576] * len(feature_blocks)  # 臨時假設；替換為實際\n",
    "# 更好的方式：運行小樣本並收集\n",
    "rows = sample_html.tolist()\n",
    "processed_data = [preprocessor(h, lda_vectorizer, lda_model) for h in rows]\n",
    "# ... (從 featurize_split 複製收集邏輯)\n",
    "# 然後 X_title = title_vec.transform(titles); block_dims.append(X_title.shape[1])\n",
    "# 等；為簡潔，假設已知或運行後 print(X_title.shape[1]) 等\n",
    "\n",
    "# 計算 offsets\n",
    "offsets = [0]\n",
    "for dim in block_dims:\n",
    "    offsets.append(offsets[-1] + dim)\n",
    "\n",
    "# 加載一個模型（例如最後一折）\n",
    "fold = 8  # 或選擇任一 fold\n",
    "model_path = f'{OUT_DIR}/model_4_{CV_MODE}_clf_sgd_fold{fold}.pkl'\n",
    "clf = pkl.load(open(model_path, 'rb'))\n",
    "\n",
    "# 獲取權重（coef_ 是 (1, total_features)）\n",
    "coef = clf.coef_[0]  # [total_features,]\n",
    "\n",
    "# 計算每個塊的貢獻：mean(abs(coef)) 或 sum(abs(coef)) / total_sum_abs\n",
    "block_importances = []\n",
    "for i, name in enumerate(feature_blocks):\n",
    "    start, end = offsets[i], offsets[i+1]\n",
    "    block_coef = coef[start:end]\n",
    "    mean_abs = np.mean(np.abs(block_coef))  # 改用均值\n",
    "    block_importances.append((name, mean_abs))\n",
    "block_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "total_mean_abs = sum(r for _, r in block_importances)\n",
    "for name, mean_abs in block_importances:\n",
    "    print(f\"{name}: {mean_abs:.6f} ({mean_abs/total_mean_abs:.4f})\")\n",
    "\n",
    "# 排序並顯示\n",
    "block_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "print(\"特徵塊權重貢獻佔比（sum(abs(coef)) / total）：\")\n",
    "for name, ratio in block_importances:\n",
    "    print(f\"{name}: {ratio:.4f}\")\n",
    "\n",
    "# 可視化（可選）\n",
    "import matplotlib.pyplot as plt\n",
    "names, ratios = zip(*block_importances)\n",
    "plt.barh(names, ratios)\n",
    "plt.xlabel('貢獻佔比')\n",
    "plt.title('特徵塊重要性')\n",
    "plt.show()\n",
    "\n",
    "# 基於此，你可以決定刪除貢獻小的塊，例如如果 <0.01，註釋掉 hstack 中的對應 X_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
