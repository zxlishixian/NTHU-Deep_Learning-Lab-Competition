{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf61e8cc",
   "metadata": {},
   "source": [
    "前置設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "406a7404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\11958\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\11958\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== 基礎 =====\n",
    "import os, re, numpy as np, pandas as pd\n",
    "from scipy import sparse as sp\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "\n",
    "# 其他\n",
    "from joblib import Parallel, delayed\n",
    "from math import ceil\n",
    "\n",
    "# ===== 路徑與隨機種子 =====\n",
    "DATA_DIR = './dataset'\n",
    "TRAIN_PATH = f'{DATA_DIR}/train.csv'\n",
    "TEST_PATH  = f'{DATA_DIR}/test.csv'\n",
    "OUT_DIR = './output'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ===== 速度開關（訓練時可先關慢模塊）=====\n",
    "FAST_NO_NER = True        # 關 NER\n",
    "FAST_NO_LDA = True        # 關 LDA 主題\n",
    "FAST_NO_TEXTSTAT = True   # 關可讀性評分\n",
    "\n",
    "# ===== NLTK 資源（缺什麼補什麼）=====\n",
    "for rid, name in [\n",
    "    ('tokenizers/punkt', 'punkt'),\n",
    "    ('corpora/stopwords', 'stopwords'),\n",
    "    ('corpora/wordnet', 'wordnet'),\n",
    "    ('sentiment/vader_lexicon', 'vader_lexicon'),\n",
    "    ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),\n",
    "    ('chunkers/maxent_ne_chunker', 'maxent_ne_chunker'),\n",
    "    ('corpora/words', 'words'),\n",
    "]:\n",
    "    try:\n",
    "        nltk.data.find(rid)\n",
    "    except LookupError:\n",
    "        nltk.download(name)\n",
    "\n",
    "# 全局 VADER（避免每條樣本重建）\n",
    "try:\n",
    "    VADER = SentimentIntensityAnalyzer()\n",
    "except Exception:\n",
    "    VADER = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa594143",
   "metadata": {},
   "source": [
    "導入數據與切分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e20af56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26000, 3) (1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Page content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt; &lt;span c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;html&gt;&lt;head&gt;&lt;div class=\"article-info\"&gt;&lt;span cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  Popularity                                       Page content\n",
       "0   0           0  <html><head><div class=\"article-info\"> <span c...\n",
       "1   1           1  <html><head><div class=\"article-info\"><span cl..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 將 Popularity {-1,1} → {0,1}；並切出 train/val\n",
    "df_all = pd.read_csv(TRAIN_PATH)\n",
    "df_all['Popularity'] = (df_all['Popularity'].astype(int) == 1).astype(int)\n",
    "\n",
    "TRAIN_SIZE = 26000\n",
    "VAL_SIZE   = 1000\n",
    "\n",
    "train_df = df_all.iloc[:TRAIN_SIZE].reset_index(drop=True)\n",
    "val_df   = df_all.iloc[TRAIN_SIZE:TRAIN_SIZE+VAL_SIZE].reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape, val_df.shape)\n",
    "train_df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eeaefd",
   "metadata": {},
   "source": [
    "工具函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9f64080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA skipped by FAST_NO_LDA=True\n"
     ]
    }
   ],
   "source": [
    "# 小工具\n",
    "_BODY_OPEN = re.compile(\n",
    "    r'(?is)<\\s*(section|div|article)\\b[^>]*\\b'\n",
    "    r'(?:article-content|article-body|content-body|post-content)\\b[^>]*>'\n",
    ")\n",
    "_MONTH = dict(jan='01', feb='02', mar='03', apr='04', may='05', jun='06',\n",
    "              jul='07', aug='08', sep='09', oct='10', nov='11', dec='12')\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    return re.sub(r'[\\W]+', ' ', (s or '').lower()).strip()\n",
    "\n",
    "def _slug(s: str) -> str:\n",
    "    return re.sub(r'[^a-z0-9_]+', '', _norm(s).replace(' ', '_'))\n",
    "\n",
    "def _bucket(n, edges):\n",
    "    if n is None: return 'unk'\n",
    "    for i in range(len(edges)-1):\n",
    "        if edges[i] <= n < edges[i+1]:\n",
    "            return f\"b{edges[i]}_{edges[i+1]}\"\n",
    "    return f\"b{edges[-1]}p\"\n",
    "\n",
    "def _aspect_bucket(w, h):\n",
    "    if not w or not h: return 'unk'\n",
    "    r = w / h\n",
    "    if r < 0.9: return 'tall'\n",
    "    if r < 1.2: return 'squareish'\n",
    "    if r < 1.8: return 'landscape'\n",
    "    return 'ultrawide'\n",
    "\n",
    "def _img_size_bucket(w, h):\n",
    "    if not w or not h: return 'unk'\n",
    "    area = (w or 0) * (h or 0)\n",
    "    if area < 80_000: return 'xs'\n",
    "    if area < 230_000: return 'sm'\n",
    "    if area < 920_000: return 'md'\n",
    "    if area < 2_100_000: return 'lg'\n",
    "    return 'xl'\n",
    "\n",
    "def _parse_wh_from_src(src: str):\n",
    "    if not src: return (None, None)\n",
    "    m = re.search(r'/(\\d{2,5})x(\\d{2,5})/', src)\n",
    "    return (int(m.group(1)), int(m.group(2))) if m else (None, None)\n",
    "\n",
    "TRENDING_TOPICS = {\n",
    "    'elon_musk', 'ai', 'climate_change', 'covid', 'blockchain', 'taiwan',\n",
    "    'tesla', 'space', 'crypto', 'elections'\n",
    "}\n",
    "\n",
    "# LDA（可選）\n",
    "def pretrain_lda(df, column='Page content', n_components=10, max_features=1000, max_text_len=500):\n",
    "    def extract_text(html):\n",
    "        if not isinstance(html, str) or not html.strip(): return \"\"\n",
    "        m = _BODY_OPEN.search(html)\n",
    "        header_html = html[:m.start()] if m else html\n",
    "        soup = BeautifulSoup(header_html, 'html.parser')\n",
    "        return ' '.join(soup.get_text().lower().split()[:max_text_len])\n",
    "\n",
    "    corpus = [extract_text(x) for x in df[column].astype(str)]\n",
    "    if not any(corpus): return None, None\n",
    "    vec = CountVectorizer(max_features=max_features, stop_words='english')\n",
    "    X = vec.fit_transform(corpus)\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, random_state=42)\n",
    "    lda.fit(X)\n",
    "    return vec, lda\n",
    "\n",
    "lda_vectorizer, lda_model = (None, None)\n",
    "if not FAST_NO_LDA:\n",
    "    _df_lda = df_all.dropna(subset=['Page content']).astype({'Page content':'str'})\n",
    "    lda_vectorizer, lda_model = pretrain_lda(_df_lda, 'Page content', n_components=10, max_features=1000)\n",
    "    print(\"LDA pretrained\")\n",
    "else:\n",
    "    print(\"LDA skipped by FAST_NO_LDA=True\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56102f5",
   "metadata": {},
   "source": [
    "預處理函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d6077bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 幫助函數：清洗 header 文本做詞袋 ======\n",
    "porter = PorterStemmer()\n",
    "STOP = set(stopwords.words('english'))\n",
    "\n",
    "def _bow_clean(txt: str) -> str:\n",
    "    toks = [w for w in re.findall(r'[A-Za-z]+', (txt or '').lower()) if w not in STOP]\n",
    "    return ' '.join(porter.stem(w) for w in toks)\n",
    "\n",
    "# ====== 修正版 preprocessor：回傳 4 個值（多 header_bow_text）======\n",
    "def preprocessor(html: str, lda_vectorizer=None, lda_model=None, max_text_len=500):\n",
    "    if not isinstance(html, str) or not html.strip():\n",
    "        # 也回傳第四個空詞袋\n",
    "        return \"empty_content\", set(), \"author_unknown channel_unknown publisher_unknown\", \"\"\n",
    "\n",
    "    # 只取 header\n",
    "    m = _BODY_OPEN.search(html)\n",
    "    header_html = html[:m.start()] if m else html\n",
    "    soup = BeautifulSoup(header_html, 'html.parser')\n",
    "\n",
    "    # 標題\n",
    "    title_raw = None\n",
    "    h1 = soup.find('h1', class_=lambda c: (isinstance(c, list) and any('title' in x for x in c)) or (isinstance(c, str) and 'title' in c)) \\\n",
    "         or soup.find('h1')\n",
    "    if h1: title_raw = h1.get_text(' ', strip=True)\n",
    "    elif soup.title: title_raw = soup.title.get_text(' ', strip=True)\n",
    "    title_tokens = _norm(title_raw)\n",
    "\n",
    "    # 作者 / 頻道 / 發佈者\n",
    "    author = None\n",
    "    by = soup.find(class_=lambda c: c and ('byline' in c or 'author_name' in c))\n",
    "    if by: author = by.get_text(' ', strip=True)\n",
    "    if not author:\n",
    "        a = soup.find('a', href=re.compile(r'/author/[^/]+/?$', re.I))\n",
    "        if a: author = a.get_text(' ', strip=True)\n",
    "    author_slug = _slug(re.sub(r'^\\s*by\\s+', '', author or '', flags=re.I))\n",
    "    channel = None\n",
    "    art = soup.find('article')\n",
    "    if art and art.has_attr('data-channel'): channel = art['data-channel']\n",
    "    if not channel and art:\n",
    "        cls = ' '.join(art.get('class', []))\n",
    "        mch = re.search(r'\\b(news|tech|world|sports?|business|entertainment|culture|life|science)\\b', cls, re.I)\n",
    "        if mch: channel = mch.group(1)\n",
    "    channel_slug = _slug(channel or 'unknown')\n",
    "    publisher = None\n",
    "    pub = soup.find('a', href=re.compile(r'/publishers/[^/]+/?', re.I))\n",
    "    if pub: publisher = pub.get_text(' ', strip=True) or re.sub(r'.*/publishers/([^/]+)/?.*', r'\\1', pub['href'], flags=re.I)\n",
    "    publisher_slug = _slug(publisher or 'unknown')\n",
    "\n",
    "    # 時間\n",
    "    year = month = weekday = tod = season = None\n",
    "    is_weekend = None\n",
    "    tm = soup.find('time')\n",
    "    dt = tm['datetime'] if (tm and tm.has_attr('datetime')) else (tm.get_text(' ', strip=True) if tm else None)\n",
    "    if dt:\n",
    "        y = re.search(r'(20\\d{2}|19\\d{2})', dt);  year = y.group(1) if y else None\n",
    "        mo = re.search(r'-(\\d{2})-', dt) or re.search(r'\\b(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\b', dt, re.I)\n",
    "        if mo:\n",
    "            mm = mo.group(1).lower() if mo.lastindex else mo.group(0).lower()\n",
    "            month = _MONTH.get(mm, mm)\n",
    "        wd = re.search(r'\\b(mon|tue|wed|thu|fri|sat|sun)\\b', dt, re.I)\n",
    "        if wd: weekday = wd.group(1).lower(); is_weekend = weekday in ('sat','sun')\n",
    "        hh = re.search(r'\\b(\\d{2}):(\\d{2})', dt)\n",
    "        if hh:\n",
    "            h = int(hh.group(1))\n",
    "            tod = 'morning' if 5<=h<12 else 'afternoon' if 12<=h<17 else 'evening' if 17<=h<22 else 'night'\n",
    "        if month:\n",
    "            m_i = int(month)\n",
    "            season = 'spring' if 3<=m_i<=5 else 'summer' if 6<=m_i<=8 else 'autumn' if 9<=m_i<=11 else 'winter'\n",
    "\n",
    "    # 媒體元素\n",
    "    imgs = soup.find_all('img'); img_count = len(imgs); has_image = img_count>0\n",
    "    leadimg = soup.find(attrs={'data-fragment':'lead-image'}) is not None\n",
    "    max_w=max_h=None\n",
    "    for im in imgs:\n",
    "        w,h = _parse_wh_from_src(im.get('src',''))\n",
    "        if w and h:\n",
    "            if not max_w or (w*h) > ((max_w or 0)*(max_h or 0)):\n",
    "                max_w,max_h = w,h\n",
    "    img_size_bucket = _img_size_bucket(max_w, max_h)\n",
    "    img_aspect_bucket = _aspect_bucket(max_w, max_h)\n",
    "    videos = soup.find_all('video'); iframes = soup.find_all('iframe')\n",
    "    has_video = bool(videos) or any(re.search(r'(youtube|vimeo|dailymotion)', (fr.get('src') or ''), re.I) for fr in iframes)\n",
    "    audio = soup.find_all('audio'); has_audio = len(audio)>0\n",
    "    interactive_elements = soup.find_all(['canvas','svg', lambda tag: tag.name=='div' and 'interactive' in (tag.get('class') or [])])\n",
    "    has_interactive = len(interactive_elements)>0\n",
    "    link_count = len(soup.find_all('a'))\n",
    "    link_bucket = _bucket(link_count, [0,1,3,6,10])\n",
    "    img_bucket  = _bucket(img_count, [0,1,3,5])\n",
    "\n",
    "    authoritative_domains = ['.edu','.gov','.org']\n",
    "    authoritative_links = sum(1 for a in soup.find_all('a') if any(d in (a.get('href') or '').lower() for d in authoritative_domains))\n",
    "    authoritative_link_bucket = _bucket(authoritative_links, [0,1,3,5])\n",
    "\n",
    "    # 標題形態\n",
    "    raw = title_raw or ''\n",
    "    title_has_num = bool(re.search(r'\\d', raw))\n",
    "    title_has_year = bool(re.search(r'\\b(19|20)\\d{2}\\b', raw))\n",
    "    title_has_q = '?' in raw\n",
    "    title_has_exclaim = '!' in raw\n",
    "    title_has_colon = ':' in raw\n",
    "    is_listicle = bool(re.match(r'^\\s*\\d+', raw))\n",
    "    upper_ratio = (sum(ch.isupper() for ch in raw) / max(1, sum(ch.isalpha() for ch in raw)))\n",
    "    upper_bucket = 'low' if upper_ratio < 0.15 else 'mid' if upper_ratio < 0.4 else 'high'\n",
    "    title_word_len = len(_norm(raw).split()); tw_bucket = _bucket(title_word_len, [0,4,8,12,20])\n",
    "    title_char_len = len(re.sub(r'\\s+','',raw)); tc_bucket = _bucket(title_char_len, [0,30,60,90,140])\n",
    "\n",
    "    # 社交\n",
    "    social_keywords = ['share','twitter','facebook','linkedin','whatsapp','telegram']\n",
    "    social_elements = soup.find_all(lambda tag: any(\n",
    "        kw in (tag.get('class') or []) or kw in (tag.get('id') or '') or kw in tag.get_text().lower()\n",
    "        for kw in social_keywords\n",
    "    ))\n",
    "    social_count = len(social_elements)\n",
    "    social_count_bucket = _bucket(social_count, [0,1,3,5])\n",
    "    share_count = 0\n",
    "    for elem in social_elements:\n",
    "        m = re.search(r'(\\d+)\\s*(shares?|likes?|retweets?)', elem.get_text(), re.I)\n",
    "        if m: share_count += int(m.group(1))\n",
    "    shares_bucket = _bucket(share_count, [0,10,100,1000])\n",
    "\n",
    "    comment_selectors = ['.comments','#comments','.comment','.discussion']\n",
    "    comment_count = sum(len(soup.select(sel)) for sel in comment_selectors)\n",
    "    comment_count_bucket = _bucket(comment_count, [0,1,3,5])\n",
    "\n",
    "    # 內容與情感\n",
    "    text_content = ' '.join(soup.get_text().lower().split()[:max_text_len])\n",
    "    sentiment_compound = VADER.polarity_scores(text_content)['compound'] if 'VADER' in globals() and VADER else 0.0\n",
    "    sentiment_bucket = ('strong_positive' if sentiment_compound>0.5 else\n",
    "                        'positive' if sentiment_compound>0.05 else\n",
    "                        'strong_negative' if sentiment_compound<-0.5 else\n",
    "                        'negative' if sentiment_compound<-0.05 else 'neutral')\n",
    "    positive_words = ['amazing','great','excellent','wonderful','best','success','win','good','positive']\n",
    "    negative_words = ['terrible','awful','bad','worst','failure','lose','problem','negative']\n",
    "    pos_count = sum(1 for w in positive_words if w in text_content)\n",
    "    neg_count = sum(1 for w in negative_words if w in text_content)\n",
    "\n",
    "    # 緊急/問句/名詞/CTA\n",
    "    cta_phrases = ['read more','subscribe now','click here','learn more','join us','sign up']\n",
    "    cta_count = sum(1 for p in cta_phrases if p in text_content)\n",
    "    cta_count_bucket = _bucket(cta_count, [0,1,3,5])\n",
    "\n",
    "    urgency_indicators = ['breaking','urgent','alert','crisis','emergency','important']\n",
    "    urgency_count = sum(1 for w in urgency_indicators if w in text_content)\n",
    "    urgency_count_bucket = _bucket(urgency_count, [0,1,3,5])\n",
    "\n",
    "    question_words = ['what','why','how','when','where','who']\n",
    "    question_count = sum(1 for w in question_words if w in text_content)\n",
    "    question_count_bucket = _bucket(question_count, [0,1,3,5])\n",
    "\n",
    "    tokens = word_tokenize(text_content)\n",
    "    try:\n",
    "        tagged = pos_tag(tokens)\n",
    "    except Exception:\n",
    "        tagged = [(w,'NN') for w in tokens]\n",
    "    nouns = [w for w,pos_ in tagged if pos_.startswith('NN') and w.lower() not in STOP]\n",
    "    noun_count_bucket = _bucket(len(nouns), [0,5,10,20,50])\n",
    "\n",
    "    # NER（可關）\n",
    "    entities = set()\n",
    "    if 'FAST_NO_NER' not in globals() or not FAST_NO_NER:\n",
    "        try:\n",
    "            from nltk import ne_chunk\n",
    "            chunked = ne_chunk(tagged)\n",
    "            for ch in chunked:\n",
    "                if hasattr(ch,'label') and ch.label() in ['PERSON','ORGANIZATION','GPE']:\n",
    "                    ent = '_'.join(c[0].lower() for c in ch)\n",
    "                    entities.add(f'entity_{ent}')\n",
    "        except Exception:\n",
    "            pass\n",
    "    entity_count_bucket = _bucket(len(entities), [0,1,3,5])\n",
    "\n",
    "    # LDA（可關）\n",
    "    if ('FAST_NO_LDA' not in globals() or not FAST_NO_LDA) and (lda_vectorizer is not None) and (lda_model is not None):\n",
    "        X_ = lda_vectorizer.transform([text_content] if text_content else [''])\n",
    "        topic_dist = lda_model.transform(X_)\n",
    "        dom = int(np.argmax(topic_dist[0])) if topic_dist.size>0 else 0\n",
    "        score = float(topic_dist[0][dom]) if topic_dist.size>0 else 0.0\n",
    "        topic_bucket = f'topic_{dom}_b{_bucket(score,[0,0.5,0.7,0.9])}'\n",
    "    else:\n",
    "        topic_bucket = 'topic_unk'\n",
    "\n",
    "    # 參與度\n",
    "    engagement_metrics = {'clicks':0, 'shares':share_count, 'comments':comment_count}\n",
    "    click_elements = soup.find_all(lambda tag: 'click' in tag.get_text().lower() and re.search(r'\\d+', tag.get_text()))\n",
    "    for elem in click_elements:\n",
    "        m = re.search(r'(\\d+)\\s*clicks?', elem.get_text(), re.I)\n",
    "        if m: engagement_metrics['clicks'] += int(m.group(1))\n",
    "    clicks_bucket   = _bucket(engagement_metrics['clicks'],   [0,100,1000,10000])\n",
    "    comments_bucket = _bucket(engagement_metrics['comments'], [0,1,10,50])\n",
    "\n",
    "    # 可讀性（可關）\n",
    "    if 'FAST_NO_TEXTSTAT' not in globals() or not FAST_NO_TEXTSTAT:\n",
    "        try:\n",
    "            from textstat import flesch_reading_ease\n",
    "            readability_score = flesch_reading_ease(text_content) if text_content else 0\n",
    "        except Exception:\n",
    "            readability_score = 0\n",
    "    else:\n",
    "        readability_score = 50  # 中性\n",
    "    readability_bucket = ('very_easy' if readability_score>80 else\n",
    "                          'easy' if readability_score>60 else\n",
    "                          'standard' if readability_score>50 else\n",
    "                          'difficult' if readability_score>30 else 'very_difficult')\n",
    "\n",
    "    # 版面結構\n",
    "    div_count = len(soup.find_all('div'))\n",
    "    section_count = len(soup.find_all('section'))\n",
    "    list_count = len(soup.find_all(['ul','ol']))\n",
    "    div_count_bucket = _bucket(div_count, [0,5,10,20,50])\n",
    "    section_count_bucket = _bucket(section_count, [0,1,3,5])\n",
    "    list_count_bucket = _bucket(list_count, [0,1,3,5])\n",
    "    header_word_count_bucket = _bucket(len(text_content.split()), [0,50,100,200,500])\n",
    "\n",
    "    # 拼元特徵 token\n",
    "    feats = []\n",
    "    feats += [\n",
    "        f'author_{author_slug or \"unknown\"}',\n",
    "        f'channel_{channel_slug}',\n",
    "        f'publisher_{publisher_slug}',\n",
    "        f'year_{year or \"unk\"}', f'month_{month or \"unk\"}',\n",
    "        f'weekday_{weekday or \"unk\"}', f'tod_{tod or \"unk\"}', f'season_{season or \"unk\"}',\n",
    "        'weekend' if is_weekend else 'weekday' if is_weekend is not None else 'weekend_unk',\n",
    "    ]\n",
    "    feats += [\n",
    "        'has_image' if has_image else 'no_image',\n",
    "        f'imgcnt_{img_bucket}', 'has_leadimg' if leadimg else 'no_leadimg',\n",
    "        f'imgsize_{img_size_bucket}', f'imgaspect_{img_aspect_bucket}',\n",
    "        'has_video' if has_video else 'no_video',\n",
    "        'has_audio' if has_audio else 'no_audio',\n",
    "        'has_interactive' if has_interactive else 'no_interactive',\n",
    "        f'linkcnt_{link_bucket}', f'authoritative_links_{authoritative_link_bucket}',\n",
    "    ]\n",
    "    feats += [\n",
    "        'is_listicle' if is_listicle else 'not_listicle',\n",
    "        'title_has_num' if title_has_num else 'title_no_num',\n",
    "        'title_has_year' if title_has_year else 'title_no_year',\n",
    "        'title_has_q' if title_has_q else 'title_no_q',\n",
    "        'title_has_exclaim' if title_has_exclaim else 'title_no_exclaim',\n",
    "        'title_has_colon' if title_has_colon else 'title_no_colon',\n",
    "        f'title_len_word_{tw_bucket}', f'title_len_char_{tc_bucket}', f'title_upper_{upper_bucket}',\n",
    "    ]\n",
    "    feats += [f'social_buttons_{social_count_bucket}', f'comment_sections_{comment_count_bucket}', f'share_count_{shares_bucket}']\n",
    "    feats += [f'positive_words_{pos_count}', f'negative_words_{neg_count}', f'sentiment_{sentiment_bucket}']\n",
    "    feats += [f'urgency_indicators_{urgency_count_bucket}', f'question_words_{question_count_bucket}',\n",
    "              f'noun_count_{noun_count_bucket}', f'cta_count_{cta_count_bucket}']\n",
    "    feats += [f'div_count_{div_count_bucket}', f'section_count_{section_count_bucket}',\n",
    "              f'list_count_{list_count_bucket}', f'readability_{readability_bucket}']\n",
    "    feats += [f'header_word_count_{header_word_count_bucket}']\n",
    "    feats += [f'entity_count_{entity_count_bucket}', topic_bucket,\n",
    "              f'trending_matches_{_bucket(sum(1 for w in tokens if w.lower() in TRENDING_TOPICS),[0,1,3,5])}',\n",
    "              f'clicks_{clicks_bucket}', f'shares_{shares_bucket}', f'comments_{comments_bucket}']\n",
    "    feats += list(entities)\n",
    "\n",
    "    # ★ 新增：header 詞袋文本（清洗 + 詞幹）\n",
    "    header_bow_text = _bow_clean(text_content)\n",
    "\n",
    "    # ★ 回傳 4 個值\n",
    "    return _norm(title_tokens), entities, ' '.join(feats), header_bow_text\n",
    "\n",
    "# ====== 分塊編碼器（你原本的尺寸配置保留；僅新增 header_vec）======\n",
    "title_vec = HashingVectorizer(n_features=2**20, alternate_sign=False, ngram_range=(1,2),\n",
    "                              token_pattern=r'(?u)\\b\\w+\\b')\n",
    "header_vec = HashingVectorizer(n_features=2**18, alternate_sign=True, ngram_range=(1,2),\n",
    "                               token_pattern=r'(?u)\\b\\w+\\b')\n",
    "entities_vec = HashingVectorizer(n_features=2**12, alternate_sign=False,\n",
    "                                 token_pattern=r'(?u)\\b\\w+\\b')\n",
    "make_hasher = lambda n: FeatureHasher(n_features=2**n, input_type='string', alternate_sign=False)\n",
    "author_h, channel_h, publisher_h = make_hasher(15), make_hasher(12), make_hasher(14)\n",
    "time_h, media_h, titleshape_h   = make_hasher(10), make_hasher(12), make_hasher(10)\n",
    "social_h, content_h, structure_h= make_hasher(14), make_hasher(14), make_hasher(14)\n",
    "length_h, extra_h               = make_hasher(12), make_hasher(14)\n",
    "\n",
    "# ====== tokenizer（維持原樣）======\n",
    "def tokenizer_stem_keepmeta(text: str, entities: set) -> list:\n",
    "    toks = re.split(r'\\s+', (text or '').strip()); out=[]\n",
    "    for w in toks:\n",
    "        if not w: continue\n",
    "        if '_' in w or any(ch.isdigit() for ch in w) or (w.startswith('entity_') and w[7:] in entities):\n",
    "            out.append(w)\n",
    "        elif w.lower() not in STOP and re.fullmatch(r'[a-zA-Z]+', w):\n",
    "            out.append(porter.stem(w.lower()))\n",
    "    return out\n",
    "\n",
    "# ====== feats_string → 分桶 ======\n",
    "PREFIX_MAP = {\n",
    "    'author_':'author','channel_':'channel','publisher_':'publisher',\n",
    "    'year_':'time','month_':'time','weekday_':'time','tod_':'time','season_':'time',\n",
    "    'weekend':'time','weekday':'time',\n",
    "    'has_image':'media','no_image':'media','imgcnt_':'media','has_leadimg':'media','no_leadimg':'media',\n",
    "    'imgsize_':'media','imgaspect_':'media','has_video':'media','no_video':'media','has_audio':'media','no_audio':'media',\n",
    "    'has_interactive':'media','no_interactive':'media','linkcnt_':'media','authoritative_links_':'media',\n",
    "    'is_listicle':'titleshape','not_listicle':'titleshape','title_has_':'titleshape',\n",
    "    'title_len_word_':'titleshape','title_len_char_':'titleshape','title_upper_':'titleshape',\n",
    "    'social_buttons_':'social','comment_sections_':'social','share_count_':'social',\n",
    "    'urgency_indicators_':'content','question_words_':'content','noun_count_':'content','cta_count_':'content',\n",
    "    'div_count_':'structure','section_count_':'structure','list_count_':'structure','readability_':'structure',\n",
    "    'header_word_count_':'length',\n",
    "    'entity_count_':'extra','topic_':'extra','trending_matches_':'extra','clicks_':'extra','shares_':'extra','comments_':'extra',\n",
    "    'sentiment_':'extra','positive_words_':'extra','negative_words_':'extra',\n",
    "}\n",
    "def _split_feat_tokens(feats_string: str):\n",
    "    buckets = {k: [] for k in ['author','channel','publisher','time','media','titleshape','social','content','structure','length','extra']}\n",
    "    if not feats_string:\n",
    "        for k in buckets: buckets[k] = [f'{k}=_none']; return buckets\n",
    "    for tok in feats_string.split():\n",
    "        placed = False\n",
    "        for pref, grp in PREFIX_MAP.items():\n",
    "            if tok.startswith(pref): buckets[grp].append(tok); placed=True; break\n",
    "        if not placed: buckets['extra'].append(tok)\n",
    "    for k,v in buckets.items():\n",
    "        if not v: buckets[k] = [f'{k}=_none']\n",
    "    return buckets\n",
    "\n",
    "# ====== 修正版 featurize_split：正確收集 header_bows 並 transform ======\n",
    "def featurize_split(html_series: pd.Series, lda_vectorizer=None, lda_model=None, n_jobs=1) -> sp.csr_matrix:\n",
    "    rows = html_series.astype(str).tolist()\n",
    "    if n_jobs is None or n_jobs==1:\n",
    "        quads = [preprocessor(h, lda_vectorizer, lda_model) for h in rows]  # 4-tuple\n",
    "    else:\n",
    "        quads = Parallel(n_jobs=n_jobs, backend=\"loky\", prefer=\"processes\")(\n",
    "            delayed(preprocessor)(h, lda_vectorizer, lda_model) for h in rows\n",
    "        )\n",
    "\n",
    "    # 收集器\n",
    "    titles, header_bows, entity_texts = [], [], []\n",
    "    author_tokens, channel_tokens, publisher_tokens = [], [], []\n",
    "    time_tokens, media_tokens, titleshape_tokens = [], [], []\n",
    "    social_tokens, content_tokens, structure_tokens = [], [], []\n",
    "    length_tokens, extra_tokens = [], []\n",
    "\n",
    "    # 逐行解包（4 個值）\n",
    "    for toks, ents, feats, bow in quads:\n",
    "        titles.append(' '.join(tokenizer_stem_keepmeta(toks, set(ents))) if toks else '')\n",
    "        header_bows.append(bow or '')\n",
    "        entity_texts.append(' '.join(sorted(ents)) if ents else '')\n",
    "        b = _split_feat_tokens(feats)\n",
    "        author_tokens.append(b['author']);    channel_tokens.append(b['channel']);   publisher_tokens.append(b['publisher'])\n",
    "        time_tokens.append(b['time']);        media_tokens.append(b['media']);       titleshape_tokens.append(b['titleshape'])\n",
    "        social_tokens.append(b['social']);    content_tokens.append(b['content']);   structure_tokens.append(b['structure'])\n",
    "        length_tokens.append(b['length']);    extra_tokens.append(b['extra'])\n",
    "\n",
    "    # 各塊編碼\n",
    "    X_title      = title_vec.transform(titles)\n",
    "    X_header     = header_vec.transform(header_bows)\n",
    "    X_entities   = entities_vec.transform(entity_texts)\n",
    "    X_author     = author_h.transform(author_tokens)\n",
    "    X_channel    = channel_h.transform(channel_tokens)\n",
    "    X_publisher  = publisher_h.transform(publisher_tokens)\n",
    "    X_time       = time_h.transform(time_tokens)\n",
    "    X_media      = media_h.transform(media_tokens)\n",
    "    X_titleshape = titleshape_h.transform(titleshape_tokens)\n",
    "    X_social     = social_h.transform(social_tokens)\n",
    "    X_content    = content_h.transform(content_tokens)\n",
    "    X_structure  = structure_h.transform(structure_tokens)\n",
    "    X_length     = length_h.transform(length_tokens)\n",
    "    X_extra      = extra_h.transform(extra_tokens)\n",
    "\n",
    "    # 拼接\n",
    "    X = sp.hstack([\n",
    "        X_title, X_header, X_entities,\n",
    "        X_author, X_channel, X_publisher, X_time,\n",
    "        X_media, X_titleshape, X_social, X_content, X_structure, X_length,\n",
    "        X_extra\n",
    "    ], format='csr')\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145702c0",
   "metadata": {},
   "source": [
    "把驗證集轉成特徵，後面評估直接用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc7e50e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1443840), array([508, 492], dtype=int64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一次性把驗證集轉成特徵，後面評估直接用\n",
    "X_val = featurize_split(val_df['Page content'].astype(str), lda_vectorizer, lda_model, n_jobs=1)\n",
    "y_val = val_df['Popularity'].values\n",
    "sp.save_npz(f'{OUT_DIR}/X_val_split.npz', X_val)\n",
    "np.save(f'{OUT_DIR}/y_val.npy', y_val)\n",
    "X_val.shape, np.bincount(y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d36b72",
   "metadata": {},
   "source": [
    "訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建訓練生成器\n",
    "BATCH_SIZE =  2000  # 可調大些提高吞吐\n",
    "def train_streamer(df, batch_size):\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        yield df.iloc[i:i+batch_size]\n",
    "\n",
    "classes = np.array([0,1])\n",
    "clf = SGDClassifier(\n",
    "    loss=\"log_loss\", \n",
    "    penalty=\"elasticnet\", \n",
    "    alpha=1e-4, \n",
    "    l1_ratio=0.05,\n",
    "    learning_rate=\"optimal\", \n",
    "    eta0= 500 , \n",
    "    average=True,\n",
    "    max_iter=1, \n",
    "    tol=None, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "EVAL_EVERY = 1  # 每x步評估一次\n",
    "train_auc_hist, val_auc_hist = [], []\n",
    "\n",
    "stream = train_streamer(train_df, BATCH_SIZE)\n",
    "iters = ceil(len(train_df)/BATCH_SIZE)\n",
    "\n",
    "for i, batch in enumerate(stream, start=1):\n",
    "    Xtr = featurize_split(batch['Page content'].astype(str), lda_vectorizer, lda_model, n_jobs=1)\n",
    "    ytr = batch['Popularity'].values\n",
    "    if i == 1: clf.partial_fit(Xtr, ytr, classes=classes)\n",
    "    else:      clf.partial_fit(Xtr, ytr)\n",
    "\n",
    "    # 批內 AUC\n",
    "    if np.unique(ytr).size == 2:\n",
    "        tr_auc = roc_auc_score(ytr, clf.decision_function(Xtr))\n",
    "    else:\n",
    "        tr_auc = np.nan\n",
    "    train_auc_hist.append(tr_auc)\n",
    "\n",
    "    # 週期性評估（用預計算 X_val）\n",
    "    if i % EVAL_EVERY == 0 or i == iters:\n",
    "        va_auc = roc_auc_score(y_val, clf.decision_function(X_val))\n",
    "        val_auc_hist.append(va_auc)\n",
    "        print(f\"[{i}/{iters}] Train AUC={tr_auc:.4f} | Val AUC={va_auc:.4f}\")\n",
    "    else:\n",
    "        print(f\"[{i}/{iters}] Train AUC={tr_auc:.4f}\")\n",
    "\n",
    "# 保存模型與（可選）LDA組\n",
    "import _pickle as pkl\n",
    "pkl.dump(clf, open(f'{OUT_DIR}/clf-sgd.pkl','wb'))\n",
    "pkl.dump({'lda_vec': lda_vectorizer, 'lda_model': lda_model}, open(f'{OUT_DIR}/lda.pkl','wb'))\n",
    "print(\"Model saved:\", f'{OUT_DIR}/clf-sgd.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc5d0dd",
   "metadata": {},
   "source": [
    "繪製學習曲線"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66224d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(np.arange(1, len(train_auc_hist)+1), train_auc_hist, label='Train AUC')\n",
    "if len(val_auc_hist)>0:\n",
    "    # 將每 EVAL_EVERY 次的 val AUC 對齊到對應的 iteration（簡單處理）\n",
    "    xs = np.linspace(EVAL_EVERY, len(train_auc_hist), num=len(val_auc_hist))\n",
    "    plt.plot(xs, val_auc_hist, label='Val AUC', marker='o', linestyle='--')\n",
    "plt.xlabel('Iteration'); plt.ylabel('AUC'); plt.title('AUC over iterations'); plt.legend(); plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44171c8d",
   "metadata": {},
   "source": [
    "定期保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2868eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as pkl\n",
    "from scipy.special import expit\n",
    "\n",
    "# 讀模型與 LDA\n",
    "clf = pkl.load(open(f'{OUT_DIR}/clf-sgd.pkl','rb'))\n",
    "lda_pack = pkl.load(open(f'{OUT_DIR}/lda.pkl','rb'))\n",
    "lda_vectorizer = lda_pack.get('lda_vec', None)\n",
    "lda_model = lda_pack.get('lda_model', None)\n",
    "\n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "# 分批推理避免爆內存\n",
    "B = 2000\n",
    "preds = []\n",
    "for i in range(0, len(df_test), B):\n",
    "    sl = df_test['Page content'].iloc[i:i+B].astype(str)\n",
    "    Xt = featurize_split(sl, lda_vectorizer, lda_model, n_jobs=1)\n",
    "    scores = clf.decision_function(Xt)\n",
    "    preds.append(expit(scores))\n",
    "test_pred = np.concatenate(preds)\n",
    "\n",
    "submission = pd.DataFrame({'Id': df_test['Id'], 'Popularity': test_pred})\n",
    "sub_path = f'{OUT_DIR}/submission_66.csv'\n",
    "submission.to_csv(sub_path, index=False)\n",
    "print(\"Saved:\", sub_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de06d39",
   "metadata": {},
   "source": [
    "校準精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9badad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 1) 用“最佳”clf在验证集上做概率校准 ======\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy.sparse import vstack\n",
    "import numpy as np\n",
    "\n",
    "# 你前面已经把验证流固化成了 val_batches = list(val_stream)\n",
    "# 如果你用的是 featurize(...)（比如双通道词+字），就用它；\n",
    "# 如果你是单个 hashvec，就把下面的 featurize(...) 换成 hashvec.transform(...)\n",
    "\n",
    "Xv_list, yv_list = [], []\n",
    "for vb in val_batches:\n",
    "    Xv_list.append(featurize(vb['Page content']))   # ← 或者：hashvec.transform(vb['Page content'].astype(str))\n",
    "    yv_list.append(vb['Popularity'].values)\n",
    "\n",
    "X_val = vstack(Xv_list, format='csr')\n",
    "y_val = np.concatenate(yv_list)\n",
    "\n",
    "# 用已训练好的 clf 做“预拟合”校准\n",
    "# 验证集较小（你设的是 1000），优先用 'sigmoid'(Platt)；'isotonic' 更灵活但容易过拟合\n",
    "cal_clf = CalibratedClassifierCV(clf, method='sigmoid', cv='prefit')\n",
    "cal_clf.fit(X_val, y_val)\n",
    "\n",
    "# （可选）看看“校准后”的验证 AUC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "val_pred_cal = cal_clf.predict_proba(X_val)[:, 1]\n",
    "print(f\"Calibrated Val AUC: {roc_auc_score(y_val, val_pred_cal):.4f}\")\n",
    "\n",
    "# ====== 2) 用校准后的模型生成测试集预测 ======\n",
    "df_test = pd.read_csv('./dataset/test.csv')\n",
    "Xt = featurize(df_test['Page content'])             # ← 或者：hashvec.transform(...)\n",
    "test_pred = cal_clf.predict_proba(Xt)[:, 1]\n",
    "\n",
    "submission = pd.DataFrame({'Id': df_test['Id'], 'Popularity': test_pred})\n",
    "submission.to_csv('output/submission_46.csv', index=False)\n",
    "print('Saved: output/submission_46.csv')\n",
    "\n",
    "# （可选）持久化校准后的模型，之后直接加载 cal_clf 用\n",
    "import _pickle as pkl, os\n",
    "os.makedirs('output', exist_ok=True)\n",
    "pkl.dump(cal_clf, open('output/clf-calibrated.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97c0307",
   "metadata": {},
   "source": [
    "k 折"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be63a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using StratifiedKFold (shuffle=True)\n",
      "\n",
      "Start 8-fold CV: EPOCHS=5, BATCH_SIZE=300, DO_FOLD_LDA=False, MODE=stratified\n",
      "\n",
      "========== Fold 1/8 ==========\n",
      "Fold 1 | epoch 1/5 | Val AUC=0.5649\n",
      "Fold 1 | epoch 2/5 | Val AUC=0.5637\n",
      "Fold 1 | epoch 3/5 | Val AUC=0.5647\n",
      "  Early stopping at epoch 3 (no improve 2×)\n",
      "Fold 1 BEST: epoch=1, AUC=0.5649 | saved ./output\\model_3_stratified_clf_sgd_fold1.pkl\n",
      "\n",
      "========== Fold 2/8 ==========\n",
      "Fold 2 | epoch 1/5 | Val AUC=0.5704\n",
      "Fold 2 | epoch 2/5 | Val AUC=0.5752\n",
      "Fold 2 | epoch 3/5 | Val AUC=0.5755\n",
      "Fold 2 | epoch 4/5 | Val AUC=0.5742\n",
      "Fold 2 | epoch 5/5 | Val AUC=0.5747\n",
      "  Early stopping at epoch 5 (no improve 2×)\n",
      "Fold 2 BEST: epoch=3, AUC=0.5755 | saved ./output\\model_3_stratified_clf_sgd_fold2.pkl\n",
      "\n",
      "========== Fold 3/8 ==========\n",
      "Fold 3 | epoch 1/5 | Val AUC=0.5763\n",
      "Fold 3 | epoch 2/5 | Val AUC=0.5772\n",
      "Fold 3 | epoch 3/5 | Val AUC=0.5751\n",
      "Fold 3 | epoch 4/5 | Val AUC=0.5742\n",
      "  Early stopping at epoch 4 (no improve 2×)\n",
      "Fold 3 BEST: epoch=2, AUC=0.5772 | saved ./output\\model_3_stratified_clf_sgd_fold3.pkl\n",
      "\n",
      "========== Fold 4/8 ==========\n",
      "Fold 4 | epoch 1/5 | Val AUC=0.5710\n",
      "Fold 4 | epoch 2/5 | Val AUC=0.5686\n",
      "Fold 4 | epoch 3/5 | Val AUC=0.5730\n",
      "Fold 4 | epoch 4/5 | Val AUC=0.5762\n",
      "Fold 4 | epoch 5/5 | Val AUC=0.5766\n",
      "Fold 4 BEST: epoch=5, AUC=0.5766 | saved ./output\\model_3_stratified_clf_sgd_fold4.pkl\n",
      "\n",
      "========== Fold 5/8 ==========\n",
      "Fold 5 | epoch 1/5 | Val AUC=0.5673\n",
      "Fold 5 | epoch 2/5 | Val AUC=0.5669\n",
      "Fold 5 | epoch 3/5 | Val AUC=0.5663\n",
      "  Early stopping at epoch 3 (no improve 2×)\n",
      "Fold 5 BEST: epoch=1, AUC=0.5673 | saved ./output\\model_3_stratified_clf_sgd_fold5.pkl\n",
      "\n",
      "========== Fold 6/8 ==========\n",
      "Fold 6 | epoch 1/5 | Val AUC=0.5708\n",
      "Fold 6 | epoch 2/5 | Val AUC=0.5713\n",
      "Fold 6 | epoch 3/5 | Val AUC=0.5711\n",
      "Fold 6 | epoch 4/5 | Val AUC=0.5721\n",
      "Fold 6 | epoch 5/5 | Val AUC=0.5718\n",
      "Fold 6 BEST: epoch=4, AUC=0.5721 | saved ./output\\model_3_stratified_clf_sgd_fold6.pkl\n",
      "\n",
      "========== Fold 7/8 ==========\n",
      "Fold 7 | epoch 1/5 | Val AUC=0.5770\n",
      "Fold 7 | epoch 2/5 | Val AUC=0.5813\n",
      "Fold 7 | epoch 3/5 | Val AUC=0.5807\n",
      "Fold 7 | epoch 4/5 | Val AUC=0.5816\n",
      "Fold 7 | epoch 5/5 | Val AUC=0.5825\n",
      "Fold 7 BEST: epoch=5, AUC=0.5825 | saved ./output\\model_3_stratified_clf_sgd_fold7.pkl\n",
      "\n",
      "========== Fold 8/8 ==========\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# K 折交叉驗證 + 每折最佳模型集成輸出 test 預測（支援 group / stratified / kfold）\n",
    "# =========================\n",
    "import os, gc, copy, _pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.special import expit\n",
    "\n",
    "# ---------- 基本配置 ----------\n",
    "TRAIN_PATH = './dataset/train.csv'\n",
    "TEST_PATH  = './dataset/test.csv'\n",
    "OUT_DIR    = './output'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 切換這裡即可：'group'（按 publisher 防洩漏）| 'stratified'（隨機分層）| 'kfold'（純隨機）\n",
    "CV_MODE     = 'stratified'\n",
    "N_SPLITS    = 8\n",
    "EPOCHS      = 5\n",
    "BATCH_SIZE  = 300\n",
    "SEED        = 28\n",
    "PATIENCE    = 2          # 早停：連續 PATIENCE 個 epoch 無提升就停\n",
    "DO_FOLD_LDA = False      # True：每折在訓練集上預訓練 LDA（更穩但更慢）\n",
    "\n",
    "# ----------（僅 group 模式會用到）publisher 提取 ----------\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "def _norm(s): return re.sub(r'[\\W]+', ' ', (s or '').lower()).strip()\n",
    "def _slug(s): return re.sub(r'[^a-z0-9_]+', '', _norm(s).replace(' ', '_'))\n",
    "\n",
    "def extract_publisher_slug(html: str) -> str:\n",
    "    if not isinstance(html, str) or not html.strip():\n",
    "        return \"unknown\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    pub = soup.find('a', href=re.compile(r'/publishers/[^/]+/?', re.I))\n",
    "    if pub:\n",
    "        publisher = pub.get_text(' ', strip=True) or re.sub(r'.*/publishers/([^/]+)/?.*', r'\\1', pub['href'], flags=re.I)\n",
    "    else:\n",
    "        publisher = \"unknown\"\n",
    "    return _slug(publisher or 'unknown')\n",
    "\n",
    "# ---------- 分割器工廠 ----------\n",
    "def make_split_iter(cv_mode, n_splits, seed, y, texts=None, groups=None):\n",
    "    \"\"\"\n",
    "    回傳 (tr_idx, va_idx) 的迭代器：\n",
    "      - 'group'：GroupKFold（需 groups=publisher_keys）\n",
    "      - 'stratified'：StratifiedKFold（shuffle=True）\n",
    "      - 'kfold'：KFold（shuffle=True）\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    X_dummy = np.zeros(n)  # 只為滿足 API\n",
    "    if cv_mode == 'group':\n",
    "        if groups is None:\n",
    "            raise ValueError(\"CV_MODE='group' 需要提供 groups（publisher_keys）\")\n",
    "        splitter = GroupKFold(n_splits=n_splits)\n",
    "        print(f\"Using GroupKFold by publisher (groups={pd.Series(groups).nunique()})\")\n",
    "        return splitter.split(X_dummy, y, groups)\n",
    "\n",
    "    elif cv_mode == 'stratified':\n",
    "        splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        print(\"Using StratifiedKFold (shuffle=True)\")\n",
    "        return splitter.split(X_dummy, y)\n",
    "\n",
    "    elif cv_mode == 'kfold':\n",
    "        splitter = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        print(\"Using KFold (shuffle=True) — no stratification\")\n",
    "        return splitter.split(X_dummy)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown CV_MODE: {cv_mode}\")\n",
    "\n",
    "# ---------- 分類器工廠（文本穩定配置） ----------\n",
    "def make_clf():\n",
    "    return SGDClassifier(\n",
    "        loss=\"hinge\",        # 用邏輯損失以獲得 predict_proba\n",
    "        penalty=\"elasticnet\",\n",
    "        l1_ratio=0.25,\n",
    "        alpha=1e-6,\n",
    "        learning_rate=\"optimal\",\n",
    "        eta0=0.01,\n",
    "        average=True,\n",
    "        random_state=SEED\n",
    "    )\n",
    "\n",
    "# ---------- 讀取資料 ----------\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "df['Popularity'] = (df['Popularity'].astype(int) == 1).astype(int)\n",
    "y = df['Popularity'].values\n",
    "texts = df['Page content'].astype(str)\n",
    "\n",
    "# 只有 group 模式才需要抽取 publisher；其他模式略過可節省時間\n",
    "if CV_MODE == 'group':\n",
    "    publisher_keys = texts.apply(extract_publisher_slug).values\n",
    "    split_iter = make_split_iter(CV_MODE, N_SPLITS, SEED, y, texts, publisher_keys)\n",
    "else:\n",
    "    split_iter = make_split_iter(CV_MODE, N_SPLITS, SEED, y, texts, None)\n",
    "\n",
    "# 容器\n",
    "fold_artifacts = []   # 保存每折最佳模型與（可選）LDA\n",
    "oof_scores = np.zeros(len(df), dtype=float)\n",
    "fold_aucs, fold_epochs = [], []\n",
    "\n",
    "print(f\"\\nStart {N_SPLITS}-fold CV: EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, DO_FOLD_LDA={DO_FOLD_LDA}, MODE={CV_MODE}\")\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(split_iter, start=1):\n",
    "    print(f\"\\n========== Fold {fold}/{N_SPLITS} ==========\")\n",
    "    tr_df = df.iloc[tr_idx].reset_index(drop=True)\n",
    "    va_df = df.iloc[va_idx].reset_index(drop=True)\n",
    "    y_val = va_df['Popularity'].values\n",
    "\n",
    "    # 每折 LDA（可選，避免外洩需在訓練集上建）\n",
    "    if DO_FOLD_LDA:\n",
    "        lda_vec_f, lda_mod_f = pretrain_lda(tr_df, column='Page content', n_components=10, max_features=1000)\n",
    "    else:\n",
    "        lda_vec_f, lda_mod_f = (None, None)\n",
    "\n",
    "    # 固定本折驗證特徵\n",
    "    X_val = featurize_split(va_df['Page content'].astype(str), lda_vec_f, lda_mod_f, n_jobs=1)\n",
    "\n",
    "    # 模型與早停\n",
    "    clf = make_clf()\n",
    "    best_auc, best_epoch = -1, -1\n",
    "    best_state = None\n",
    "    no_improve = 0\n",
    "\n",
    "    # 多 epoch 訓練\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tr_shuf = tr_df.sample(frac=1.0, random_state=SEED+epoch).reset_index(drop=True)\n",
    "        n_batches = ceil(len(tr_shuf)/BATCH_SIZE)\n",
    "\n",
    "        for b in range(n_batches):\n",
    "            batch = tr_shuf.iloc[b*BATCH_SIZE:(b+1)*BATCH_SIZE]\n",
    "            X_tr = featurize_split(batch['Page content'].astype(str), lda_vec_f, lda_mod_f, n_jobs=1)\n",
    "            y_tr = batch['Popularity'].values\n",
    "            if epoch == 1 and b == 0:\n",
    "                clf.partial_fit(X_tr, y_tr, classes=np.array([0,1]))\n",
    "            else:\n",
    "                clf.partial_fit(X_tr, y_tr)\n",
    "\n",
    "        # epoch 結束：評估本折 Val\n",
    "        if hasattr(clf, \"predict_proba\"):\n",
    "            val_prob = clf.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            val_prob = expit(clf.decision_function(X_val))\n",
    "        val_auc = roc_auc_score(y_val, val_prob)\n",
    "        print(f\"Fold {fold} | epoch {epoch}/{EPOCHS} | Val AUC={val_auc:.4f}\")\n",
    "\n",
    "        if val_auc > best_auc:\n",
    "            best_auc = val_auc\n",
    "            best_epoch = epoch\n",
    "            best_state = copy.deepcopy(clf)\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= PATIENCE:\n",
    "                print(f\"  Early stopping at epoch {epoch} (no improve {PATIENCE}×)\")\n",
    "                break\n",
    "\n",
    "    # 保存 OOF（用最佳狀態）\n",
    "    if hasattr(best_state, \"predict_proba\"):\n",
    "        oof_scores[va_idx] = best_state.predict_proba(X_val)[:, 1]\n",
    "    else:\n",
    "        oof_scores[va_idx] = expit(best_state.decision_function(X_val))\n",
    "\n",
    "    fold_aucs.append(best_auc); fold_epochs.append(best_epoch)\n",
    "\n",
    "    # 保存每折最佳模型到硬碟\n",
    "    model_path = os.path.join(OUT_DIR, f'model_3_{CV_MODE}_clf_sgd_fold{fold}.pkl')\n",
    "    pkl.dump(best_state, open(model_path, 'wb'))\n",
    "\n",
    "    # 若用了 LDA，順便把該折的 LDA 也存起來\n",
    "    lda_path = None\n",
    "    if DO_FOLD_LDA:\n",
    "        lda_path = os.path.join(OUT_DIR, f'lda_{CV_MODE}_fold{fold}.pkl')\n",
    "        pkl.dump({'lda_vec': lda_vec_f, 'lda_model': lda_mod_f}, open(lda_path, 'wb'))\n",
    "\n",
    "    fold_artifacts.append({'model_path': model_path, 'lda_path': lda_path})\n",
    "    print(f\"Fold {fold} BEST: epoch={best_epoch}, AUC={best_auc:.4f} | saved {model_path}\")\n",
    "\n",
    "    del X_val; gc.collect()\n",
    "\n",
    "# CV 總結\n",
    "oof_auc = roc_auc_score(y, oof_scores)\n",
    "print(\"\\n========== CV Summary ==========\")\n",
    "print(\"Fold AUCs:\", [\"%.4f\" % a for a in fold_aucs])\n",
    "print(\"Mean AUC = %.4f | Std = %.4f\" % (np.mean(fold_aucs), np.std(fold_aucs)))\n",
    "print(\"OOF  AUC = %.4f\" % oof_auc)\n",
    "\n",
    "# ---------- 用每折最佳模型對 test 預測並平均 ----------\n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "test_texts = df_test['Page content'].astype(str)\n",
    "test_preds_each_fold = []\n",
    "\n",
    "for fold, art in enumerate(fold_artifacts, start=1):\n",
    "    # 讀模型\n",
    "    clf = pkl.load(open(art['model_path'], 'rb'))\n",
    "    # 讀折內 LDA（可選）\n",
    "    if DO_FOLD_LDA and art['lda_path'] is not None:\n",
    "        lda_pack = pkl.load(open(art['lda_path'], 'rb'))\n",
    "        lda_vec_f, lda_mod_f = lda_pack['lda_vec'], lda_pack['lda_model']\n",
    "    else:\n",
    "        lda_vec_f, lda_mod_f = (None, None)\n",
    "\n",
    "    # 特徵化（若各折 LDA 不同，需各自 transform 一次）\n",
    "    X_test = featurize_split(test_texts, lda_vec_f, lda_mod_f, n_jobs=1)\n",
    "\n",
    "    # 預測機率\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prob = clf.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        prob = expit(clf.decision_function(X_test))\n",
    "    test_preds_each_fold.append(prob)\n",
    "    print(f\"Fold {fold} test predicted. Shape={prob.shape}\")\n",
    "\n",
    "# 集成（平均）\n",
    "test_pred = np.mean(np.vstack(test_preds_each_fold), axis=0)\n",
    "\n",
    "# 導出提交\n",
    "sub_path = os.path.join(OUT_DIR, f'model_3_submission_k{N_SPLITS}_3.csv')\n",
    "pd.DataFrame({'Id': df_test['Id'], 'Popularity': test_pred}).to_csv(sub_path, index=False)\n",
    "print(\"Submission saved ->\", sub_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
